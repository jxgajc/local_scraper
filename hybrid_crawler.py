import os
import sys

PROJECT_NAME = "hybrid_crawler"

def write_file(path, content):
    filepath = os.path.join(PROJECT_NAME, path)
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content.strip())
    print(f"[+] Created: {filepath}")

def main():
    if not os.path.exists(PROJECT_NAME):
        os.makedirs(PROJECT_NAME)
    
    # =========================================================================
    # 0. é¡¹ç›®æ–‡æ¡£ README.md (æ–°å¢)
    # =========================================================================
    write_file("README.md", f"""
# é«˜æ€§èƒ½æ··åˆæ¶æ„çˆ¬è™«ç³»ç»Ÿ (Hybrid Crawler)

è¿™æ˜¯ä¸€ä¸ªä¼ä¸šçº§çš„ Scrapy çˆ¬è™«è„šæ‰‹æ¶ï¼Œé›†æˆäº† HTTP é«˜å¹¶å‘é‡‡é›†ä¸ Playwright åŠ¨æ€æ¸²æŸ“é‡‡é›†ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

1.  **æ··åˆæ¶æ„**ï¼šåŒæ—¶æ”¯æŒè½»é‡çº§ HTTP è¯·æ±‚ï¼ˆ`BaseRequestSpider`ï¼‰å’Œé‡é‡çº§æµè§ˆå™¨æ¸²æŸ“ï¼ˆ`BasePlaywrightSpider`ï¼‰ã€‚
2.  **æ™ºèƒ½é‡è¯•**ï¼š
    * **ç½‘ç»œé”™è¯¯**ï¼šæŒ‡æ•°é€€é¿ï¼ˆç­‰å¾…æ—¶é—´ç¿»å€ï¼‰ã€‚
    * **é€»è¾‘é”™è¯¯**ï¼šå‡€å®¤é‡è¯•ï¼ˆé”€æ¯æµè§ˆå™¨ Contextï¼Œæ¸…ç† Cookie åé‡è¯•ï¼‰ã€‚
3.  **é«˜å¯ç”¨ç®¡é“**ï¼š
    * **å¼‚æ­¥ IO**ï¼šæ•°æ®åº“å†™å…¥æ“ä½œåœ¨ç‹¬ç«‹çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œä¸é˜»å¡çˆ¬è™«ä¸»å¾ªç¯ã€‚
    * **é™çº§ç­–ç•¥**ï¼šæ‰¹é‡å†™å…¥å¤±è´¥æ—¶è‡ªåŠ¨æ‹†åŒ…ï¼Œé€æ¡å†™å…¥ï¼Œéš”ç¦»è„æ•°æ®ã€‚
4.  **èµ„æºéš”ç¦»**ï¼šæµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼ˆContextï¼‰åŸºäº URL å“ˆå¸Œéš”ç¦»ï¼Œé˜²æ­¢ä¼šè¯æ±¡æŸ“ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå®‰è£…

éœ€è¦ Python 3.9+ã€‚

```bash
# å®‰è£… Python ä¾èµ–
pip install -r requirements.txt

# å®‰è£… Playwright æµè§ˆå™¨å†…æ ¸ (å¿…é¡»)
playwright install chromium
```

### 2. æ•°æ®åº“é…ç½®

æœ¬é¡¹ç›®é»˜è®¤ä½¿ç”¨ MySQLã€‚è¯·ç¡®ä¿æœ¬åœ°å·²å®‰è£… MySQL æˆ–ä½¿ç”¨ Docker å¯åŠ¨ã€‚

ä¿®æ”¹ `settings.py` æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ï¼š

```bash
export DATABASE_URL="mysql+pymysql://root:password@localhost:3306/spider_db"
```s

### 3. è¿è¡Œçˆ¬è™«

**æ™®é€šæ¨¡å¼è¿è¡Œï¼š**
```bash
python run.py
```

**è°ƒè¯•æ¨¡å¼è¿è¡Œ (è¾“å‡ºè¯¦ç»†æ—¥å¿—)ï¼š**
```bash
python run.py debug
```

## ğŸ› ï¸ Debug æŒ‡å—

### Q1: å¦‚ä½•çœ‹åˆ°æµè§ˆå™¨ç•Œé¢ï¼Ÿ
ä¿®æ”¹ `settings.py` ä¸­çš„ `PLAYWRIGHT_LAUNCH_OPTIONS`ï¼š
```python
'headless': False,  # æ”¹ä¸º False å³å¯çœ‹åˆ°æµè§ˆå™¨æ“ä½œ
'slow_mo': 500,     # å¢åŠ æ…¢åŠ¨ä½œå»¶è¿Ÿï¼Œæ–¹ä¾¿äººçœ¼è§‚å¯Ÿ
```

### Q2: æ•°æ®åº“å†™å…¥æŠ¥é”™æ€ä¹ˆåŠï¼Ÿ
åœ¨ `models/__init__.py` ä¸­å¼€å¯ SQL å›æ˜¾ï¼š
```python
engine = create_engine(..., echo=True) # è®¾ç½®ä¸º True å¯åœ¨æ§åˆ¶å°çœ‹åˆ°æ‰€æœ‰ SQL è¯­å¥
```

### Q3: Playwright æŠ¥é”™ "Target closed"
é€šå¸¸æ˜¯å› ä¸ºå†…å­˜ä¸è¶³æˆ–å¹¶å‘è¿‡é«˜ã€‚
1. é™ä½ `settings.py` ä¸­çš„ `CONCURRENT_REQUESTS`ã€‚
2. ç¡®ä¿ `base_spiders.py` ä¸­çš„ `page.close()` é€»è¾‘æ­£ç¡®æ‰§è¡Œã€‚

## ğŸ“‚ ç›®å½•ç»“æ„

* `spiders/base_spiders.py`: **æ ¸å¿ƒ**ã€‚å®šä¹‰äº† HTTP å’Œ Playwright çš„åŸºç±»ã€‚
* `middlewares.py`: å®šä¹‰äº†æ™ºèƒ½é‡è¯•é€»è¾‘å’Œè¯·æ±‚è·¯ç”±ã€‚
* `pipelines.py`: å®šä¹‰äº†å¼‚æ­¥æ‰¹é‡å†™å…¥å’Œé™çº§é€»è¾‘ã€‚
* `models/`: å®šä¹‰äº† SQLAlchemy æ•°æ®æ¨¡å‹ã€‚
""")

    # =========================================================================
    # 1. åŸºç¡€ä¾èµ– requirements.txt
    # =========================================================================
    write_file("requirements.txt", """
Scrapy>=2.11.0
scrapy-playwright>=0.0.33
SQLAlchemy>=2.0.0
PyMySQL>=1.1.0
twisted>=23.8.0
cryptography
itemadapter
psutil
""")

    write_file("scrapy.cfg", f"""
[settings]
default = {PROJECT_NAME}.settings

[deploy]
project = {PROJECT_NAME}
""")

    write_file(f"{PROJECT_NAME}/__init__.py", "")

    # =========================================================================
    # 2. å¼‚å¸¸ä½“ç³» exceptions.py
    # =========================================================================
    write_file(f"{PROJECT_NAME}/exceptions.py", """
\"\"\"
å¼‚å¸¸åˆ†ç±»ä½“ç³»
ç”¨äºæŒ‡å¯¼ä¸­é—´ä»¶è¿›è¡Œä¸åŒçš„é‡è¯•ç­–ç•¥
\"\"\"

class CrawlerNetworkError(IOError):
    \"\"\"
    [ç½‘ç»œå±‚é”™è¯¯]
    åœºæ™¯ï¼šè¿æ¥è¶…æ—¶ã€DNSå¤±è´¥ã€TCPé‡ç½®ã€‚
    ç­–ç•¥ï¼šè§¦å‘æŒ‡æ•°é€€é¿é‡è¯• (Exponential Backoff)ã€‚
    \"\"\"
    pass

class ElementNotFoundError(ValueError):
    \"\"\"
    [é€»è¾‘å±‚é”™è¯¯]
    åœºæ™¯ï¼šé¡µé¢åŠ è½½æˆåŠŸä½†å…³é”®å…ƒç´ æœªæ‰¾åˆ°ï¼ˆå¯èƒ½é‡åˆ°éªŒè¯ç æˆ–å¸ƒå±€å˜æ›´ï¼‰ã€‚
    ç­–ç•¥ï¼šè§¦å‘å‡€å®¤é‡è¯•ï¼ˆClean Slate Retryï¼‰ï¼Œé”€æ¯ Context é‡å¯ã€‚
    \"\"\"
    pass

class BrowserCrashError(RuntimeError):
    \"\"\"
    [è¿è¡Œæ—¶é”™è¯¯]
    åœºæ™¯ï¼šPlaywright Page å¯¹è±¡å´©æºƒæˆ– Target Closedã€‚
    ç­–ç•¥ï¼šè§¦å‘å‡€å®¤é‡è¯•ã€‚
    \"\"\"
    pass

class DataValidationError(ValueError):
    \"\"\"
    [æ•°æ®å±‚é”™è¯¯]
    åœºæ™¯ï¼šæ¸…æ´—ç®¡é“å‘ç°ç¼ºå°‘å¿…å¡«å­—æ®µã€‚
    ç­–ç•¥ï¼šç›´æ¥ä¸¢å¼ƒ Item å¹¶è®°å½•è­¦å‘Šï¼Œä¸é‡è¯•ã€‚
    \"\"\"
    pass
""")

    # =========================================================================
    # 3. æ•°æ®æ¨¡å‹ models/
    # =========================================================================
    write_file(f"{PROJECT_NAME}/models/__init__.py", """
import os
from sqlalchemy import create_engine, Column, Integer, DateTime
from sqlalchemy.orm import sessionmaker, declarative_base
from datetime import datetime

# ğŸ’¡ Debugæç¤º: å°† echo=False æ”¹ä¸º True å¯ä»¥æŸ¥çœ‹ç”Ÿæˆçš„ SQL è¯­å¥
DATABASE_URL = os.getenv('DATABASE_URL', 'mysql+pymysql://root:password@localhost:3306/spider_db')

engine = create_engine(
    DATABASE_URL,
    pool_size=20,           # æ ¸å¿ƒè¿æ¥æ•°ï¼šä¿æŒå¸¸é©»çš„è¿æ¥æ•°é‡
    max_overflow=40,        # çªå‘è¿æ¥æ•°ï¼šé«˜å¹¶å‘æ—¶å…è®¸ä¸´æ—¶åˆ›å»ºçš„è¿æ¥
    pool_recycle=3600,      # è¿æ¥å›æ”¶ï¼šé˜²æ­¢ MySQL 8å°æ—¶æ–­å¼€é—®é¢˜
    pool_timeout=30,
    echo=False              # ç”Ÿäº§ç¯å¢ƒå»ºè®®å…³é—­ SQL æ—¥å¿—
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

class BaseModel(Base):
    \"\"\"æ‰€æœ‰æ¨¡å‹çš„åŸºç±»ï¼ŒåŒ…å«é€šç”¨å®¡è®¡å­—æ®µ\"\"\"
    __abstract__ = True
    id = Column(Integer, primary_key=True, index=True, autoincrement=True)
    created_at = Column(DateTime, default=datetime.now, nullable=False)
    updated_at = Column(DateTime, default=datetime.now, onupdate=datetime.now, nullable=False)

def init_db():
    \"\"\"åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„\"\"\"
    Base.metadata.create_all(bind=engine)
""")

    write_file(f"{PROJECT_NAME}/models/crawl_data.py", """
from sqlalchemy import Column, String, Text, Integer, JSON
from . import BaseModel

class CrawlData(BaseModel):
    __tablename__ = 'crawl_data'
    
    url = Column(String(768), nullable=False, index=True, comment="URL")
    url_hash = Column(String(64), unique=True, index=True, comment="æŒ‡çº¹ç”¨äºå»é‡")
    title = Column(String(512), nullable=True)
    content = Column(Text, nullable=True)
    meta_info = Column(JSON, nullable=True, comment="å­˜å‚¨é¢å¤–çš„JSONå…ƒæ•°æ®")
    status_code = Column(Integer, default=200)
    source = Column(String(64), index=True, comment="æ•°æ®æ¥æºæ ‡è¯†")
""")

    # =========================================================================
    # 4. Items items.py
    # =========================================================================
    write_file(f"{PROJECT_NAME}/items.py", """
import scrapy

class HybridCrawlerItem(scrapy.Item):
    url = scrapy.Field()
    title = scrapy.Field()
    content = scrapy.Field()
    status_code = scrapy.Field()
    request_type = scrapy.Field()
    source = scrapy.Field()
    meta_info = scrapy.Field()
    # å†…éƒ¨ä½¿ç”¨å­—æ®µ
    url_hash = scrapy.Field()
""")

    # =========================================================================
    # 5. ä¸­é—´ä»¶ middlewares.py
    # =========================================================================
    write_file(f"{PROJECT_NAME}/middlewares.py", """
import time
import random
import logging
from scrapy.downloadermiddlewares.retry import RetryMiddleware
from twisted.internet.error import (
    ConnectionRefusedError, DNSLookupError, TimeoutError, TCPTimedOutError
)
from .exceptions import CrawlerNetworkError, ElementNotFoundError, BrowserCrashError

logger = logging.getLogger(__name__)

class StrategyRoutingMiddleware:
    \"\"\"
    ã€ç­–ç•¥è·¯ç”±ä¸­é—´ä»¶ã€‘
    ä½œç”¨ï¼šåœ¨è¯·æ±‚å‘å‡ºå‰ï¼Œæ ¹æ® Request çš„ meta æ ‡è®°ï¼Œå†³å®šæ˜¯å¦å¯ç”¨ Playwrightã€‚
    \"\"\"
    def process_request(self, request, spider):
        request_type = request.meta.get('request_type', 'http')
        
        # å¦‚æœæ ‡è®°ä¸º playwrightï¼Œåˆ™æ¿€æ´» scrapy-playwright æ’ä»¶çš„å‚æ•°
        if request_type == 'playwright':
            request.meta['playwright'] = True
            request.meta['dont_merge_cookies'] = True # æµè§ˆå™¨è‡ªå·±ç®¡ç† Cookieï¼Œä¸ä½¿ç”¨ Scrapy çš„ CookieJar
        return None

class SmartRetryMiddleware(RetryMiddleware):
    \"\"\"
    ã€æ™ºèƒ½é‡è¯•ä¸­é—´ä»¶ã€‘
    ä½œç”¨ï¼šæ›¿ä»£é»˜è®¤çš„ RetryMiddlewareï¼Œå®ç°åˆ†çº§é‡è¯•ç­–ç•¥ã€‚
    \"\"\"
    NETWORK_ERRORS = (ConnectionRefusedError, DNSLookupError, TimeoutError, TCPTimedOutError, CrawlerNetworkError)
    LOGIC_ERRORS = (ElementNotFoundError, BrowserCrashError)

    def process_exception(self, request, exception, spider):
        retry_times = request.meta.get('retry_times', 0) + 1
        max_retries = self.max_retry_times

        if retry_times > max_retries:
            logger.error(f"âŒ æ”¾å¼ƒè¯·æ±‚ {request.url}: è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°")
            return None

        # ç­–ç•¥ 1: ç½‘ç»œé”™è¯¯ -> æŒ‡æ•°é€€é¿ (Wait time = 2^(n-1))
        if isinstance(exception, self.NETWORK_ERRORS):
            delay = 2 ** (retry_times - 1)
            logger.warning(f"âš ï¸ ç½‘ç»œæ³¢åŠ¨ ({exception}), {delay}s åé‡è¯•: {request.url}")
            time.sleep(delay) # æ³¨æ„ï¼šè¿™é‡Œç®€å•çš„sleepä¼šé˜»å¡çº¿ç¨‹ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ twisted çš„ callLaterï¼Œæ­¤å¤„ä¸ºæ¼”ç¤ºé€»è¾‘
            return self._retry(request, exception, spider)
            
        # ç­–ç•¥ 2: é€»è¾‘/æ¸²æŸ“é”™è¯¯ -> å‡€å®¤é‡è¯• (Clean Slate)
        elif isinstance(exception, self.LOGIC_ERRORS):
            logger.warning(f"ğŸ”„ é€»è¾‘é”™è¯¯ ({exception}), è§¦å‘å‡€å®¤é‡è¯•: {request.url}")
            # å…³é”®ï¼šæ ‡è®° clean_slateï¼Œå‘Šè¯‰ Spider åœ¨ä¸‹æ¬¡è¯·æ±‚æ—¶é”€æ¯ Context
            request.meta['clean_slate'] = True 
            request.dont_filter = True
            return self._retry(request, exception, spider)

        return super().process_exception(request, exception, spider)
""")

    # =========================================================================
    # 6. æ•°æ®ç®¡é“ pipelines.py
    # =========================================================================
    write_file(f"{PROJECT_NAME}/pipelines.py", """
import logging
import time
import hashlib
from twisted.internet import threads
from itemadapter import ItemAdapter
from .models import SessionLocal, init_db
from .models.crawl_data import CrawlData
from .exceptions import DataValidationError

logger = logging.getLogger(__name__)

class DataCleaningPipeline:
    \"\"\"æ•°æ®æ¸…æ´—ä¸æ ¡éªŒå±‚\"\"\"
    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        
        # 1. å¿…å¡«é¡¹æ ¡éªŒ
        if not adapter.get('url'):
            raise DataValidationError("Drop item: Missing URL")
            
        # 2. ç”ŸæˆæŒ‡çº¹
        if not adapter.get('url_hash'):
            url = adapter.get('url')
            adapter['url_hash'] = hashlib.md5(url.encode('utf-8')).hexdigest()
            
        # 3. åŸºç¡€æ¸…æ´— (å»é™¤é¦–å°¾ç©ºæ ¼)
        for k, v in adapter.items():
            if isinstance(v, str):
                adapter[k] = v.strip()
                
        return item

class AsyncBatchWritePipeline:
    \"\"\"
    ã€å¼‚æ­¥æ‰¹é‡å†™å…¥å±‚ã€‘
    æ ¸å¿ƒæœºåˆ¶ï¼š
    1. Buffer: å†…å­˜ä¸­æš‚å­˜ Itemã€‚
    2. DeferToThread: å°†è€—æ—¶çš„ DB å†™å…¥æ“ä½œæ‰”åˆ°çº¿ç¨‹æ± ï¼Œé¿å…é˜»å¡ Scrapy çš„ Reactorã€‚
    3. Fallback: æ‰¹é‡å¤±è´¥æ—¶è‡ªåŠ¨é™çº§ã€‚
    \"\"\"
    def __init__(self, buffer_size=50, timeout=2):
        self.buffer = []
        self.buffer_size = buffer_size
        self.timeout = timeout
        self.last_flush = time.time()
        self.session_maker = SessionLocal

    @classmethod
    def from_crawler(cls, crawler):
        init_db() # ç¡®ä¿è¡¨å­˜åœ¨
        settings = crawler.settings
        return cls(
            buffer_size=settings.getint('BUFFER_THRESHOLD', 100),
            timeout=settings.getfloat('BUFFER_TIMEOUT_SEC', 2.0)
        )

    def process_item(self, item, spider):
        self.buffer.append(item)
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ° æ•°é‡é˜ˆå€¼ æˆ– æ—¶é—´é˜ˆå€¼
        if self._should_flush():
            # å¼‚æ­¥è°ƒç”¨ _flush_buffer
            df = threads.deferToThread(self._flush_buffer, list(self.buffer))
            df.addErrback(self._handle_error)
            
            # æ¸…ç©º Buffer
            self.buffer.clear()
            self.last_flush = time.time()
        return item

    def _should_flush(self):
        return (len(self.buffer) >= self.buffer_size) or \
               (time.time() - self.last_flush >= self.timeout and self.buffer)

    def _flush_buffer(self, items):
        \"\"\"åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­æ‰§è¡Œ\"\"\"
        session = self.session_maker()
        try:
            orm_objects = [
                CrawlData(
                    url=i['url'], url_hash=i['url_hash'], title=i.get('title'),
                    content=i.get('content'), source=i.get('source'),
                    meta_info=i.get('meta_info')
                ) for i in items
            ]
            # å°è¯•æ‰¹é‡å†™å…¥
            session.add_all(orm_objects)
            session.commit()
            logger.info(f"âœ… æˆåŠŸæ‰¹é‡å†™å…¥ {len(items)} æ¡æ•°æ®")
        except Exception as e:
            session.rollback()
            logger.error(f"âš ï¸ æ‰¹é‡å†™å…¥å¤±è´¥: {e}ï¼Œæ­£åœ¨å°è¯•é™çº§ä¸ºé€æ¡å†™å…¥...")
            self._fallback_single_write(session, orm_objects)
        finally:
            session.close()

    def _fallback_single_write(self, session, objects):
        \"\"\"é™çº§ç­–ç•¥ï¼šé€æ¡å†™å…¥ï¼Œéš”ç¦»è„æ•°æ®\"\"\"
        success = 0
        for obj in objects:
            try:
                session.merge(obj) # ä½¿ç”¨ merge é¿å…ä¸»é”®é‡å¤æŠ¥é”™
                session.commit()
                success += 1
            except Exception as e:
                session.rollback()
                logger.error(f"âŒ å•æ¡å†™å…¥å¤±è´¥ (Hash: {obj.url_hash}): {e}")
        logger.info(f"ğŸ†— é™çº§å†™å…¥å®Œæˆ: æˆåŠŸ {success} / æ€»æ•° {len(objects)}")

    def _handle_error(self, failure):
        logger.error(f"ğŸ”¥ å¼‚æ­¥å†™å…¥çº¿ç¨‹ä¸¥é‡å¼‚å¸¸: {failure}")

    def close_spider(self, spider):
        \"\"\"çˆ¬è™«å…³é—­æ—¶ï¼Œå¼ºåˆ¶åˆ·æ–°å‰©ä½™ Buffer\"\"\"
        if self.buffer:
            self._flush_buffer(self.buffer)
""")

    # =========================================================================
    # 7. çˆ¬è™«åŸºç±» spiders/base_spiders.py
    # =========================================================================
    write_file(f"{PROJECT_NAME}/spiders/__init__.py", "")
    write_file(f"{PROJECT_NAME}/spiders/base_spiders.py", """
import scrapy
from abc import ABC, abstractmethod

class BaseRequestSpider(scrapy.Spider, ABC):
    \"\"\"
    ã€HTTP é‡‡é›†åŸºç±»ã€‘
    é€‚ç”¨ï¼šé™æ€é¡µé¢ã€API æ¥å£ã€‚
    ç‰¹ç‚¹ï¼šæç®€ã€é«˜å¹¶å‘ã€‚
    \"\"\"
    custom_settings = {
        'CONCURRENT_REQUESTS': 32,
        'DOWNLOAD_DELAY': 0.1,
    }

    def make_request(self, url, meta=None):
        meta = meta or {}
        meta['request_type'] = 'http'
        return scrapy.Request(url, meta=meta, callback=self.parse)

    @abstractmethod
    def parse_logic(self, response):
        \"\"\"ä¸šåŠ¡é€»è¾‘ï¼Œå­ç±»å®ç°\"\"\"
        pass

    def parse(self, response):
        # HTTP æ¨¡å¼ä¸‹ï¼Œç›´æ¥å§”æ‰˜
        yield from self.parse_logic(response)


class BasePlaywrightSpider(BaseRequestSpider):
    \"\"\"
    ã€Playwright é‡‡é›†åŸºç±»ã€‘
    é€‚ç”¨ï¼šSPAã€JS åŠ¨æ€æ¸²æŸ“ã€é«˜åçˆ¬ã€‚
    ç‰¹ç‚¹ï¼šèµ„æºéš”ç¦»ã€è‡ªåŠ¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€‚
    \"\"\"
    custom_settings = {
        'CONCURRENT_REQUESTS': 4, # æµè§ˆå™¨å†…å­˜å ç”¨å¤§ï¼ŒåŠ¡å¿…é™åˆ¶å¹¶å‘
        'DOWNLOAD_DELAY': 1.0,
        'PLAYWRIGHT_LAUNCH_OPTIONS': {
            'headless': True, # Debugæ—¶æ”¹ä¸ºFalse
            'args': ['--no-sandbox', '--disable-dev-shm-usage', '--disable-gpu']
        }
    }

    def make_request(self, url, meta=None):
        meta = meta or {}
        # ç”Ÿæˆä¸Šä¸‹æ–‡ IDï¼Œç¡®ä¿åŒä¸€ä»»åŠ¡å¤ç”¨ Contextï¼Œä¸åŒä»»åŠ¡éš”ç¦»
        context_id = f"ctx_{hash(url) % 10000}"
        meta.update({
            'request_type': 'playwright',
            'playwright': True,
            'playwright_include_page': True,
            'playwright_context': context_id,
        })
        return scrapy.Request(url, meta=meta, callback=self.parse, errback=self.errback)

    async def parse(self, response):
        \"\"\"
        ç»Ÿä¸€çš„ Playwright è§£æå…¥å£ã€‚
        è´Ÿè´£å¤„ç† 'å‡€å®¤é‡è¯•' å’Œ Page å…³é—­ã€‚
        \"\"\"
        page = response.meta.get("playwright_page")
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å‡€å®¤é‡è¯• (Clean Slate Retry)
            if response.meta.get('clean_slate'):
                await self._reset_context(page)

            # âš¡ï¸ æ ¸å¿ƒï¼šåœ¨ async def ä¸­å¿…é¡»ä½¿ç”¨ async for éå†å¼‚æ­¥ç”Ÿæˆå™¨
            async for item in self.parse_logic(response, page):
                yield item

        except Exception as e:
            self.logger.error(f"Playwright è§£æå¼‚å¸¸: {e} | URL: {response.url}")
            raise e # æŠ›å‡ºç»™ä¸­é—´ä»¶è¿›è¡Œé‡è¯•åˆ¤æ–­
        finally:
            if page:
                try:
                    await page.close()
                except:
                    pass

    @abstractmethod
    async def parse_logic(self, response, page):
        \"\"\"å­ç±»å¿…é¡»å®ç°æ­¤æ–¹æ³•ï¼Œä½¿ç”¨ yield è¿”å›æ•°æ®\"\"\"
        pass

    async def errback(self, failure):
        page = failure.request.meta.get("playwright_page")
        if page:
            try:
                await page.close()
            except:
                pass
        self.logger.error(f"Playwright è¯·æ±‚å¤±è´¥: {failure.getErrorMessage()}")

    async def _reset_context(self, page):
        \"\"\"å†…éƒ¨æ–¹æ³•ï¼šæ¸…ç† Cookie å’Œ æƒé™ï¼Œæ¨¡æ‹Ÿæ–°ç”¨æˆ·\"\"\"
        if not page: return
        try:
            context = page.context
            await context.clear_cookies()
            await context.clear_permissions()
            self.logger.info("Context å·²æ¸…ç† (Cookies/Permissions)")
        except Exception as e:
            self.logger.warning(f"Context æ¸…ç†å¤±è´¥: {e}")

    async def wait_and_scroll(self, page, steps=3):
        \"\"\"å·¥å…·æ–¹æ³•ï¼šæ™ºèƒ½ç­‰å¾…ä¸æ»šåŠ¨\"\"\"
        try:
            await page.wait_for_load_state('networkidle', timeout=10000)
            for _ in range(steps):
                if page.is_closed(): break
                await page.evaluate("window.scrollBy(0, document.body.scrollHeight/3)")
                await page.wait_for_timeout(500)
        except Exception as e:
            self.logger.warning(f"æ»šåŠ¨äº¤äº’å¼‚å¸¸ (éè‡´å‘½): {e}")
""")

    # =========================================================================
    # 8. ç¤ºä¾‹çˆ¬è™« spiders/example.py
    # =========================================================================
    write_file(f"{PROJECT_NAME}/spiders/example.py", """
from .base_spiders import BaseRequestSpider, BasePlaywrightSpider
from ..items import HybridCrawlerItem

class HackerNewsSpider(BaseRequestSpider):
    \"\"\"
    ç¤ºä¾‹ 1: æ™®é€š HTTP çˆ¬è™«
    ç›®æ ‡: HackerNews é¦–é¡µåˆ—è¡¨
    \"\"\"
    name = "hn_simple"
    start_urls = ["https://news.ycombinator.com/"]

    def parse_logic(self, response):
        for row in response.css('tr.athing'):
            yield HybridCrawlerItem(
                url=row.css('.titleline a::attr(href)').get(),
                title=row.css('.titleline a::text').get(),
                source='HackerNews',
                request_type='http'
            )

class DynamicQuotesSpider(BasePlaywrightSpider):
    \"\"\"
    ç¤ºä¾‹ 2: Playwright åŠ¨æ€çˆ¬è™«
    ç›®æ ‡: Quotes to Scrape (JSç‰ˆæœ¬)
    \"\"\"
    name = "quotes_dynamic"
    start_urls = ["https://quotes.toscrape.com/js/"]

    def start_requests(self):
        for url in self.start_urls:
            # å¿…é¡»ä½¿ç”¨ self.make_request æ¥ç¡®ä¿ playwright å‚æ•°æ­£ç¡®
            yield self.make_request(url)

    async def parse_logic(self, response, page):
        # 1. æ‰§è¡Œäº¤äº’ (æ»šåŠ¨)
        await self.wait_and_scroll(page)
        
        # 2. æå–æ•°æ® (ä½¿ç”¨ Playwright API)
        quotes = await page.query_selector_all('div.quote')
        for q in quotes:
            text_el = await q.query_selector('span.text')
            text = await text_el.inner_text() if text_el else ""
            
            yield HybridCrawlerItem(
                url=response.url,
                content=text,
                source='QuotesJS',
                request_type='playwright'
            )
""")

    # =========================================================================
    # 9. æ ¸å¿ƒé…ç½® settings.py
    # =========================================================================
    write_file(f"{PROJECT_NAME}/settings.py", f"""
BOT_NAME = '{PROJECT_NAME}'
SPIDER_MODULES = ['{PROJECT_NAME}.spiders']
NEWSPIDER_MODULE = '{PROJECT_NAME}.spiders'

# =============================================================================
# æ ¸å¿ƒå¹¶å‘é…ç½®
# =============================================================================
CONCURRENT_REQUESTS = 32
DOWNLOAD_DELAY = 0

# =============================================================================
# ä¸­é—´ä»¶ç®¡é“é…ç½®
# =============================================================================
DOWNLOADER_MIDDLEWARES = {{
    '{PROJECT_NAME}.middlewares.StrategyRoutingMiddleware': 100, # è·¯ç”±ç­–ç•¥
    '{PROJECT_NAME}.middlewares.SmartRetryMiddleware': 550,      # æ™ºèƒ½é‡è¯•
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,  # ç¦ç”¨é»˜è®¤é‡è¯•
}}

ITEM_PIPELINES = {{
    '{PROJECT_NAME}.pipelines.DataCleaningPipeline': 300,        # æ¸…æ´—
    '{PROJECT_NAME}.pipelines.AsyncBatchWritePipeline': 400,     # å…¥åº“
}}

# =============================================================================
# å¼‚æ­¥å†™å…¥ç¼“å†²é…ç½®
# =============================================================================
BUFFER_THRESHOLD = 100   # ç§¯æ”’ 100 æ¡å†™å…¥ä¸€æ¬¡
BUFFER_TIMEOUT_SEC = 1.5 # æˆ–æœ€é•¿ç­‰å¾… 1.5 ç§’å†™å…¥ä¸€æ¬¡

# =============================================================================
# Playwright ä¸“ç”¨é…ç½®
# =============================================================================
DOWNLOAD_HANDLERS = {{
    "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
    "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
}}
# å¿…é¡»ä½¿ç”¨ Asyncio ååº”å †
TWISTED_REACTOR = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"

# ğŸ’¡ Debug: å°† headless æ”¹ä¸º False å¯çœ‹åˆ°æµè§ˆå™¨
PLAYWRIGHT_LAUNCH_OPTIONS = {{
    'headless': True,
    'args': ['--no-sandbox', '--disable-dev-shm-usage', '--disable-gpu'],
    'timeout': 30000, # å¯åŠ¨è¶…æ—¶æ—¶é—´
}}

LOG_LEVEL = 'INFO'
""")

    # =========================================================================
    # 10. è¿è¡Œå…¥å£ run.py (æ”¯æŒ debug å‚æ•°)
    # =========================================================================
    write_file("run.py", f"""
import os
import sys
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

# è®¾ç½® Scrapy é…ç½®æ–‡ä»¶è·¯å¾„
os.environ['SCRAPY_SETTINGS_MODULE'] = '{PROJECT_NAME}.settings'

# å¯¼å…¥çˆ¬è™« (éœ€ç¡®ä¿å·²å®‰è£… Twisted Asyncio Reactor)
from {PROJECT_NAME}.spiders.example import HackerNewsSpider, DynamicQuotesSpider

def run():
    print(">>> æ­£åœ¨å¯åŠ¨é«˜æ€§èƒ½æ··åˆçˆ¬è™«ç³»ç»Ÿ...")
    
    # ç®€å•çš„å‚æ•°è§£æï¼Œç”¨äºå¼€å¯ Debug æ¨¡å¼
    is_debug = 'debug' in sys.argv
    
    settings = get_project_settings()
    
    if is_debug:
        print(">>> ğŸ Debug æ¨¡å¼å·²å¼€å¯: æ—¥å¿—çº§åˆ« DEBUG")
        settings.set('LOG_LEVEL', 'DEBUG')
    
    process = CrawlerProcess(settings)
    
    # åœ¨è¿™é‡Œé€‰æ‹©è¦è¿è¡Œçš„çˆ¬è™«
    # process.crawl(HackerNewsSpider)
    process.crawl(DynamicQuotesSpider) 
    
    process.start()

if __name__ == '__main__':
    run()
""")

    print(f"\\n[Fixed] å¢å¼ºç‰ˆé¡¹ç›®ç”Ÿæˆå®Œæ¯•ï¼åŒ…å«è¯¦ç»†æ–‡æ¡£ä¸è°ƒè¯•æŒ‡å—ã€‚")
    print(f"è¯·é˜…è¯» {PROJECT_NAME}/README.md å¼€å§‹ä½¿ç”¨ã€‚")

if __name__ == "__main__":
    main()