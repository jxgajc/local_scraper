## 实现计划

### 1. 创建统一补采脚本
**文件位置**：`/Users/jcagito/xxl_job/xxl-job/myspider/local_scraper/hybrid_crawler/recrawl_checker.py`

### 2. 脚本结构设计

#### 2.1 基类 `BaseRecrawler`
```python
class BaseRecrawler:
    def __init__(self):
        self.db_session = SessionLocal()
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def get_existing_ids(self):
        """从数据库获取已采集的唯一标识"""
        pass
    
    def fetch_api_total(self):
        """从官网API获取总数"""
        pass
    
    def find_missing(self):
        """找出缺失的唯一标识"""
        pass
    
    def recrawl(self):
        """执行补采"""
        pass
    
    def close(self):
        """关闭数据库连接"""
        self.db_session.close()
```

#### 2.2 各爬虫补采类
```python
class FujianRecrawler(BaseRecrawler):
    def __init__(self):
        super().__init__()
        self.table_name = "drug_hospital_fujian"
        self.unique_id = "ext_code"
        self.list_api = "https://open.ybj.fujian.gov.cn:10013/tps-local/web/tender/plus/item-cfg-info/list"
        self.detail_api = "https://open.ybj.fujian.gov.cn:10013/tps-local/web/trans/api/open/v2/queryHospital"

# 其他爬虫补采类类似...
```

#### 2.3 主函数与命令行支持
```python
def main():
    parser = argparse.ArgumentParser(description="爬虫数据补采工具")
    parser.add_argument("--check", action="store_true", help="检查所有爬虫缺失情况")
    parser.add_argument("--recrawl", type=str, help="执行特定爬虫补采")
    args = parser.parse_args()
    
    if args.check:
        check_all_spiders()
    elif args.recrawl:
        recrawl_spider(args.recrawl)
    else:
        parser.print_help()
```

### 3. 关键功能实现

#### 3.1 数据库操作
- 使用现有的SQLAlchemy SessionLocal获取数据库连接
- 实现get_existing_ids()方法，从各数据表中获取已有的唯一标识

#### 3.2 API请求处理
- 使用requests库发送HTTP请求
- 处理分页逻辑，获取所有API数据
- 解析响应，提取唯一标识

#### 3.3 缺失数据检测
- 对比数据库中已有的唯一标识和API返回的唯一标识
- 找出缺失的唯一标识

#### 3.4 补采逻辑
- 对缺失的数据，重新执行采集流程
- 复用现有爬虫的解析逻辑

#### 3.5 报告生成
- 生成补采报告，包含缺失数量、补采成功数量等信息

### 4. 各爬虫具体实现

根据文档中的API信息，为每个爬虫实现具体的补采逻辑：

| 爬虫 | 数据表 | 唯一标识 | 列表API | 详情API |
|------|--------|---------|--------|--------|
| fujian | drug_hospital_fujian | ext_code | POST /item-cfg-info/list | POST /queryHospital |
| hainan | drug_shop_hainan | drug_code | GET /getDrugStore | GET /getDrugStoreDetl |
| guangdong | drug_hospital_guangdong | drug_code | POST /queryPubonlnPage | POST /getPurcHospitalInfoListNew |
| tianjin | drug_hospital_tianjin | med_id | POST /guideGetMedList | POST /guideGetHosp |
| liaoning | drug_hospital_liaoning | goodscode | POST /GetYPYYCG | 无 |
| ningxia | drug_hospital_ningxia | procurecatalogId | POST /getRecentPurchaseDetailData | POST /getDrugDetailDate |
| hebei | drug_hospital_hebei | prodCode | GET /queryPubonlnDrudInfoList | GET /queryProcurementMedinsList |

### 5. 运行方式
```bash
# 检查所有爬虫缺失情况
python recrawl_checker.py --check

# 执行特定爬虫补采
python recrawl_checker.py --recrawl fujian
python recrawl_checker.py --recrawl guangdong
```

### 6. 验证方式
- 运行 `--check` 模式查看各爬虫缺失情况
- 运行前后对比各数据表记录数
- 生成补采报告

### 7. 依赖项
- requests
- sqlalchemy
- argparse
- logging

## 预期成果

1. 成功创建补采脚本，支持所有7个爬虫的补采
2. 能够准确检测各爬虫的缺失数据
3. 能够执行补采并将数据存入数据库
4. 生成清晰的补采报告
5. 支持命令行参数，方便使用