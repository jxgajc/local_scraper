#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆå§‹åŒ–_testè¡¨è„šæœ¬ (ä¼˜åŒ–ç‰ˆ)
åŠŸèƒ½ï¼šæ£€æŸ¥å„ä¸ªçˆ¬è™«å¯¹åº”çš„_testè¡¨æ˜¯å¦ä¸ºç©ºï¼Œå¦‚æœä¸ºç©ºï¼Œåˆ™ä»æ­£å¼è¡¨åŒæ­¥æ•°æ®ã€‚
ä¼˜åŒ–ç‚¹ï¼šé‡‡ç”¨åˆ†æ‰¹æ¬¡åŒæ­¥ (Batch Processing)ï¼Œé¿å…ä¸€æ¬¡æ€§å…¨é‡æ’å…¥å¯¼è‡´æ•°æ®åº“é”æ­»ã€‚
"""

import os
import sys
import time
import logging
from sqlalchemy import text, func

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

# å°è¯•å¯¼å…¥ recrawl_checker
try:
    from recrawl_checker import SPIDER_MAPPING
except ImportError:
    sys.path.append(os.path.dirname(current_dir))
    from recrawl_checker import SPIDER_MAPPING

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("init_test_tables")

BATCH_SIZE = 5000  # æ¯æ¬¡åŒæ­¥çš„è¡Œæ•°
SLEEP_INTERVAL = 0.1  # æ¯æ‰¹æ¬¡é—´éš”æ—¶é—´(ç§’)

def init_tables():
    logger.info("ğŸš€ å¼€å§‹æ£€æŸ¥å¹¶åˆå§‹åŒ–_testè¡¨ (Batch Mode)...")
    
    for spider_name, crawler_class in SPIDER_MAPPING.items():
        crawler = None
        try:
            crawler = crawler_class()
            test_table = crawler.table_name
            
            if not test_table.endswith('_test'):
                logger.warning(f"[{spider_name}] è¡¨å {test_table} ä¸ä»¥ _test ç»“å°¾ï¼Œè·³è¿‡åˆå§‹åŒ–")
                continue
                
            prod_table = test_table.replace('_test', '')
            
            # 1. æ£€æŸ¥_testè¡¨æ˜¯å¦ä¸ºç©º
            count_sql = text(f"SELECT COUNT(1) FROM {test_table}")
            test_count = crawler.db_session.execute(count_sql).scalar()
            
            logger.info(f"[{spider_name}] {test_table} å½“å‰æ•°æ®é‡: {test_count}")
            
            if test_count == 0:
                logger.info(f"[{spider_name}] _testè¡¨ä¸ºç©ºï¼Œå‡†å¤‡ä» {prod_table} åŒæ­¥æ•°æ®...")
                
                # 2. è·å–æºè¡¨ ID èŒƒå›´
                range_sql = text(f"SELECT MIN(id), MAX(id), COUNT(1) FROM {prod_table}")
                min_id, max_id, total_rows = crawler.db_session.execute(range_sql).fetchone()
                
                if not total_rows or total_rows == 0:
                    logger.info(f"[{spider_name}] æºè¡¨ {prod_table} ä¸ºç©ºï¼Œæ— éœ€åŒæ­¥")
                    continue
                    
                if min_id is None or max_id is None:
                    logger.warning(f"[{spider_name}] æºè¡¨ {prod_table} æ²¡æœ‰æœ‰æ•ˆçš„ ID èŒƒå›´ï¼Œå°è¯•å…¨é‡åŒæ­¥...")
                    # é™çº§æ–¹æ¡ˆï¼šå…¨é‡åŒæ­¥ (é’ˆå¯¹æ—  ID çš„è¡¨ï¼Œè™½ç„¶ä¸å¤ªå¯èƒ½)
                    sync_sql = text(f"INSERT IGNORE INTO {test_table} SELECT * FROM {prod_table}")
                    crawler.db_session.execute(sync_sql)
                    crawler.db_session.commit()
                    continue

                logger.info(f"[{spider_name}] æºæ•°æ®æ€»é‡: {total_rows}, IDèŒƒå›´: {min_id} -> {max_id}")
                
                # 3. åˆ†æ‰¹æ¬¡åŒæ­¥
                processed_count = 0
                current_id = min_id
                
                while current_id <= max_id:
                    next_id = current_id + BATCH_SIZE
                    
                    # æ„é€ æ‰¹æ¬¡æ’å…¥ SQL
                    batch_sql = text(f"""
                        INSERT IGNORE INTO {test_table} 
                        SELECT * FROM {prod_table} 
                        WHERE id >= :start_id AND id < :end_id
                    """)
                    
                    result = crawler.db_session.execute(batch_sql, {"start_id": current_id, "end_id": next_id})
                    crawler.db_session.commit() # æ¯æ¬¡æäº¤ï¼Œé‡Šæ”¾é”
                    
                    rows_affected = result.rowcount
                    processed_count += rows_affected
                    
                    # è¿›åº¦æ—¥å¿—
                    progress = min(100, int((current_id - min_id) / (max_id - min_id) * 100))
                    if current_id % (BATCH_SIZE * 5) == 0 or rows_affected > 0:
                         logger.info(f"[{spider_name}] è¿›åº¦ {progress}% | åŒæ­¥æ‰¹æ¬¡ {current_id}-{next_id} | æœ¬æ¬¡å†™å…¥: {rows_affected}")
                    
                    current_id = next_id
                    time.sleep(SLEEP_INTERVAL) # ä¼‘æ¯ä¸€ä¸‹ï¼Œé˜²æ­¢æ•°æ®åº“é«˜è´Ÿè½½
                
                logger.info(f"[{spider_name}] âœ… åŒæ­¥å®Œæˆï¼Œå…±å†™å…¥: {processed_count} æ¡")
                
                # æœ€ç»ˆç¡®è®¤
                final_count = crawler.db_session.execute(count_sql).scalar()
                logger.info(f"[{spider_name}] {test_table} æœ€ç»ˆæ•°æ®é‡: {final_count}")
                
            else:
                logger.info(f"[{spider_name}] _testè¡¨å·²æœ‰æ•°æ®ï¼Œè·³è¿‡åŒæ­¥")
                
        except Exception as e:
            logger.error(f"[{spider_name}] âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            if crawler:
                crawler.db_session.rollback()
        finally:
            if crawler:
                crawler.close()

if __name__ == "__main__":
    init_tables()
