import sys
import os
import asyncio
import time
import logging
from typing import Callable

# Add project root to sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from hybrid_crawler.recrawl.manager import RecrawlManager
from hybrid_crawler.recrawl.registry import get_adapter
from hybrid_crawler.models import SessionLocal
from sqlalchemy import text

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def run_test(spider_name: str):
    logger.info(f"=== Starting Recrawl Debug for {spider_name} ===")
    
    # 1. Setup Timeout Check
    start_time = time.time()
    def stop_check() -> bool:
        if time.time() - start_time > 120: # 2 minutes
            logger.warning("â° Timeout reached (120s)!")
            return True
        return False

    # 2. Get Adapter
    adapter = get_adapter(spider_name)
    if not adapter:
        logger.error(f"Adapter not found for {spider_name}")
        return

    # 3. Find Missing
    logger.info("ğŸ” Step 1: Finding missing data...")
    try:
        # ä½¿ç”¨ wait_for å¢åŠ ä¸€å±‚è¶…æ—¶ä¿éšœ
        missing = await asyncio.wait_for(RecrawlManager.find_missing(spider_name, stop_check), timeout=60)
        logger.info(f"Found {len(missing)} missing items.")
    except asyncio.TimeoutError:
        logger.error("Find missing timed out!")
        missing = {}
    except Exception as e:
        logger.error(f"Find missing failed: {e}")
        missing = {}

    target_ids = list(missing.keys())[:5]
    subset = {}
    
    if target_ids:
        logger.info(f"ğŸ¯ Step 2: Triggering Recrawl for {len(target_ids)} items (Subset)...")
        subset = {k: missing[k] for k in target_ids}
    else:
        logger.info("âš ï¸ No missing data found.")
        logger.info("ğŸ› ï¸ Force Check: Attempting to verify DB connectivity and existence...")
        # å°è¯•æŸ¥è¯¢æ•°æ®åº“ä¸­è¯¥çœä»½çš„è¡¨æ˜¯å¦å­˜åœ¨æ•°æ®
        # ç”±äºæ— æ³•ç›´æ¥è·å– Modelï¼Œæˆ‘ä»¬å°è¯•çŒœæµ‹è¡¨åæˆ–è·³è¿‡
        session = SessionLocal()
        try:
            # ç®€å•éªŒè¯ DB è¿æ¥
            session.execute(text("SELECT 1"))
            logger.info("âœ… DB Connection OK")
            
            # å¦‚æœèƒ½çŸ¥é“è¡¨åæœ€å¥½ï¼Œä¸çŸ¥é“åˆ™æç¤ºç”¨æˆ·æ‰‹åŠ¨æ£€æŸ¥
            logger.info("Please verify database records manually.")
            
        except Exception as e:
            logger.error(f"âŒ DB Check Failed: {e}")
        finally:
            session.close()
            
        # æ„é€  Mock Data è¿›è¡Œè¡¥é‡‡æµ‹è¯• (å¦‚æœæ”¯æŒ)
        # è¿™é‡Œçš„ Mock éœ€è¦çœŸå®çš„ ID æ ¼å¼ï¼Œæ¯”è¾ƒå›°éš¾ï¼Œæ•…ä»…åœ¨æœ‰çœŸå®ç¼ºå¤±æ—¶æ‰§è¡Œ Recrawl
        logger.info("Skipping Recrawl execution as no valid IDs available.")

    if subset:
        try:
            count = await asyncio.wait_for(
                RecrawlManager.recrawl(spider_name, missing_ids=subset, stop_check=stop_check),
                timeout=120
            )
            logger.info(f"âœ… Recrawl finished. Processed: {count}")
            
        except asyncio.TimeoutError:
            logger.error("âŒ Recrawl timed out!")
        except Exception as e:
            logger.error(f"âŒ Recrawl failed: {e}")

    logger.info(f"=== Finished in {time.time() - start_time:.2f}s ===")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python debug_recrawl_job.py <spider_name>")
        sys.exit(1)
        
    spider = sys.argv[1]
    asyncio.run(run_test(spider))
