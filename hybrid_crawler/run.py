import os
import sys

from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

# è®¾ç½® Scrapy é…ç½®æ–‡ä»¶è·¯å¾„
os.environ['SCRAPY_SETTINGS_MODULE'] = 'hybrid_crawler.settings'

# å¯¼å…¥çˆ¬è™« (éœ€ç¡®ä¿å·²å®‰è£… Twisted Asyncio Reactor)
from hybrid_crawler.spiders.example import HackerNewsSpider, DynamicQuotesSpider

from hybrid_crawler.spiders.fujian_drug_store import FujianDrugSpider
from hybrid_crawler.spiders.hainan_drug_store import HainanDrugSpider
from hybrid_crawler.spiders.hebei_drug_store import HebeiDrugSpider
from hybrid_crawler.spiders.liaoning_drug_store import LiaoningDrugSpider
from hybrid_crawler.spiders.ningxia_drug_store import NingxiaDrugSpider
from hybrid_crawler.spiders.nhsa_drug_spider import NhsaDrugSpider
from hybrid_crawler.spiders.shandong_drug_store import ShandongDrugSpider
from hybrid_crawler.spiders.guangdong_drug_store import GuangdongDrugSpider
from hybrid_crawler.spiders.tianjin_drug_store import TianjinDrugSpider
# çˆ¬è™«æ˜ å°„è¡¨
SPIDER_MAP = {
    # 'hn_simple': HackerNewsSpider,
    # 'quotes_dynamic': DynamicQuotesSpider,

    'fujian_drug_store': FujianDrugSpider,
    'hainan_drug_store': HainanDrugSpider,
    'hebei_drug_store': HebeiDrugSpider,
    'liaoning_drug_store': LiaoningDrugSpider,
    'ningxia_drug_store': NingxiaDrugSpider,
    'guangdong_drug_spider': GuangdongDrugSpider,
    'tianjin_drug_spider': TianjinDrugSpider,
    # 'nhsa_drug_spider': NhsaDrugSpider,

    # 'shandong_drug_store': ShandongDrugSpider,    
}

def run_spider(spider_cls, spider_name, is_debug):
    """åœ¨å•ä¸ªçº¿ç¨‹ä¸­è¿è¡ŒæŒ‡å®šçš„çˆ¬è™«"""
    settings = get_project_settings()
    
    if is_debug:
        settings.set('LOG_LEVEL', 'DEBUG')
    
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # ä¸ºæ¯ä¸ªçˆ¬è™«è®¾ç½®å•ç‹¬çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_dir = os.path.join(script_dir, 'log')
    log_file = os.path.join(log_dir, f'{spider_name}.log')
    
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs(log_dir, exist_ok=True)
    
    settings.set('LOG_FILE', log_file)
    
    process = CrawlerProcess(settings)
    process.crawl(spider_cls)
    process.start(stop_after_crawl=True)


def run():
    print(">>> æ­£åœ¨å¯åŠ¨æ··åˆçˆ¬è™«ç³»ç»Ÿ...")
    
    # ç®€å•çš„å‚æ•°è§£æï¼Œç”¨äºå¼€å¯ Debug æ¨¡å¼å’ŒæŒ‡å®šçˆ¬è™«
    is_debug = 'debug' in sys.argv
    
    # è·å–è¦è¿è¡Œçš„çˆ¬è™«åç§°
    spider_name = None
    for arg in sys.argv[1:]:
        if arg != 'debug' and arg in SPIDER_MAP:
            spider_name = arg
            break
    
    if is_debug:
        print(">>> ğŸ Debug æ¨¡å¼å·²å¼€å¯: æ—¥å¿—çº§åˆ« DEBUG")
    
    # è·å–é¡¹ç›®è®¾ç½®
    settings = get_project_settings()
    
    if is_debug:
        settings.set('LOG_LEVEL', 'DEBUG')
    
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    log_dir = os.path.join(script_dir, 'log')
    os.makedirs(log_dir, exist_ok=True)
    
    # åˆ›å»ºå•ä¸ª CrawlerProcess å®ä¾‹
    process = CrawlerProcess(settings)
    
    if spider_name:
        # è¿è¡ŒæŒ‡å®šçš„çˆ¬è™«
        print(f">>> æ­£åœ¨è¿è¡Œçˆ¬è™«: {spider_name}")
        spider_cls = SPIDER_MAP[spider_name]
        # ä¸ºå•ä¸ªçˆ¬è™«è®¾ç½®æ—¥å¿—æ–‡ä»¶
        settings.set('LOG_FILE', os.path.join(log_dir, f'{spider_name}.log'))
        process.crawl(spider_cls)
    else:
        # æ·»åŠ æ‰€æœ‰çˆ¬è™«åˆ°åŒä¸€ä¸ªè¿›ç¨‹
        print(">>> æ­£åœ¨æ·»åŠ æ‰€æœ‰çˆ¬è™«åˆ°è¿è¡Œé˜Ÿåˆ—")
        
        for name, spider_cls in SPIDER_MAP.items():
            print(f">>> æ·»åŠ çˆ¬è™«: {name}")
            # ä¸ºæ¯ä¸ªçˆ¬è™«è®¾ç½®ç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶
            # æ³¨æ„: å½“è¿è¡Œå¤šä¸ªçˆ¬è™«æ—¶ï¼Œæ—¥å¿—ä¼šåˆå¹¶åˆ°ä¸€ä¸ªæ–‡ä»¶
            # å¦‚æœéœ€è¦åˆ†ç¦»æ—¥å¿—ï¼Œéœ€è¦æ›´å¤æ‚çš„é…ç½®
            process.crawl(spider_cls)
    
    # å¯åŠ¨è¿›ç¨‹ï¼Œæ‰€æœ‰çˆ¬è™«å°†åŒæ—¶è¿è¡Œ
    print(">>> æ­£åœ¨å¯åŠ¨æ‰€æœ‰çˆ¬è™«...")
    process.start(stop_after_crawl=True)
    
    print(">>> æ‰€æœ‰çˆ¬è™«è¿è¡Œå®Œæˆ")

if __name__ == '__main__':
    run()