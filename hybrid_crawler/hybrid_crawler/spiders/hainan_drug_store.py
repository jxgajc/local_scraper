import scrapy
import json
import uuid
from urllib.parse import urlencode
from ..models.hainan_drug import HainanDrugItem
from ..utils.logger_utils import get_spider_logger
import pandas as pd
import os

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
# æ„å»ºExcelæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
excel_path = os.path.join(script_dir, "../../å…³é”®å­—é‡‡é›†(2).xlsx")

class HainanDrugSpider(scrapy.Spider):
    """
    æµ·å—çœåŒ»ä¿æœåŠ¡å¹³å° - è¯å“é—¨åº—æŸ¥è¯¢çˆ¬è™«
    Target: https://ybj.hainan.gov.cn
    """
    name = "hainan_drug_spider"
    
    # API Endpoints
    list_api_base = "https://ybj.hainan.gov.cn/tps-local/local/web/std/drugStore/getDrugStore"
    detail_api_base = "https://ybj.hainan.gov.cn/tps-local/local/web/std/drugStore/getDrugStoreDetl"
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.spider_log = get_spider_logger(self.name)
        self.crawl_id = str(uuid.uuid4())
        
        # åŠ è½½å…³é”®è¯
        try:
            df_name = pd.read_excel(excel_path)
            self.keywords = df_name.loc[:, "é‡‡é›†å…³é”®å­—"].to_list()
            self.spider_log.info(f"ğŸš€ çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œcrawl_id: {self.crawl_id}ï¼ŒåŠ è½½å…³é”®è¯: {len(self.keywords)} ä¸ª")
        except Exception as e:
            self.spider_log.error(f"âŒ å…³é”®è¯æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            self.keywords = []

    custom_settings = {
        'CONCURRENT_REQUESTS': 5,
        'DOWNLOAD_DELAY': 3,
        'DEFAULT_REQUEST_HEADERS': {
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Connection': 'keep-alive',
            'Referer': 'https://ybj.hainan.gov.cn/tps-local/b/',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',
            'sec-ch-ua': '"Google Chrome";v="143", "Chromium";v="143", "Not A(Brand";v="24"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"'
        },
        'ITEM_PIPELINES': {
            'hybrid_crawler.pipelines.DataCleaningPipeline': 300,        # æ¸…æ´—
            'hybrid_crawler.pipelines.CrawlStatusPipeline': 350,         # çŠ¶æ€ç›‘æ§ (æ–°å¢)
            'hybrid_crawler.pipelines.HainanDrugPipeline': 400,          # å…¥åº“
        }
    }

    def start_requests(self):
        """éå†å…³é”®è¯ï¼Œå‘èµ·åˆ—è¡¨è¯·æ±‚"""
        self.spider_log.info(f"ğŸ“‹ å¼€å§‹é‡‡é›†ï¼Œå…± {len(self.keywords)} ä¸ªå…³é”®è¯")
        
        for keyword in self.keywords:
            params = {
                'current': 1,
                'size': 500,
                'prodName': keyword
            }
            url = f"{self.list_api_base}?{urlencode(params)}"
            
            self.spider_log.info(f"ğŸ” æ­£åœ¨é‡‡é›†å…³é”®è¯: {keyword}")

            # ä¸ŠæŠ¥å¼€å§‹é‡‡é›†çŠ¶æ€
            # yield {
            #     '_status_': True,
            #     'crawl_id': self.crawl_id,
            #     'stage': 'start_requests',
            #     'page_no': 1,
            #     'params': params,
            #     'api_url': self.list_api_base,
            #     'reference_id': keyword,
            #     'success': True
            # }
            
            yield scrapy.Request(
                url=url,
                callback=self.parse_list,
                meta={
                    'keyword': keyword, 
                    'current_page': 1, 
                    'page_size': 500, 
                    'crawl_id': self.crawl_id
                }
            )

    def parse_list(self, response):
        """è§£æè¯å“åˆ—è¡¨å¹¶å¤„ç†ç¿»é¡µ"""
        page_crawl_id = str(uuid.uuid4())
        keyword = response.meta['keyword']
        current_page = response.meta['current_page']
        parent_crawl_id = response.meta['crawl_id']
        
        try:
            res_json = json.loads(response.text)
            if res_json.get("code") != 0:
                error_msg = res_json.get('msg', 'Unknown Error')
                self.spider_log.error(f"âŒ å…³é”®è¯ [{keyword}] åˆ—è¡¨APIé”™è¯¯ (Page {current_page}): {error_msg}")
                
                # ä¸ŠæŠ¥å¤±è´¥çŠ¶æ€
                yield {
                    '_status_': True,
                    'crawl_id': page_crawl_id,
                    'stage': 'list_page',
                    'page_no': current_page,
                    'params': {'prodName': keyword, 'current': current_page},
                    'api_url': self.list_api_base,
                    'reference_id': keyword,
                    'success': False,
                    'error_message': error_msg,
                    'parent_crawl_id': parent_crawl_id
                }
                return

            data = res_json.get("data", {})
            records = data.get("records", [])
            total_pages = data.get("pages", 0)
            page_size = data.get("size", 500)

            self.spider_log.info(f"ğŸ“„ å…³é”®è¯ [{keyword}] åˆ—è¡¨é¡µé¢ [{current_page}/{total_pages}] - å‘ç° {len(records)} æ¡è¯å“è®°å½•")
            
            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current_page,
                'total_pages': total_pages,
                'page_size': page_size,
                'items_found': len(records),
                'params': {'prodName': keyword, 'current': current_page},
                'api_url': self.list_api_base,
                'reference_id': keyword,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            item_count = 0
            for record in records:
                # 1. æå–è¯å“åŸºç¡€ä¿¡æ¯
                base_info = {
                    'drug_code': record.get('prodCode'),
                    'prod_name': record.get('prodName'),
                    'dosform': record.get('dosform'),
                    'spec': record.get('prodSpec'),
                    'pac': record.get('prodPac'),
                    'conv_rat': record.get('convrat'),
                    'prod_entp': record.get('prodentpName'),
                    'dcla_entp': record.get('dclaEntpName'),
                    'aprv_no': record.get('aprvno'),
                    'source_data': json.dumps(record, ensure_ascii=False)
                }

                # 2. å¦‚æœæœ‰è¯å“ç¼–ç ï¼ŒæŸ¥è¯¢é—¨åº—è¯¦æƒ…
                drug_code = record.get('prodCode')
                if drug_code:
                    detail_params = {
                        'current': 1,
                        'size': 20,
                        'drugCode': drug_code
                    }
                    detail_url = f"{self.detail_api_base}?{urlencode(detail_params)}"
                    
                    yield scrapy.Request(
                        url=detail_url,
                        callback=self.parse_detail,
                        meta={
                            'base_info': base_info,
                            'current_page': 1,
                            'page_size': 20,
                            'drug_code': drug_code,
                            'parent_crawl_id': page_crawl_id
                        }
                    )
                    item_count += 1
                else:
                    # æ— ç¼–ç ï¼Œç›´æ¥ä¿å­˜
                    item = HainanDrugItem()
                    item.update(base_info)
                    item['has_shop_record'] = False
                    item.generate_md5_id()
                    yield item
                    item_count += 1

            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€ï¼Œè®°å½•æˆåŠŸå­˜å‚¨çš„æ¡æ•°ï¼ˆåŒ…å«è§¦å‘çš„å­è¯·æ±‚ï¼‰
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current_page,
                'total_pages': total_pages,
                'items_found': len(records),
                'items_stored': item_count,
                'params': {'prodName': keyword, 'current': current_page},
                'api_url': self.list_api_base,
                'reference_id': keyword,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            # 3. åˆ—è¡¨é¡µç¿»é¡µ
            if current_page < total_pages:
                self.spider_log.info(f"ğŸ”„ å‡†å¤‡é‡‡é›†å…³é”®è¯ [{keyword}] ä¸‹ä¸€é¡µåˆ—è¡¨ [{current_page + 1}/{total_pages}]")
                next_page = current_page + 1
                params = {
                    'current': next_page,
                    'size': page_size,
                    'prodName': keyword
                }
                url = f"{self.list_api_base}?{urlencode(params)}"
                yield scrapy.Request(
                    url=url,
                    callback=self.parse_list,
                    meta={
                        'keyword': keyword,
                        'current_page': next_page,
                        'page_size': page_size,
                        'crawl_id': parent_crawl_id
                    }
                )
            else:
                self.spider_log.info(f"âœ… å…³é”®è¯ [{keyword}] åˆ—è¡¨é‡‡é›†å®Œæˆï¼Œå…± {total_pages} é¡µ")

        except Exception as e:
            self.spider_log.error(f"âŒ è§£æå…³é”®è¯ [{keyword}] åˆ—è¡¨å¤±è´¥ (Page {current_page}): {e}", exc_info=True)
            
            # ä¸ŠæŠ¥å¼‚å¸¸çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current_page,
                'params': {'prodName': keyword, 'current': current_page},
                'api_url': self.list_api_base,
                'reference_id': keyword,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }

    def parse_detail(self, response):
        """è§£æé—¨åº—/åŒ»é™¢è¯¦æƒ…å¹¶å¤„ç†ç¿»é¡µ"""
        base_info = response.meta['base_info']
        current_page = response.meta['current_page']
        drug_code = response.meta['drug_code']
        parent_crawl_id = response.meta['parent_crawl_id']
        prod_name = base_info.get('prod_name', 'Unknown')
        detail_crawl_id = str(uuid.uuid4())

        try:
            res_json = json.loads(response.text)
            if res_json.get("code") != 0:
                error_msg = res_json.get('msg', 'Unknown Error')
                self.spider_log.warning(f"âš ï¸ è¯å“ [{prod_name}] è¯¦æƒ…APIé”™è¯¯ (Page {current_page}): {error_msg}")
                
                # ä¸ŠæŠ¥å¤±è´¥çŠ¶æ€
                yield {
                    '_status_': True,
                    'crawl_id': detail_crawl_id,
                    'stage': 'detail_page',
                    'page_no': current_page,
                    'params': {'drugCode': drug_code, 'current': current_page},
                    'api_url': self.detail_api_base,
                    'reference_id': drug_code,
                    'success': False,
                    'error_message': error_msg,
                    'parent_crawl_id': parent_crawl_id
                }
                return

            data = res_json.get("data", {})
            records = data.get("records", [])
            total_pages = data.get("pages", 0)
            page_size = data.get("size", 20)

            self.spider_log.info(f"ğŸ¥ è¯å“ [{prod_name}] è¯¦æƒ…é¡µé¢ [{current_page}/{total_pages}] - å‘ç° {len(records)} æ¡é—¨åº—è®°å½•")
            
            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': detail_crawl_id,
                'stage': 'detail_page',
                'page_no': current_page,
                'total_pages': total_pages,
                'page_size': page_size,
                'items_found': len(records),
                'params': {'drugCode': drug_code, 'current': current_page},
                'api_url': self.detail_api_base,
                'reference_id': drug_code,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            item_count = 0
            if records:
                for shop in records:
                    item = HainanDrugItem()
                    item.update(base_info)
                    
                    # æ³¨å…¥é—¨åº—ä¿¡æ¯
                    item['has_shop_record'] = True
                    item['shop_name'] = shop.get('medinsName')
                    item['shop_code'] = shop.get('medinsCode')
                    item['shop_type_memo'] = shop.get('memo')
                    item['price'] = shop.get('pric')
                    item['inventory'] = shop.get('invCnt')
                    item['update_time'] = shop.get('invChgTime')
                    item['hilist_name'] = shop.get('fixmedinsHilistName')
                    
                    # æ›´æ–° source_data åŒ…å«ä¸¤éƒ¨åˆ†ä¿¡æ¯
                    full_source = {
                        "drug_info": json.loads(base_info['source_data']),
                        "shop_info": shop
                    }
                    item['source_data'] = json.dumps(full_source, ensure_ascii=False)
                    
                    item.generate_md5_id()
                    yield item
                    item_count += 1
                
                # è¯¦æƒ…é¡µç¿»é¡µ
                if current_page < total_pages:
                    self.spider_log.info(f"ğŸ”„ å‡†å¤‡é‡‡é›†è¯å“ [{prod_name}] ä¸‹ä¸€é¡µè¯¦æƒ… [{current_page + 1}/{total_pages}]")
                    next_page = current_page + 1
                    params = {
                        'current': next_page,
                        'size': page_size,
                        'drugCode': drug_code
                    }
                    url = f"{self.detail_api_base}?{urlencode(params)}"
                    
                    yield scrapy.Request(
                        url=url,
                        callback=self.parse_detail,
                        meta={
                            'base_info': base_info,
                            'current_page': next_page,
                            'page_size': page_size,
                            'drug_code': drug_code,
                            'parent_crawl_id': parent_crawl_id
                        }
                    )
            elif current_page == 1:
                # ç¬¬ä¸€é¡µå°±æ²¡æ•°æ®ï¼Œè¯´æ˜è¯¥è¯æ²¡åº“å­˜è®°å½•ï¼Œä¿å­˜ä¸€æ¡åŸºç¡€ä¿¡æ¯
                self.spider_log.info(f"ğŸ“‹ è¯å“ [{prod_name}] æ²¡æœ‰é—¨åº—è®°å½•")
                
                item = HainanDrugItem()
                item.update(base_info)
                item['has_shop_record'] = False
                item.generate_md5_id()
                yield item
                item_count += 1

            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€ï¼Œè®°å½•æˆåŠŸå­˜å‚¨çš„æ¡æ•°
            yield {
                '_status_': True,
                'crawl_id': detail_crawl_id,
                'stage': 'detail_page',
                'page_no': current_page,
                'total_pages': total_pages,
                'items_found': len(records),
                'items_stored': item_count,
                'params': {'drugCode': drug_code, 'current': current_page},
                'api_url': self.detail_api_base,
                'reference_id': drug_code,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

        except Exception as e:
            self.spider_log.error(f"âŒ è§£æè¯å“ [{prod_name}] è¯¦æƒ…å¤±è´¥ (Page {current_page}): {e}", exc_info=True)
            
            # ä¸ŠæŠ¥å¼‚å¸¸çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': detail_crawl_id,
                'stage': 'detail_page',
                'page_no': current_page,
                'params': {'drugCode': drug_code, 'current': current_page},
                'api_url': self.detail_api_base,
                'reference_id': drug_code,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }