import scrapy
import json
import uuid
from urllib.parse import urlencode
from ..models.hainan_drug import HainanDrugItem
from ..utils.logger_utils import get_spider_logger
import pandas as pd
import os
from .mixins import SpiderStatusMixin

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
# æ„å»ºExcelæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
excel_path = os.path.join(script_dir, "../../å…³é”®å­—é‡‡é›†(2).xlsx")

class HainanDrugSpider(SpiderStatusMixin, scrapy.Spider):
    """
    æµ·å—çœåŒ»ä¿æœåŠ¡å¹³å° - è¯å“é—¨åº—æŸ¥è¯¢çˆ¬è™«
    Target: https://ybj.hainan.gov.cn
    """
    name = "hainan_drug_spider"
    
    # API Endpoints
    list_api_base = "https://ybj.hainan.gov.cn/tps-local/local/web/std/drugStore/getDrugStore"
    detail_api_base = "https://ybj.hainan.gov.cn/tps-local/local/web/std/drugStore/getDrugStoreDetl"
    
    def __init__(self, recrawl_ids=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.spider_log = get_spider_logger(self.name)
        self.crawl_id = str(uuid.uuid4())

        # è¡¥é‡‡æ¨¡å¼ï¼šåªé‡‡é›†æŒ‡å®šçš„ drug_code
        self.recrawl_ids = set(recrawl_ids.split(',')) if recrawl_ids else None
        self.recrawl_mode = self.recrawl_ids is not None

        # åŠ è½½å…³é”®è¯
        try:
            df_name = pd.read_excel(excel_path)
            self.keywords = df_name.loc[:, "é‡‡é›†å…³é”®å­—"].to_list()
            mode_str = f"è¡¥é‡‡æ¨¡å¼ï¼Œç›®æ ‡ {len(self.recrawl_ids)} æ¡" if self.recrawl_mode else "å…¨é‡é‡‡é›†"
            self.spider_log.info(f"ğŸš€ çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œcrawl_id: {self.crawl_id}ï¼Œæ¨¡å¼: {mode_str}ï¼ŒåŠ è½½å…³é”®è¯: {len(self.keywords)} ä¸ª")
        except Exception as e:
            self.spider_log.error(f"âŒ å…³é”®è¯æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            self.keywords = []

    custom_settings = {
        'CONCURRENT_REQUESTS': 5,
        'DOWNLOAD_DELAY': 3,
        'DEFAULT_REQUEST_HEADERS': {
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Connection': 'keep-alive',
            'Referer': 'https://ybj.hainan.gov.cn/tps-local/b/',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            # 'User-Agent': Handled by RandomUserAgentMiddleware
            'sec-ch-ua': '"Google Chrome";v="143", "Chromium";v="143", "Not A(Brand";v="24"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"'
        },
        # Pipeline é…ç½®å·²ç§»è‡³å…¨å±€ settings.py
    }

    def start_requests(self):
        """éå†å…³é”®è¯ï¼Œå‘èµ·åˆ—è¡¨è¯·æ±‚"""
        self.spider_log.info(f"ğŸ“‹ å¼€å§‹é‡‡é›†ï¼Œå…± {len(self.keywords)} ä¸ªå…³é”®è¯")
        
        for keyword in self.keywords:
            params = {
                'current': 1,
                'size': 500,
                'prodName': keyword
            }
            url = f"{self.list_api_base}?{urlencode(params)}"
            
            self.spider_log.info(f"ğŸ” æ­£åœ¨é‡‡é›†å…³é”®è¯: {keyword}")

            yield scrapy.Request(
                url=url,
                callback=self.parse_list,
                meta={
                    'keyword': keyword, 
                    'current_page': 1, 
                    'page_size': 500, 
                    'crawl_id': self.crawl_id
                }
            )

    def parse_list(self, response):
        """è§£æè¯å“åˆ—è¡¨å¹¶å¤„ç†ç¿»é¡µ"""
        page_crawl_id = str(uuid.uuid4())
        keyword = response.meta['keyword']
        current_page = response.meta['current_page']
        parent_crawl_id = response.meta['crawl_id']
        
        try:
            res_json = json.loads(response.text)
            if res_json.get("code") != 0:
                error_msg = res_json.get('msg', 'Unknown Error')
                self.spider_log.error(f"âŒ å…³é”®è¯ [{keyword}] åˆ—è¡¨APIé”™è¯¯ (Page {current_page}): {error_msg}")
                
                yield self.report_error(
                    stage='list_page',
                    error_msg=error_msg,
                    crawl_id=page_crawl_id,
                    params={'prodName': keyword, 'current': current_page},
                    api_url=self.list_api_base,
                    parent_crawl_id=parent_crawl_id
                )
                return

            data = res_json.get("data", {})
            records = data.get("records", [])
            total_pages = data.get("pages", 0)
            page_size = data.get("size", 500)

            self.spider_log.info(f"ğŸ“„ å…³é”®è¯ [{keyword}] åˆ—è¡¨é¡µé¢ [{current_page}/{total_pages}] - å‘ç° {len(records)} æ¡è¯å“è®°å½•")
            
            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=current_page,
                total_pages=total_pages,
                items_found=len(records),
                params={'prodName': keyword, 'current': current_page},
                api_url=self.list_api_base,
                parent_crawl_id=parent_crawl_id,
                page_size=page_size
            )

            item_count = 0
            for record in records:
                drug_code = record.get('prodCode')
                # è¡¥é‡‡æ¨¡å¼ï¼šè·³è¿‡ä¸åœ¨ç›®æ ‡åˆ—è¡¨ä¸­çš„è®°å½•
                if self.recrawl_mode:
                    if drug_code not in self.recrawl_ids:
                        continue
                    self.recrawl_ids.discard(drug_code)  # å·²å¤„ç†ï¼Œä»åˆ—è¡¨ç§»é™¤

                # 1. æå–è¯å“åŸºç¡€ä¿¡æ¯
                base_info = {
                    'drug_code': record.get('prodCode'),
                    'prod_name': record.get('prodName'),
                    'dosform': record.get('dosform'),
                    'spec': record.get('prodSpec'),
                    'pac': record.get('prodPac'),
                    'conv_rat': record.get('convrat'),
                    'prod_entp': record.get('prodentpName'),
                    'dcla_entp': record.get('dclaEntpName'),
                    'aprv_no': record.get('aprvno'),
                    'source_data': json.dumps(record, ensure_ascii=False)
                }

                # 2. å¦‚æœæœ‰è¯å“ç¼–ç ï¼ŒæŸ¥è¯¢é—¨åº—è¯¦æƒ…
                drug_code = record.get('prodCode')
                if drug_code:
                    detail_params = {
                        'current': 1,
                        'size': 20,
                        'drugCode': drug_code
                    }
                    detail_url = f"{self.detail_api_base}?{urlencode(detail_params)}"
                    
                    yield scrapy.Request(
                        url=detail_url,
                        callback=self.parse_detail,
                        meta={
                            'base_info': base_info,
                            'current_page': 1,
                            'page_size': 20,
                            'drug_code': drug_code,
                            'parent_crawl_id': page_crawl_id
                        }
                    )
                    item_count += 1
                else:
                    # æ— ç¼–ç ï¼Œç›´æ¥ä¿å­˜
                    item = HainanDrugItem()
                    item.update(base_info)
                    item['has_shop_record'] = False
                    item.generate_md5_id()
                    yield item
                    item_count += 1

            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€ï¼Œè®°å½•æˆåŠŸå­˜å‚¨çš„æ¡æ•°ï¼ˆåŒ…å«è§¦å‘çš„å­è¯·æ±‚ï¼‰
            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=current_page,
                total_pages=total_pages,
                items_found=len(records),
                params={'prodName': keyword, 'current': current_page},
                api_url=self.list_api_base,
                parent_crawl_id=parent_crawl_id,
                page_size=page_size,
                items_stored=item_count
            )

            # 3. åˆ—è¡¨é¡µç¿»é¡µ
            if current_page < total_pages:
                # è¡¥é‡‡æ¨¡å¼ï¼šå¦‚æœæ‰€æœ‰ç›®æ ‡éƒ½å·²é‡‡é›†å®Œæˆï¼Œæå‰ç»“æŸ
                if self.recrawl_mode and not self.recrawl_ids:
                    self.spider_log.info(f"âœ… è¡¥é‡‡æ¨¡å¼ï¼šæ‰€æœ‰ç›®æ ‡æ•°æ®å·²é‡‡é›†å®Œæˆ")
                    return

                self.spider_log.info(f"ğŸ”„ å‡†å¤‡é‡‡é›†å…³é”®è¯ [{keyword}] ä¸‹ä¸€é¡µåˆ—è¡¨ [{current_page + 1}/{total_pages}]")
                next_page = current_page + 1
                params = {
                    'current': next_page,
                    'size': page_size,
                    'prodName': keyword
                }
                url = f"{self.list_api_base}?{urlencode(params)}"
                yield scrapy.Request(
                    url=url,
                    callback=self.parse_list,
                    meta={
                        'keyword': keyword,
                        'current_page': next_page,
                        'page_size': page_size,
                        'crawl_id': parent_crawl_id
                    }
                )
            else:
                self.spider_log.info(f"âœ… å…³é”®è¯ [{keyword}] åˆ—è¡¨é‡‡é›†å®Œæˆï¼Œå…± {total_pages} é¡µ")

        except Exception as e:
            self.spider_log.error(f"âŒ è§£æå…³é”®è¯ [{keyword}] åˆ—è¡¨å¤±è´¥ (Page {current_page}): {e}", exc_info=True)
            
            yield self.report_error(
                stage='list_page',
                error_msg=e,
                crawl_id=page_crawl_id,
                params={'prodName': keyword, 'current': current_page},
                api_url=self.list_api_base,
                parent_crawl_id=parent_crawl_id
            )

    def parse_detail(self, response):
        """è§£æé—¨åº—/åŒ»é™¢è¯¦æƒ…å¹¶å¤„ç†ç¿»é¡µ"""
        base_info = response.meta['base_info']
        current_page = response.meta['current_page']
        drug_code = response.meta['drug_code']
        parent_crawl_id = response.meta['parent_crawl_id']
        prod_name = base_info.get('prod_name', 'Unknown')
        detail_crawl_id = str(uuid.uuid4())

        try:
            res_json = json.loads(response.text)
            if res_json.get("code") != 0:
                error_msg = res_json.get('msg', 'Unknown Error')
                self.spider_log.warning(f"âš ï¸ è¯å“ [{prod_name}] è¯¦æƒ…APIé”™è¯¯ (Page {current_page}): {error_msg}")
                
                yield self.report_error(
                    stage='detail_page',
                    error_msg=error_msg,
                    crawl_id=detail_crawl_id,
                    params={'drugCode': drug_code, 'current': current_page},
                    api_url=self.detail_api_base,
                    parent_crawl_id=parent_crawl_id,
                    reference_id=drug_code
                )
                return

            data = res_json.get("data", {})
            records = data.get("records", [])
            total_pages = data.get("pages", 0)
            page_size = data.get("size", 20)

            self.spider_log.info(f"ğŸ¥ è¯å“ [{prod_name}] è¯¦æƒ…é¡µé¢ [{current_page}/{total_pages}] - å‘ç° {len(records)} æ¡é—¨åº—è®°å½•")
            
            # ä¼˜åŒ–ï¼šç§»é™¤å†—ä½™çŠ¶æ€ä¸ŠæŠ¥

            item_count = 0
            if records:
                for shop in records:
                    item = HainanDrugItem()
                    item.update(base_info)
                    
                    # æ³¨å…¥é—¨åº—ä¿¡æ¯
                    item['has_shop_record'] = True
                    item['shop_name'] = shop.get('medinsName')
                    item['shop_code'] = shop.get('medinsCode')
                    item['shop_type_memo'] = shop.get('memo')
                    item['price'] = shop.get('pric')
                    item['inventory'] = shop.get('invCnt')
                    item['update_time'] = shop.get('invChgTime')
                    item['hilist_name'] = shop.get('fixmedinsHilistName')
                    
                    # æ›´æ–° source_data åŒ…å«ä¸¤éƒ¨åˆ†ä¿¡æ¯
                    full_source = {
                        "drug_info": json.loads(base_info['source_data']),
                        "shop_info": shop
                    }
                    item['source_data'] = json.dumps(full_source, ensure_ascii=False)
                    
                    item.generate_md5_id()
                    yield item
                    item_count += 1
                
                # è¯¦æƒ…é¡µç¿»é¡µ
                if current_page < total_pages:
                    self.spider_log.info(f"ğŸ”„ å‡†å¤‡é‡‡é›†è¯å“ [{prod_name}] ä¸‹ä¸€é¡µè¯¦æƒ… [{current_page + 1}/{total_pages}]")
                    next_page = current_page + 1
                    params = {
                        'current': next_page,
                        'size': page_size,
                        'drugCode': drug_code
                    }
                    url = f"{self.detail_api_base}?{urlencode(params)}"
                    
                    yield scrapy.Request(
                        url=url,
                        callback=self.parse_detail,
                        meta={
                            'base_info': base_info,
                            'current_page': next_page,
                            'page_size': page_size,
                            'drug_code': drug_code,
                            'parent_crawl_id': parent_crawl_id
                        }
                    )
            elif current_page == 1:
                # ç¬¬ä¸€é¡µå°±æ²¡æ•°æ®ï¼Œè¯´æ˜è¯¥è¯æ²¡åº“å­˜è®°å½•ï¼Œä¿å­˜ä¸€æ¡åŸºç¡€ä¿¡æ¯
                self.spider_log.info(f"ğŸ“‹ è¯å“ [{prod_name}] æ²¡æœ‰é—¨åº—è®°å½•")
                
                item = HainanDrugItem()
                item.update(base_info)
                item['has_shop_record'] = False
                item.generate_md5_id()
                yield item
                item_count += 1

            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€ï¼Œè®°å½•æˆåŠŸå­˜å‚¨çš„æ¡æ•°
            yield self.report_detail_page(
                crawl_id=detail_crawl_id,
                page_no=current_page,
                items_found=len(records),
                params={'drugCode': drug_code, 'current': current_page},
                api_url=self.detail_api_base,
                parent_crawl_id=parent_crawl_id,
                reference_id=drug_code,
                total_pages=total_pages,
                items_stored=item_count
            )

        except Exception as e:
            self.spider_log.error(f"âŒ è§£æè¯å“ [{prod_name}] è¯¦æƒ…å¤±è´¥ (Page {current_page}): {e}", exc_info=True)
            
            yield self.report_error(
                stage='detail_page',
                error_msg=e,
                crawl_id=detail_crawl_id,
                params={'drugCode': drug_code, 'current': current_page},
                api_url=self.detail_api_base,
                parent_crawl_id=parent_crawl_id,
                reference_id=drug_code
            )
