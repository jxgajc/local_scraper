from ast import Param
from .base_spiders import BaseRequestSpider
from ..models.ningxia_drug import NingxiaDrugItem
from urllib.parse import urlencode
from ..utils.logger_utils import get_spider_logger
import json
import scrapy
import time
import uuid

# http://ylbzj.hebei.gov.cn/category/162
class NingxiaDrugSpider(BaseRequestSpider):
    """
    å®å¤åŒ»ä¿å±€è¯å“åŠé‡‡è´­åŒ»é™¢çˆ¬è™«
    ç›®æ ‡: å…ˆè·å–è¯å“åˆ—è¡¨ï¼Œå†æ ¹æ® procurecatalogId è·å–é‡‡è´­è¯¥è¯å“çš„åŒ»é™¢ä¿¡æ¯ï¼ˆèšåˆæ¨¡å¼ï¼‰
    """
    name = "ningxia_drug_spider"
    
    # æ¥å£åœ°å€
    list_api_url = "https://nxyp.ylbz.nx.gov.cn/cms/recentPurchaseDetail/getRecentPurchaseDetailData.html" 
    hospital_api_url = "https://nxyp.ylbz.nx.gov.cn/cms/recentPurchaseDetail/getDrugDetailDate.html"
    
    # å­˜å‚¨cookie
    cookies = {}

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.logger = get_spider_logger(self.name)
        self.crawl_id = str(uuid.uuid4())
        self.logger.info(f"ğŸš€ çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œcrawl_id: {self.crawl_id}")

    custom_settings = {
        'CONCURRENT_REQUESTS': 4, # æ ¹æ®æœåŠ¡å™¨å‹åŠ›é€‚å½“è°ƒæ•´
        'DOWNLOAD_DELAY': 5,
        'DEFAULT_REQUEST_HEADERS': {
            'Accept': '*/*',
            'Connection': 'keep-alive',
            'Content-Type': 'application/json',
            'User-Agent': 'PostmanRuntime-ApipostRuntime/1.1.0',
            'prodType': '2'
        },
        # ä½¿ç”¨æ•°æ®ç®¡é“
        'ITEM_PIPELINES': {
            'hybrid_crawler.pipelines.DataCleaningPipeline': 300,        # æ¸…æ´—
            'hybrid_crawler.pipelines.CrawlStatusPipeline': 350,         # çŠ¶æ€ç›‘æ§ (æ–°å¢)
            'hybrid_crawler.pipelines.NingxiaDrugPipeline': 400,         # å…¥åº“
        }
    }

    def start_requests(self):
        """æ„é€ åˆå§‹çš„GETè¯·æ±‚"""
        payload = {
            "_search": "false",
            "page": 1,
            "rows": 1000, 
            "sidx": "",
            "sord": "asc"
        }
        
        self.logger.info(f"ğŸ“‹ å¼€å§‹é‡‡é›†è¯å“åˆ—è¡¨ï¼Œåˆå§‹payload: {json.dumps(payload)}")
        
        # ä¸ŠæŠ¥å¼€å§‹é‡‡é›†çŠ¶æ€
        yield {
            '_status_': True,
            'crawl_id': self.crawl_id,
            'stage': 'start_requests',
            'page_no': 1,
            'params': payload,
            'api_url': self.list_api_url,
            'success': True
        }
        
        form_data_str = {k: str(v) for k, v in payload.items()}
        yield scrapy.FormRequest(
            url=self.list_api_url,
            method='POST',
            formdata=form_data_str,
            callback=self.parse_logic,
            meta={'payload': payload, 'crawl_id': self.crawl_id}, 
            dont_filter=True
        )

    def parse_logic(self, response):
        """å¤„ç†è¯å“åˆ—è¡¨åˆå§‹å“åº”ï¼šå¤„ç†ç¬¬ä¸€é¡µæ•°æ® + ç”Ÿæˆåç»­é¡µç è¯·æ±‚"""
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta['crawl_id']
        current_payload = response.meta['payload']
        
        try:
            # æ›´æ–°cookies
            if response.headers.getlist('Set-Cookie'):
                self._update_cookies(response)
                
            res_json = json.loads(response.text)
            
            total_pages = int(res_json.get("total", 0))
            records = res_json.get("rows", [])
            current = int(res_json.get("page", 1))
            total_records = int(res_json.get("records", 0))

            self.logger.info(f"ğŸ“„ åˆ—è¡¨é¡µé¢ [{current}/{total_pages}] - å‘ç° {len(records)} æ¡è¯å“è®°å½• (æ€»è®¡: {total_records})")

            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current,
                'total_pages': total_pages,
                'items_found': len(records),
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            item_count = 0
            # 1. å¤„ç†å½“å‰é¡µçš„æ¯ä¸€æ¡è¯å“æ•°æ® -> å‘èµ·è¯¦æƒ…è¯·æ±‚
            for drug_item in records:
                # ä¼ é€’ page_crawl_id ä½œä¸º parent
                yield from self._request_hospital_detail(drug_item, current, page_crawl_id)
                item_count += 1

            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€ (è®°å½•è§¦å‘äº†å¤šå°‘ä¸ªè¯¦æƒ…è¯·æ±‚)
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current,
                'total_pages': total_pages,
                'items_found': len(records),
                'items_stored': item_count,
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            # 2. ç”Ÿæˆå‰©ä½™é¡µç è¯·æ±‚ (ä»ç¬¬2é¡µå¼€å§‹)
            if current < total_pages:
                self.logger.info(f"ğŸ”„ å‡†å¤‡é‡‡é›†åç»­åˆ—è¡¨é¡µé¢ (2-{total_pages})")
                
                for page in range(2, total_pages + 1):
                    next_payload = current_payload.copy()
                    next_payload['page'] = page
                    
                    form_data_str = {k: str(v) for k, v in next_payload.items()}
                    yield scrapy.FormRequest(
                        url=self.list_api_url,
                        method='POST',
                        formdata=form_data_str,
                        callback=self.parse_list_page,
                        meta={
                            'payload': next_payload, 
                            'parent_crawl_id': parent_crawl_id, # åˆ—è¡¨é¡µçš„çˆ¶çº§æ˜¯ Root
                            'page_num': page
                        }, 
                        dont_filter=True
                    )

        except Exception as e:
            self.logger.error(f"âŒ åˆ—è¡¨é¡µè§£æå¤±è´¥ (Page 1): {e}", exc_info=True)
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': 1,
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }

    def parse_list_page(self, response):
        """å¤„ç†åç»­è¯å“åˆ—è¡¨é¡µ"""
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta['parent_crawl_id']
        current_payload = response.meta['payload']
        page_num = response.meta['page_num']
        
        try:
            if response.headers.getlist('Set-Cookie'):
                self._update_cookies(response)
                
            res_json = json.loads(response.text)
            records = res_json.get("rows", [])
            api_page = int(res_json.get('page', page_num))
            total_pages = int(res_json.get("total", 0))
            
            self.logger.info(f"ğŸ“„ åˆ—è¡¨é¡µé¢ [{api_page}/{total_pages}] - å‘ç° {len(records)} æ¡è¯å“è®°å½•")
            
            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': api_page,
                'total_pages': total_pages,
                'items_found': len(records),
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }
            
            item_count = 0
            for drug_item in records:
                yield from self._request_hospital_detail(drug_item, api_page, page_crawl_id)
                item_count += 1
            
            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': api_page,
                'total_pages': total_pages,
                'items_found': len(records),
                'items_stored': item_count,
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }
                
        except Exception as e:
            self.logger.error(f"âŒ åˆ†é¡µè§£æå¤±è´¥ Page {page_num}: {e}", exc_info=True)
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': page_num,
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }

    def _request_hospital_detail(self, drug_item, page_num, parent_crawl_id):
        """æ„é€ è·å–åŒ»é™¢è¯¦æƒ…çš„è¯·æ±‚ï¼Œå°† drug_item ä¼ é€’ä¸‹å»"""
        procurecatalog_id = drug_item.get("procurecatalogId")
        
        if not procurecatalog_id:
            self.logger.warning(f"âš ï¸ ç¼ºå°‘ procurecatalogIdï¼Œè·³è¿‡è¯¦æƒ…æŸ¥è¯¢: {drug_item.get('productName')}")
            return

        # æ„é€ è¯¦æƒ…é¡µ Payload
        payload = {
            "page": 1,
            "rows": 1000,
            "procurecatalogId": procurecatalog_id,
            "_search": "false",
            "sidx": "",
            "sord": "asc"
        }
        
        # æ³¨æ„ï¼šè¿™é‡ŒåŸä»£ç ä½¿ç”¨çš„æ˜¯ GET è¯·æ±‚å¸¦ formdataï¼Œåœ¨ Scrapy ä¸­ FormRequest é»˜è®¤æ˜¯ POSTã€‚
        # å¦‚æœç›®æ ‡æ¥å£ç¡®å®æ”¯æŒ GET ä¸”å‚æ•°åœ¨ URL ä¸­ï¼Œå»ºè®®ç”¨ scrapy.Request(url=...urlencode(params))ã€‚
        # å¦‚æœç›®æ ‡æ˜¯ POSTï¼Œè¯·å°† method æ”¹ä¸º 'POST'ã€‚
        # è¿™é‡Œä¸ºäº†å…¼å®¹åŸé€»è¾‘ï¼Œä¿æŒ FormRequest ä½†éœ€æ³¨æ„ methodã€‚
        # å‡è®¾åŸæ„æ˜¯ POST (å› ä¸ºæœ‰ rows/page å‚æ•°)ï¼Œè¿™é‡Œæ˜¾å¼æ”¹ä¸º POST ä¼šæ›´ç¨³å¦¥ï¼Œ
        # ä½†å¦‚æœå¿…é¡» GETï¼Œformdata ä¼šè¢«å¿½ç•¥ï¼ˆé™¤éåº“æœ‰ç‰¹æ®Šå¤„ç†ï¼‰ã€‚
        # æ­¤å¤„ä¿ç•™ FormRequest ç»“æ„ä»¥ä¾¿å…¼å®¹ payload ä¼ é€’ã€‚
        
        form_data_str = {k: str(v) for k, v in payload.items()}
        yield scrapy.FormRequest(
            url=self.hospital_api_url,
            method='GET', # ä¿æŒåŸä»£ç çš„ GETï¼Œä½†éœ€æ³¨æ„å¯èƒ½éœ€è¦ urlencode åˆ° URL
            formdata=form_data_str,
            callback=self.parse_detail,
            meta={
                'drug_info': drug_item, 
                'page_num': page_num,
                'procurecatalog_id': procurecatalog_id,
                'current_hospital_page': 1,
                'hospital_list': [],
                'parent_crawl_id': parent_crawl_id,
                'payload': payload
            }, 
            cookies=self.cookies if self.cookies else None,
            dont_filter=True
        )

    def parse_detail(self, response):
        """å¤„ç†åŒ»é™¢è¯¦æƒ…å“åº”ï¼Œåˆå¹¶æ•°æ®å¹¶ç”Ÿæˆ Item"""
        drug_info = response.meta['drug_info']
        page_num = response.meta.get('page_num', 1)
        procurecatalog_id = response.meta.get('procurecatalog_id')
        current_hospital_page = response.meta.get('current_hospital_page', 1)
        hospital_list = response.meta.get('hospital_list', [])
        parent_crawl_id = response.meta['parent_crawl_id']
        current_payload = response.meta['payload']
        
        # ä¸ºå½“å‰è¯å“çš„è¯¦æƒ…æŠ“å–ç”Ÿæˆä¸€ä¸ª ID (å¦‚æœè¿˜æ²¡ç”Ÿæˆè¿‡)
        detail_crawl_id = response.meta.get('detail_crawl_id', str(uuid.uuid4()))

        try:
            res_json = json.loads(response.text)
            
            # 3. æå–å½“å‰é¡µåŒ»é™¢æ•°æ®å’Œåˆ†é¡µä¿¡æ¯
            current_page_hospitals = res_json.get("rows", [])
            total_pages = int(res_json.get("total", 0))
            
            self.logger.info(f"ğŸ¥ è¯å“ [{procurecatalog_id}] åŒ»é™¢åˆ—è¡¨ [{current_hospital_page}/{total_pages}] - å‘ç° {len(current_page_hospitals)} å®¶åŒ»é™¢")
            
            # ä¸ŠæŠ¥è¯¦æƒ…é¡µé‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': detail_crawl_id,
                'stage': 'detail_page',
                'page_no': current_hospital_page,
                'total_pages': total_pages,
                'items_found': len(current_page_hospitals),
                'params': current_payload,
                'api_url': self.hospital_api_url,
                'reference_id': procurecatalog_id,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            # 4. ç´¯åŠ åŒ»é™¢æ•°æ®
            hospital_list.extend(current_page_hospitals)
            
            # 5. å¦‚æœè¿˜æœ‰ä¸‹ä¸€é¡µï¼Œç»§ç»­è¯·æ±‚
            if current_hospital_page < total_pages:
                next_page = current_hospital_page + 1
                
                # æ„é€ ä¸‹ä¸€é¡µè¯·æ±‚
                next_payload = current_payload.copy()
                next_payload['page'] = next_page
                
                form_data_str = {k: str(v) for k, v in next_payload.items()}
                yield scrapy.FormRequest(
                    url=self.hospital_api_url,
                    method='GET',
                    formdata=form_data_str,
                    callback=self.parse_detail,
                    meta={
                        'drug_info': drug_info,
                        'page_num': page_num,
                        'procurecatalog_id': procurecatalog_id,
                        'current_hospital_page': next_page,
                        'hospital_list': hospital_list,
                        'parent_crawl_id': parent_crawl_id,
                        'detail_crawl_id': detail_crawl_id, # ä¼ é€’åŒä¸€ä¸ªè¯¦æƒ…ID
                        'payload': next_payload
                    },
                    cookies=self.cookies if self.cookies else None,
                    dont_filter=True
                )
            else:
                # 6. æ‰€æœ‰é¡µé¢å¤„ç†å®Œæˆï¼Œç”Ÿæˆæœ€ç»ˆItem
                self.logger.info(f"âœ… è¯å“ [{procurecatalog_id}] æ‰€æœ‰åŒ»é™¢æ•°æ®è·å–å®Œæˆï¼Œå…± {len(hospital_list)} å®¶åŒ»é™¢")
                yield self._create_item(drug_info, hospital_list, page_num)
                
                # æ›´æ–°çŠ¶æ€ï¼šç¡®è®¤å­˜å‚¨äº†1æ¡èšåˆæ•°æ®
                yield {
                    '_status_': True,
                    'crawl_id': detail_crawl_id,
                    'stage': 'detail_page',
                    'items_stored': 1, # èšåˆæ¨¡å¼ä¸‹ï¼Œæœ€ç»ˆåªå…¥åº“1æ¡è®°å½•
                    'params': current_payload,
                    'api_url': self.hospital_api_url,
                    'reference_id': procurecatalog_id,
                    'success': True,
                    'parent_crawl_id': parent_crawl_id
                }

        except Exception as e:
            self.logger.error(f"âŒ è¯¦æƒ…é¡µè§£æå¤±è´¥: {e} | URL: {response.url}", exc_info=True)
            yield {
                '_status_': True,
                'crawl_id': detail_crawl_id,
                'stage': 'detail_page',
                'page_no': current_hospital_page,
                'params': current_payload,
                'api_url': self.hospital_api_url,
                'reference_id': procurecatalog_id,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }

    def _create_item(self, drug_info, hospital_list, page_num=1):
        """
        æ„å»º NingxiaDrugItem
        """
        item = NingxiaDrugItem()
        
        prodentp_code = drug_info.get("prodentpCode")
        prod_code = drug_info.get("prodCode")
        
        # 1. è®¾ç½®è¯å“åŸºç¡€ä¿¡æ¯
        for field_name in item.fields:
            if field_name in ['md5_id', 'collect_time', 'url', 'url_hash', 'hospital_purchases', 'page_num']:
                continue  # è·³è¿‡éœ€è¦å•ç‹¬å¤„ç†çš„å­—æ®µ
            if field_name in drug_info:
                item[field_name] = drug_info[field_name]
        
        # 2. è®¾ç½®åŒ»é™¢é‡‡è´­ä¿¡æ¯
        item['hospital_purchases'] = hospital_list
        
        # 3. è®¾ç½®URLå­—æ®µ
        item['url'] = f"{self.hospital_api_url}?pageNo=1&pageSize=1000&prodCode={prod_code}&prodEntpCode={prodentp_code}&isPublicHospitals="
        
        # 4. è®¾ç½®é¡µç 
        item['page_num'] = page_num
        
        # 5. ç”ŸæˆMD5å”¯ä¸€IDå’Œé‡‡é›†æ—¶é—´
        item.generate_md5_id()
        
        return item
        
    def _update_cookies(self, response):
        """
        ä»å“åº”ä¸­æå–å¹¶æ›´æ–°cookies
        """
        try:
            for cookie_header in response.headers.getlist('Set-Cookie'):
                cookie_str = cookie_header.decode('utf-8')
                if '=' in cookie_str:
                    cookie_parts = cookie_str.split(';')[0].split('=')
                    if len(cookie_parts) >= 2:
                        cookie_name = cookie_parts[0].strip()
                        cookie_value = '='.join(cookie_parts[1:]).strip()
                        self.cookies[cookie_name] = cookie_value
            # self.logger.debug(f"æ›´æ–°åçš„cookies: {self.cookies}")
        except Exception as e:
            self.logger.warning(f"Cookies æ›´æ–°å¤±è´¥: {e}")