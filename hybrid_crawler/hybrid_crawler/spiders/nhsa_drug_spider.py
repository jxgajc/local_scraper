from .base_spiders import BaseRequestSpider
from ..models.nhsa_drug import NhsaDrugItem
from ..utils.logger_utils import get_spider_logger
import json
import scrapy
import time
import uuid

class NhsaDrugSpider(BaseRequestSpider):
    """
    å›½å®¶åŒ»ä¿è¯å“æ•°æ®çˆ¬è™«
    ç›®æ ‡: é‡‡é›†å›½å®¶åŒ»ä¿è¯å“æ•°æ®APIï¼Œè·å–è¯å“ä¿¡æ¯
    Target: https://code.nhsa.gov.cn
    """
    name = "nhsa_drug_spider"
    
    # è¯å“åˆ—è¡¨API URL
    list_api_url = "https://code.nhsa.gov.cn/yp/getPublishGoodsDataInfo.html" 
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.logger = get_spider_logger(self.name)
        self.crawl_id = str(uuid.uuid4())
        self.logger.info(f"ğŸš€ çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œcrawl_id: {self.crawl_id}")

    custom_settings = {
        'CONCURRENT_REQUESTS': 1, # é™ä½å¹¶å‘æ•°ï¼Œé¿å…è§¦å‘åçˆ¬
        'DOWNLOAD_DELAY': 15, # å¢åŠ å»¶è¿Ÿï¼Œé™ä½è¯·æ±‚é¢‘ç‡
        'DEFAULT_REQUEST_HEADERS': {
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Accept-Encoding': 'gzip, deflate, br',
            'Accept-Language': 'zh-CN,zh-Hans;q=0.9',
            'Connection': 'keep-alive',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'Cookie': 'JSESSIONID=1F42D848E37CC3D56CA2B0BE3CCA994D;acw_tc=1a0c65ca17650958291751175eb31be38dda41a39d53d696605dc27af75da9',
            'Origin': 'https://code.nhsa.gov.cn',
            'Priority': 'u=3, i',
            'Referer': 'https://code.nhsa.gov.cn/yp/toPublishGoodsData.html?batchNumber=20251201',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.5 Safari/605.1.15',
            'X-Requested-With': 'XMLHttpRequest',
        },
        # ä½¿ç”¨ä¸“é—¨çš„å›½å®¶åŒ»ä¿è¯å“æ•°æ®ç®¡é“
        'ITEM_PIPELINES': {
            'hybrid_crawler.pipelines.DataCleaningPipeline': 300,        # æ¸…æ´—
            'hybrid_crawler.pipelines.CrawlStatusPipeline': 350,         # çŠ¶æ€ç›‘æ§ (æ–°å¢)
            'hybrid_crawler.pipelines.NhsaDrugPipeline': 400,           # å…¥åº“
        }
    }

    def start_requests(self):
        """æ„é€ åˆå§‹çš„POSTè¯·æ±‚ï¼Œä½¿ç”¨application/x-www-form-urlencodedæ ¼å¼"""
        # æ„é€ åˆå§‹è¡¨å•æ•°æ®
        form_data = {
            'goodsCode': '',
            'companyNameSc': '',
            'registeredProductName': '',
            'approvalCode': '',
            'batchNumber': '20251201',
            '_search': 'false',
            'nd': str(int(time.time() * 1000)),
            'rows': '1000',
            'page': '1',
            'sidx': '',
            'sord': 'asc'
        }
        
        self.logger.info(f"ğŸ“‹ å¼€å§‹é‡‡é›†å›½å®¶åŒ»ä¿è¯å“æ•°æ®ï¼ŒBatch: {form_data['batchNumber']}")
        
        # ä¸ŠæŠ¥å¼€å§‹é‡‡é›†çŠ¶æ€
        yield {
            '_status_': True,
            'crawl_id': self.crawl_id,
            'stage': 'start_requests',
            'page_no': 1,
            'params': form_data,
            'api_url': self.list_api_url,
            'success': True
        }
        
        # å‘èµ·ç¬¬ä¸€é¡µè¯·æ±‚
        yield scrapy.FormRequest(
            url=self.list_api_url,
            method='POST',
            formdata=form_data,
            callback=self.parse_logic,
            meta={'form_data': form_data, 'crawl_id': self.crawl_id}, # ä¼ é€’ form_data ä»¥ä¾¿åç»­ç¿»é¡µä½¿ç”¨
            dont_filter=True
        )

    def parse_logic(self, response):
        """å¤„ç†è¯å“åˆ—è¡¨å“åº”ï¼šå¤„ç†ç¬¬ä¸€é¡µæ•°æ® + ç”Ÿæˆåç»­é¡µç è¯·æ±‚"""
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta['crawl_id']
        current_form_data = response.meta['form_data']
        
        try:
            res_json = json.loads(response.text)
            
            # è·å–æ•°æ®å’Œåˆ†é¡µä¿¡æ¯
            rows = res_json.get("rows", [])
            total_pages = int(res_json.get("total", 0))
            current_page = int(res_json.get("page", 1))
            total_records = int(res_json.get("records", 0))
            
            self.logger.info(f"ğŸ“„ åˆ—è¡¨é¡µé¢ [{current_page}/{total_pages}] - å‘ç° {len(rows)} æ¡è®°å½• (æ€»è®¡: {total_records})")

            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current_page,
                'total_pages': total_pages,
                'items_found': len(rows),
                'params': current_form_data,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            item_count = 0
            # 1. å¤„ç†å½“å‰é¡µçš„æ¯ä¸€æ¡è¯å“æ•°æ®
            for drug_item in rows:
                yield self._create_item(drug_item, current_page)
                item_count += 1

            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current_page,
                'total_pages': total_pages,
                'items_found': len(rows),
                'items_stored': item_count,
                'params': current_form_data,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            # 2. ç”Ÿæˆå‰©ä½™é¡µç è¯·æ±‚ (ä»ç¬¬2é¡µå¼€å§‹)
            # åªæœ‰åœ¨å¤„ç†ç¬¬1é¡µæ—¶æ‰ç”Ÿæˆæ‰€æœ‰åç»­é¡µç è¯·æ±‚
            if current_page == 1 and current_page < total_pages:
                self.logger.info(f"ğŸ”„ å‡†å¤‡é‡‡é›†åç»­é¡µé¢ (2-{total_pages})")
                
                for next_page in range(current_page + 1, total_pages + 1):
                    next_form_data = current_form_data.copy()
                    next_form_data['page'] = str(next_page)
                    next_form_data['nd'] = str(int(time.time() * 1000))
                    
                    yield scrapy.FormRequest(
                        url=self.list_api_url,
                        method='POST',
                        formdata=next_form_data,
                        callback=self.parse_list_page, # ä½¿ç”¨ç‹¬ç«‹å›è°ƒå¤„ç†åç»­é¡µé¢
                        meta={
                            'form_data': next_form_data, 
                            'parent_crawl_id': parent_crawl_id,
                            'page_num': next_page
                        },
                        dont_filter=True
                    )

        except Exception as e:
            self.logger.error(f"âŒ åˆ—è¡¨é¡µè§£æå¤±è´¥ (Page 1): {e}", exc_info=True)
            
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': 1,
                'params': current_form_data,
                'api_url': self.list_api_url,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }

    def parse_list_page(self, response):
        """å¤„ç†åç»­é¡µé¢çš„å“åº”"""
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta['parent_crawl_id']
        current_form_data = response.meta['form_data']
        page_num = response.meta['page_num']

        try:
            res_json = json.loads(response.text)
            
            rows = res_json.get("rows", [])
            total_pages = int(res_json.get("total", 0))
            
            self.logger.info(f"ğŸ“„ åˆ—è¡¨é¡µé¢ [{page_num}/{total_pages}] - å‘ç° {len(rows)} æ¡è®°å½•")
            
            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': page_num,
                'total_pages': total_pages,
                'items_found': len(rows),
                'params': current_form_data,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }
            
            item_count = 0
            for drug_item in rows:
                yield self._create_item(drug_item, page_num)
                item_count += 1
            
            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': page_num,
                'total_pages': total_pages,
                'items_found': len(rows),
                'items_stored': item_count,
                'params': current_form_data,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

        except Exception as e:
            self.logger.error(f"âŒ é¡µé¢å¤„ç†å¤±è´¥ (Page {page_num}): {e}", exc_info=True)
            
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': page_num,
                'params': current_form_data,
                'api_url': self.list_api_url,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }

    def _create_item(self, drug_item, page_num):
        """
        æ„å»º NhsaDrugItem
        :param drug_item: è¯·æ±‚è·å–çš„è¯å“ä¿¡æ¯ (Dict)
        :param page_num: é‡‡é›†é¡µç 
        """
        item = NhsaDrugItem()
        
        # ç›´æ¥ä½¿ç”¨APIè¿”å›çš„å­—æ®µåï¼ˆé©¼å³°å‘½åï¼‰
        for field_name in item.fields:
            if field_name in ['id', 'collect_time', 'url', 'url_hash', 'page_num']:
                continue  # è·³è¿‡éœ€è¦å•ç‹¬å¤„ç†çš„å­—æ®µ
            item[field_name] = drug_item.get(field_name, '')
        
        # è®¾ç½®URLå­—æ®µ
        item['url'] = f"https://nhsa.drug/{drug_item.get('goodscode', 'unknown')}"
        
        # è®¾ç½®é¡µç 
        item['page_num'] = page_num
        
        # ç”ŸæˆMD5å”¯ä¸€IDå’Œé‡‡é›†æ—¶é—´
        item.generate_md5_id()
        
        return item