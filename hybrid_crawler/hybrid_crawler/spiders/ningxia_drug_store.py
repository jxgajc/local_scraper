from .base_spiders import BaseRequestSpider
from ..models.ningxia_drug import NingxiaDrugItem
from ..utils.logger_utils import get_spider_logger
import json
import scrapy
import uuid

class NingxiaDrugSpider(BaseRequestSpider):
    """
    å®å¤åŒ»ä¿å±€è¯å“åŠé‡‡è´­åŒ»é™¢çˆ¬è™«
    æµç¨‹: 
    1. è¯·æ±‚è¯å“åˆ—è¡¨ (getRecentPurchaseDetailData.html) -> æ”¯æŒç¿»é¡µ
    2. è·å– procurecatalogId
    3. è¯·æ±‚åŒ»é™¢æ˜ç»† (getDrugDetailDate.html) -> æ”¯æŒç¿»é¡µ
    """
    name = "ningxia_drug_store"
    
    # è¯å“åˆ—è¡¨æ¥å£
    list_api_url = "https://nxyp.ylbz.nx.gov.cn/cms/recentPurchaseDetail/getRecentPurchaseDetailData.html" 
    # åŒ»é™¢æ˜ç»†æ¥å£
    hospital_api_url = "https://nxyp.ylbz.nx.gov.cn/cms/recentPurchaseDetail/getDrugDetailDate.html"

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.spider_log = get_spider_logger(self.name)
        self.crawl_id = str(uuid.uuid4())
        self.spider_log.info(f"ğŸš€ çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œcrawl_id: {self.crawl_id}")

    custom_settings = {
        'CONCURRENT_REQUESTS': 4,
        'DOWNLOAD_DELAY': 1,
        'DEFAULT_REQUEST_HEADERS': {
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.5 Safari/605.1.15',
            'Origin': 'https://nxyp.ylbz.nx.gov.cn',
            'Referer': 'https://nxyp.ylbz.nx.gov.cn/cms/showListYPXQ.html',
            'X-Requested-With': 'XMLHttpRequest'
        },
        'ITEM_PIPELINES': {
            'hybrid_crawler.pipelines.DataCleaningPipeline': 300,        # æ¸…æ´—
            'hybrid_crawler.pipelines.CrawlStatusPipeline': 350,         # çŠ¶æ€ç›‘æ§ (æ–°å¢)
            'hybrid_crawler.pipelines.NingxiaDrugPipeline': 400,         # å…¥åº“
        }
    }

    def start_requests(self):
        """Step 1: æ„é€ åˆå§‹çš„è¯å“åˆ—è¡¨è¯·æ±‚"""
        payload = {
            "_search": "false",
            "page": "1",
            "rows": "100", 
            "sidx": "",
            "sord": "asc"
        }
        
        self.spider_log.info(f"ğŸ“‹ å¼€å§‹é‡‡é›†è¯å“åˆ—è¡¨ï¼Œåˆå§‹payload: {json.dumps(payload)}")
        
        # ä¸ŠæŠ¥å¼€å§‹é‡‡é›†çŠ¶æ€
        # yield {
        #     '_status_': True,
        #     'crawl_id': self.crawl_id,
        #     'stage': 'start_requests',
        #     'page_no': 1,
        #     'params': payload,
        #     'api_url': self.list_api_url,
        #     'success': True
        # }
        
        yield scrapy.FormRequest(
            url=self.list_api_url,
            method='POST',
            formdata=payload,
            callback=self.parse_logic,
            meta={'payload': payload, 'crawl_id': self.crawl_id},
            dont_filter=True
        )

    def parse_logic(self, response):
        """
        Step 2: å¤„ç†è¯å“åˆ—è¡¨ï¼Œè§¦å‘åŒ»é™¢è¯¦æƒ…è¯·æ±‚
        """
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta['crawl_id']
        current_payload = response.meta['payload']
        
        try:
            res_json = json.loads(response.text)
            
            # æå–æ•°æ®
            records = res_json.get("rows", [])
            total_pages = int(res_json.get("total", 0))
            current_page = int(res_json.get("page", 1))
            total_records = int(res_json.get("records", 0))

            self.spider_log.info(f"ğŸ“„ è¯å“åˆ—è¡¨é¡µé¢ [{current_page}/{total_pages}] - å‘ç° {len(records)} æ¡è¯å“è®°å½• (æ€»è®¡: {total_records})")

            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current_page,
                'total_pages': total_pages,
                'items_found': len(records),
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            item_count = 0
            # --- æ ¸å¿ƒé€»è¾‘ï¼šéå†è¯å“ï¼Œè¿›å…¥ç¬¬äºŒå±‚è¯¦æƒ… ---
            for drug_item in records:
                # å¿…é¡»æœ‰ procurecatalogId æ‰èƒ½æŸ¥è¯¦æƒ…
                if drug_item.get("procurecatalogId"):
                    # ä¼ é€’ page_crawl_id ä½œä¸ºè¯¦æƒ…é¡µçš„çˆ¶ID
                    yield from self._request_hospital_detail(drug_item, page_crawl_id)
                    item_count += 1
                else:
                    self.spider_log.warning(f"âš ï¸ è¯å“ç¼ºå°‘ procurecatalogId: {drug_item.get('productName')}")

            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current_page,
                'total_pages': total_pages,
                'items_found': len(records),
                'items_stored': item_count, # è§¦å‘äº†å¤šå°‘ä¸ªè¯¦æƒ…è¯·æ±‚
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            # --- åˆ—è¡¨é¡µç¿»é¡µé€»è¾‘ ---
            if current_page < total_pages:
                self.spider_log.info(f"ğŸ”„ å‡†å¤‡é‡‡é›†ä¸‹ä¸€é¡µè¯å“åˆ—è¡¨ [{current_page + 1}/{total_pages}]")
                next_page = current_page + 1
                next_payload = current_payload.copy()
                next_payload['page'] = str(next_page)
                
                yield scrapy.FormRequest(
                    url=self.list_api_url,
                    method='POST',
                    formdata=next_payload,
                    callback=self.parse_logic,
                    meta={'payload': next_payload, 'crawl_id': self.crawl_id},
                    dont_filter=True
                )

        except Exception as e:
            self.spider_log.error(f"âŒ è¯å“åˆ—è¡¨è§£æå¤±è´¥: {e}", exc_info=True)
            
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current_payload.get('page'),
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }

    def _request_hospital_detail(self, drug_item, parent_crawl_id):
        """Step 3: æ„é€ åŒ»é™¢è¯¦æƒ…è¯·æ±‚ (POST)"""
        procure_id = str(drug_item.get("procurecatalogId"))
        
        detail_payload = {
            "procurecatalogId": procure_id,
            "_search": "false",
            "rows": "100", 
            "page": "1",    
            "sidx": "",
            "sord": "asc"
        }

        yield scrapy.FormRequest(
            url=self.hospital_api_url,
            method='POST',
            formdata=detail_payload,
            callback=self.parse_hospital_detail,
            meta={
                'drug_info': drug_item,
                'procure_id': procure_id,
                'current_detail_page': 1,
                'payload': detail_payload,
                'parent_crawl_id': parent_crawl_id
            },
            dont_filter=True
        )

    def parse_hospital_detail(self, response):
        """Step 4: è§£æåŒ»é™¢åˆ—è¡¨å¹¶ç”Ÿæˆ Item"""
        drug_info = response.meta['drug_info']
        parent_crawl_id = response.meta['parent_crawl_id']
        current_payload = response.meta['payload']
        detail_crawl_id = str(uuid.uuid4())
        
        try:
            res_json = json.loads(response.text)
            
            # æå–åŒ»é™¢æ•°æ®
            hospitals = res_json.get("rows", [])
            total_detail_pages = int(res_json.get("total", 0))
            current_detail_page = int(response.meta['current_detail_page'])
            
            self.spider_log.info(f"ğŸ¥ è¯å“ [{drug_info.get('productName')}] è¯¦æƒ…é¡µ [{current_detail_page}/{total_detail_pages}] - å‘ç° {len(hospitals)} å®¶åŒ»é™¢")

            # ä¸ŠæŠ¥è¯¦æƒ…é¡µé‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': detail_crawl_id,
                'stage': 'detail_page',
                'page_no': current_detail_page,
                'total_pages': total_detail_pages,
                'items_found': len(hospitals),
                'params': current_payload,
                'api_url': self.hospital_api_url,
                'reference_id': response.meta['procure_id'],
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            item_count = 0
            # éå†å½“å‰é¡µçš„åŒ»é™¢ï¼Œç”Ÿæˆæœ€ç»ˆæ•°æ®
            for hosp_item in hospitals:
                yield self._create_item(drug_info, hosp_item, response)
                item_count += 1

            # æ›´æ–°è¯¦æƒ…é¡µé‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': detail_crawl_id,
                'stage': 'detail_page',
                'page_no': current_detail_page,
                'total_pages': total_detail_pages,
                'items_found': len(hospitals),
                'items_stored': item_count,
                'params': current_payload,
                'api_url': self.hospital_api_url,
                'reference_id': response.meta['procure_id'],
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            # --- è¯¦æƒ…é¡µç¿»é¡µé€»è¾‘ ---
            if current_detail_page < total_detail_pages:
                next_page = current_detail_page + 1
                next_payload = current_payload.copy()
                next_payload['page'] = str(next_page)
                
                yield scrapy.FormRequest(
                    url=self.hospital_api_url,
                    method='POST',
                    formdata=next_payload,
                    callback=self.parse_hospital_detail,
                    meta={
                        'drug_info': drug_info,
                        'procure_id': response.meta['procure_id'],
                        'current_detail_page': next_page,
                        'payload': next_payload,
                        'parent_crawl_id': parent_crawl_id # ä¿æŒåŒä¸€ä¸ªçˆ¶ID (åˆ—è¡¨é¡µID)
                    },
                    dont_filter=True
                )

        except Exception as e:
            self.spider_log.error(f"âŒ åŒ»é™¢è¯¦æƒ…è§£æå¤±è´¥: {e} | DrugID: {response.meta.get('procure_id')}", exc_info=True)
            
            yield {
                '_status_': True,
                'crawl_id': detail_crawl_id,
                'stage': 'detail_page',
                'page_no': current_payload.get('page'),
                'params': current_payload,
                'api_url': self.hospital_api_url,
                'reference_id': response.meta.get('procure_id'),
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }

    def _create_item(self, drug_info, hosp_item, response=None):
        """åˆå¹¶è¯å“ä¿¡æ¯å’ŒåŒ»é™¢ä¿¡æ¯"""
        item = NingxiaDrugItem()
        
        # 1. å¡«å……è¯å“åŸºç¡€ä¿¡æ¯
        for key, value in drug_info.items():
            if key in item.fields:
                item[key] = value
                
        # 2. å¡«å……/è¦†ç›–åŒ»é™¢ç‰¹æœ‰ä¿¡æ¯
        if 'hospitalName' in hosp_item:
            item['hospitalName'] = hosp_item['hospitalName']
            
        if 'areaName' in hosp_item:
            item['areaName'] = hosp_item['areaName']
            
        # 3. è¡¥å……ç³»ç»Ÿå­—æ®µ
        item['url'] = self.hospital_api_url
        item['page_num'] = response.meta.get('current_detail_page', 1) if response else 1
        
        # ç”Ÿæˆå”¯ä¸€ID
        item.generate_md5_id()
        
        return item