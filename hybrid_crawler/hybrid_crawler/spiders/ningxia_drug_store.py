from .base_spiders import BaseRequestSpider
from ..models.ningxia_drug import NingxiaDrugItem
from ..utils.logger_utils import get_spider_logger
import json
import scrapy
import uuid
from .mixins import SpiderStatusMixin

class NingxiaDrugSpider(SpiderStatusMixin, BaseRequestSpider):
    """
    å®å¤åŒ»ä¿å±€è¯å“åŠé‡‡è´­åŒ»é™¢çˆ¬è™«
    æµç¨‹: 
    1. è¯·æ±‚è¯å“åˆ—è¡¨ (getRecentPurchaseDetailData.html) -> æ”¯æŒç¿»é¡µ
    2. è·å– procurecatalogId
    3. è¯·æ±‚åŒ»é™¢æ˜ç»† (getDrugDetailDate.html) -> æ”¯æŒç¿»é¡µ
    """
    name = "ningxia_drug_store"
    
    # è¯å“åˆ—è¡¨æ¥å£
    list_api_url = "https://nxyp.ylbz.nx.gov.cn/cms/recentPurchaseDetail/getRecentPurchaseDetailData.html" 
    # åŒ»é™¢æ˜ç»†æ¥å£
    hospital_api_url = "https://nxyp.ylbz.nx.gov.cn/cms/recentPurchaseDetail/getDrugDetailDate.html"

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.spider_log = get_spider_logger(self.name)
        self.crawl_id = str(uuid.uuid4())
        self.spider_log.info(f"ğŸš€ çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œcrawl_id: {self.crawl_id}")

    custom_settings = {
        'CONCURRENT_REQUESTS': 4,
        'DOWNLOAD_DELAY': 1,
        'DEFAULT_REQUEST_HEADERS': {
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            # 'User-Agent': Handled by RandomUserAgentMiddleware
            'Origin': 'https://nxyp.ylbz.nx.gov.cn',
            'Referer': 'https://nxyp.ylbz.nx.gov.cn/cms/showListYPXQ.html',
            'X-Requested-With': 'XMLHttpRequest'
        },
        # Pipeline é…ç½®å·²ç§»è‡³å…¨å±€ settings.py
    }

    def start_requests(self):
        """Step 1: æ„é€ åˆå§‹çš„è¯å“åˆ—è¡¨è¯·æ±‚"""
        payload = {
            "_search": "false",
            "page": "1",
            "rows": "100", 
            "sidx": "",
            "sord": "asc"
        }
        
        self.spider_log.info(f"ğŸ“‹ å¼€å§‹é‡‡é›†è¯å“åˆ—è¡¨ï¼Œåˆå§‹payload: {json.dumps(payload)}")
        
        yield scrapy.FormRequest(
            url=self.list_api_url,
            method='POST',
            formdata=payload,
            callback=self.parse_logic,
            meta={'payload': payload, 'crawl_id': self.crawl_id},
            dont_filter=True
        )

    def parse_logic(self, response):
        """
        Step 2: å¤„ç†è¯å“åˆ—è¡¨ï¼Œè§¦å‘åŒ»é™¢è¯¦æƒ…è¯·æ±‚
        """
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta['crawl_id']
        current_payload = response.meta['payload']
        
        try:
            res_json = json.loads(response.text)
            
            # æå–æ•°æ®
            records = res_json.get("rows", [])
            total_pages = int(res_json.get("total", 0))
            current_page = int(res_json.get("page", 1))
            total_records = int(res_json.get("records", 0))

            self.spider_log.info(f"ğŸ“„ è¯å“åˆ—è¡¨é¡µé¢ [{current_page}/{total_pages}] - å‘ç° {len(records)} æ¡è¯å“è®°å½• (æ€»è®¡: {total_records})")

            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=current_page,
                total_pages=total_pages,
                items_found=len(records),
                params=current_payload,
                api_url=self.list_api_url,
                parent_crawl_id=parent_crawl_id
            )

            item_count = 0
            # --- æ ¸å¿ƒé€»è¾‘ï¼šéå†è¯å“ï¼Œè¿›å…¥ç¬¬äºŒå±‚è¯¦æƒ… ---
            for drug_item in records:
                # å¿…é¡»æœ‰ procurecatalogId æ‰èƒ½æŸ¥è¯¦æƒ…
                if drug_item.get("procurecatalogId"):
                    # ä¼ é€’ page_crawl_id ä½œä¸ºè¯¦æƒ…é¡µçš„çˆ¶ID
                    yield from self._request_hospital_detail(drug_item, page_crawl_id)
                    item_count += 1
                else:
                    self.spider_log.warning(f"âš ï¸ è¯å“ç¼ºå°‘ procurecatalogId: {drug_item.get('productName')}")

            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€
            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=current_page,
                total_pages=total_pages,
                items_found=len(records),
                items_stored=item_count, # è§¦å‘äº†å¤šå°‘ä¸ªè¯¦æƒ…è¯·æ±‚
                params=current_payload,
                api_url=self.list_api_url,
                parent_crawl_id=parent_crawl_id
            )

            # --- åˆ—è¡¨é¡µç¿»é¡µé€»è¾‘ ---
            if current_page < total_pages:
                self.spider_log.info(f"ğŸ”„ å‡†å¤‡é‡‡é›†ä¸‹ä¸€é¡µè¯å“åˆ—è¡¨ [{current_page + 1}/{total_pages}]")
                next_page = current_page + 1
                next_payload = current_payload.copy()
                next_payload['page'] = str(next_page)
                
                yield scrapy.FormRequest(
                    url=self.list_api_url,
                    method='POST',
                    formdata=next_payload,
                    callback=self.parse_logic,
                    meta={'payload': next_payload, 'crawl_id': self.crawl_id},
                    dont_filter=True
                )

        except Exception as e:
            self.spider_log.error(f"âŒ è¯å“åˆ—è¡¨è§£æå¤±è´¥: {e}", exc_info=True)
            
            yield self.report_error(
                stage='list_page',
                error_msg=e,
                crawl_id=page_crawl_id,
                params=current_payload,
                api_url=self.list_api_url,
                parent_crawl_id=parent_crawl_id
            )

    def _request_hospital_detail(self, drug_item, parent_crawl_id):
        """Step 3: æ„é€ åŒ»é™¢è¯¦æƒ…è¯·æ±‚ (POST)"""
        procure_id = str(drug_item.get("procurecatalogId"))
        
        detail_payload = {
            "procurecatalogId": procure_id,
            "_search": "false",
            "rows": "100", 
            "page": "1",    
            "sidx": "",
            "sord": "asc"
        }

        yield scrapy.FormRequest(
            url=self.hospital_api_url,
            method='POST',
            formdata=detail_payload,
            callback=self.parse_hospital_detail,
            meta={
                'drug_info': drug_item,
                'procure_id': procure_id,
                'current_detail_page': 1,
                'payload': detail_payload,
                'parent_crawl_id': parent_crawl_id
            },
            dont_filter=True
        )

    def parse_hospital_detail(self, response):
        """Step 4: è§£æåŒ»é™¢åˆ—è¡¨å¹¶ç”Ÿæˆ Item"""
        drug_info = response.meta['drug_info']
        parent_crawl_id = response.meta['parent_crawl_id']
        current_payload = response.meta['payload']
        detail_crawl_id = str(uuid.uuid4())
        
        try:
            res_json = json.loads(response.text)
            
            # æå–åŒ»é™¢æ•°æ®
            hospitals = res_json.get("rows", [])
            total_detail_pages = int(res_json.get("total", 0))
            current_detail_page = int(response.meta['current_detail_page'])
            
            self.spider_log.info(f"ğŸ¥ è¯å“ [{drug_info.get('productName')}] è¯¦æƒ…é¡µ [{current_detail_page}/{total_detail_pages}] - å‘ç° {len(hospitals)} å®¶åŒ»é™¢")

            yield self.report_detail_page(
                crawl_id=detail_crawl_id,
                page_no=current_detail_page,
                total_pages=total_detail_pages,
                items_found=len(hospitals),
                params=current_payload,
                api_url=self.hospital_api_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=response.meta['procure_id']
            )

            item_count = 0
            # éå†å½“å‰é¡µçš„åŒ»é™¢ï¼Œç”Ÿæˆæœ€ç»ˆæ•°æ®
            for hosp_item in hospitals:
                yield self._create_item(drug_info, hosp_item, response)
                item_count += 1

            # æ›´æ–°è¯¦æƒ…é¡µé‡‡é›†çŠ¶æ€
            yield self.report_detail_page(
                crawl_id=detail_crawl_id,
                page_no=current_detail_page,
                total_pages=total_detail_pages,
                items_found=len(hospitals),
                items_stored=item_count,
                params=current_payload,
                api_url=self.hospital_api_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=response.meta['procure_id']
            )

            # --- è¯¦æƒ…é¡µç¿»é¡µé€»è¾‘ ---
            if current_detail_page < total_detail_pages:
                next_page = current_detail_page + 1
                next_payload = current_payload.copy()
                next_payload['page'] = str(next_page)
                
                yield scrapy.FormRequest(
                    url=self.hospital_api_url,
                    method='POST',
                    formdata=next_payload,
                    callback=self.parse_hospital_detail,
                    meta={
                        'drug_info': drug_info,
                        'procure_id': response.meta['procure_id'],
                        'current_detail_page': next_page,
                        'payload': next_payload,
                        'parent_crawl_id': parent_crawl_id # ä¿æŒåŒä¸€ä¸ªçˆ¶ID (åˆ—è¡¨é¡µID)
                    },
                    dont_filter=True
                )

        except Exception as e:
            self.spider_log.error(f"âŒ åŒ»é™¢è¯¦æƒ…è§£æå¤±è´¥: {e} | DrugID: {response.meta.get('procure_id')}", exc_info=True)
            
            yield self.report_error(
                stage='detail_page',
                error_msg=e,
                crawl_id=detail_crawl_id,
                params=current_payload,
                api_url=self.hospital_api_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=response.meta.get('procure_id')
            )

    def _create_item(self, drug_info, hosp_item, response=None):
        """åˆå¹¶è¯å“ä¿¡æ¯å’ŒåŒ»é™¢ä¿¡æ¯"""
        item = NingxiaDrugItem()
        
        # 1. å¡«å……è¯å“åŸºç¡€ä¿¡æ¯
        for key, value in drug_info.items():
            if key in item.fields:
                item[key] = value
                
        # 2. å¡«å……/è¦†ç›–åŒ»é™¢ç‰¹æœ‰ä¿¡æ¯
        if 'hospitalName' in hosp_item:
            item['hospitalName'] = hosp_item['hospitalName']
            
        if 'areaName' in hosp_item:
            item['areaName'] = hosp_item['areaName']
            
        # 3. è¡¥å……ç³»ç»Ÿå­—æ®µ
        item['url'] = self.hospital_api_url
        item['page_num'] = response.meta.get('current_detail_page', 1) if response else 1
        
        # ç”Ÿæˆå”¯ä¸€ID
        item.generate_md5_id()
        
        return item
