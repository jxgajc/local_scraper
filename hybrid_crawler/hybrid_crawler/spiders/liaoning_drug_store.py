from .base_spiders import BaseRequestSpider
from ..items import HybridCrawlerItem
from ..models.liaoning_drug import LiaoningDrugItem
from ..utils.logger_utils import get_spider_logger
import json
import scrapy
import pandas as pd
import uuid
from scrapy.http import JsonRequest, FormRequest
import os

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
# æ„å»ºExcelæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
excel_path = os.path.join(script_dir, "../../å…³é”®å­—é‡‡é›†(2).xlsx")

class LiaoningDrugSpider(BaseRequestSpider):
    """
    è¾½å®è¯åº—æ•°æ®çˆ¬è™«
    ç›®æ ‡: çˆ¬å–è¾½å®åŒ»ä¿å±€è¯åº—ä¿¡æ¯
    API: http://ggzy.ln.gov.cn/yphc/gzcx/
    """
    name = "liaoning_drug_store"
    
    # è¯å“åˆ—è¡¨API URL
    list_api_url = "https://ggzy.ln.gov.cn/medical" # å»é™¤äº†åŸä»£ç ä¸­çš„ç©ºæ ¼

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.spider_log = get_spider_logger(self.name)
        self.crawl_id = str(uuid.uuid4())
        
        # åŠ è½½å…³é”®è¯
        try:
            df_name = pd.read_excel(excel_path)
            self.product_list = df_name.loc[:, "é‡‡é›†å…³é”®å­—"].to_list()
            self.spider_log.info(f"ğŸš€ çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œcrawl_id: {self.crawl_id}ï¼ŒåŠ è½½å…³é”®è¯: {len(self.product_list)} ä¸ª")
        except Exception as e:
            self.spider_log.error(f"âŒ å…³é”®è¯æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            self.product_list = []

    custom_settings = {
        'CONCURRENT_REQUESTS': 8,
        'DOWNLOAD_DELAY': 3,
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'ITEM_PIPELINES': {
            'hybrid_crawler.pipelines.DataCleaningPipeline': 300,        # æ¸…æ´—
            'hybrid_crawler.pipelines.CrawlStatusPipeline': 350,         # çŠ¶æ€ç›‘æ§ (æ–°å¢)
            'hybrid_crawler.pipelines.LiaoningDrugPipeline': 400,        # å…¥åº“
        }
    }

    def start_requests(self):
        """æ„é€ åˆå§‹çš„POSTè¯·æ±‚"""
        self.spider_log.info(f"ğŸ“‹ å¼€å§‹é‡‡é›†ï¼Œå…± {len(self.product_list)} ä¸ªå…³é”®è¯")
        
        for product in self.product_list:
            form_data = {
                "apiName": "GetYPYYCG",
                "product": product,
                "company": "",
                "pageNum": "1" # æ˜¾å¼è½¬ä¸ºå­—ç¬¦ä¸²
            }
            
            # ä¸ŠæŠ¥å¼€å§‹é‡‡é›†çŠ¶æ€
            # yield {
            #     '_status_': True,
            #     'crawl_id': self.crawl_id,
            #     'stage': 'start_requests',
            #     'page_no': 1,
            #     'params': form_data,
            #     'api_url': self.list_api_url,
            #     'reference_id': product,
            #     'success': True
            # }
            
            self.spider_log.info(f"ğŸ” æ­£åœ¨é‡‡é›†å…³é”®è¯: {product}")
            
            yield FormRequest(
                url=self.list_api_url,
                method='POST',
                formdata=form_data,
                callback=self.parse_logic,
                meta={'form_data': form_data, 'crawl_id': self.crawl_id, 'keyword': product},
                dont_filter=True
            )

    def parse_logic(self, response):
        """å¤„ç†è¯å“åˆ—è¡¨ç¬¬ä¸€é¡µå“åº” + ç”Ÿæˆåç»­é¡µç è¯·æ±‚"""
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta['crawl_id']
        current_form_data = response.meta['form_data']
        keyword = response.meta['keyword']
        
        try:
            res_json = json.loads(response.text)
            
            # è·å–æ•°æ®å’Œåˆ†é¡µä¿¡æ¯
            data_block = res_json.get("data", {})
            rows = data_block.get("data", [])
            total_pages = int(data_block.get("totalPage", 0))
            total_records = int(data_block.get("totalData", 0))
            current_page = int(current_form_data.get("pageNum", 1))
            
            self.spider_log.info(f"ğŸ“„ å…³é”®è¯ [{keyword}] åˆ—è¡¨é¡µé¢ [{current_page}/{total_pages}] - å‘ç° {len(rows)} æ¡è®°å½• (æ€»è®¡: {total_records})")

            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current_page,
                'total_pages': total_pages,
                'items_found': len(rows),
                'params': current_form_data,
                'api_url': self.list_api_url,
                'reference_id': keyword,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            item_count = 0
            # 1. å¤„ç†å½“å‰é¡µçš„æ¯ä¸€æ¡è¯å“æ•°æ®
            for drug_item in rows:
                yield self._create_item(drug_item, current_page)
                item_count += 1

            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current_page,
                'total_pages': total_pages,
                'items_found': len(rows),
                'items_stored': item_count,
                'params': current_form_data,
                'api_url': self.list_api_url,
                'reference_id': keyword,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            # 2. ç”Ÿæˆå‰©ä½™é¡µç è¯·æ±‚ (ä»ç¬¬2é¡µå¼€å§‹)
            if current_page < total_pages:
                self.spider_log.info(f"ğŸ”„ å‡†å¤‡é‡‡é›†å…³é”®è¯ [{keyword}] åç»­é¡µé¢ (2-{total_pages})")
                
                # æ‰¹é‡ç”Ÿæˆåç»­è¯·æ±‚ï¼ˆå¦‚æœé¡µæ•°éå¸¸å¤šï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–ä¸ºé€’å½’æ¨¡å¼ï¼Œä½†ç›®å‰é€»è¾‘æ²¿ç”¨åŸæ„ï¼‰
                for next_page in range(2, total_pages + 1):
                    next_form_data = current_form_data.copy()
                    next_form_data['pageNum'] = str(next_page)
                    
                    yield FormRequest(
                        url=self.list_api_url,
                        method='POST',
                        formdata=next_form_data,
                        callback=self.parse_list_page, # ä½¿ç”¨ç‹¬ç«‹å›è°ƒå¤„ç†åç»­é¡µé¢
                        meta={
                            'form_data': next_form_data, 
                            'parent_crawl_id': parent_crawl_id, # åˆ—è¡¨é¡µçš„çˆ¶çº§æ˜¯ keyword çº§çš„ id
                            'keyword': keyword,
                            'page_num': next_page
                        },
                        dont_filter=True
                    )

        except Exception as e:
            self.spider_log.error(f"âŒ åˆ—è¡¨é¡µè§£æå¤±è´¥ (Page 1): {e}", exc_info=True)
            
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': 1,
                'params': current_form_data,
                'api_url': self.list_api_url,
                'reference_id': keyword,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }

    def parse_list_page(self, response):
        """å¤„ç†åç»­é¡µç çš„å“åº”"""
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta['parent_crawl_id']
        current_form_data = response.meta['form_data']
        keyword = response.meta['keyword']
        page_num = response.meta['page_num']

        try:
            res_json = json.loads(response.text)
            data_block = res_json.get("data", {})
            rows = data_block.get("data", [])
            total_pages = int(data_block.get("totalPage", 0))
            
            self.spider_log.info(f"ğŸ“„ å…³é”®è¯ [{keyword}] åˆ—è¡¨é¡µé¢ [{page_num}/{total_pages}] - å‘ç° {len(rows)} æ¡è®°å½•")
            
            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': page_num,
                'total_pages': total_pages,
                'items_found': len(rows),
                'params': current_form_data,
                'api_url': self.list_api_url,
                'reference_id': keyword,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }
            
            item_count = 0
            for item in rows:
                yield self._create_item(item, page_num)
                item_count += 1
            
            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': page_num,
                'total_pages': total_pages,
                'items_found': len(rows),
                'items_stored': item_count,
                'params': current_form_data,
                'api_url': self.list_api_url,
                'reference_id': keyword,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }
                
        except Exception as e:
            self.spider_log.error(f"âŒ é¡µé¢å¤„ç†å¤±è´¥ (Page {page_num}): {e}", exc_info=True)
            
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': page_num,
                'params': current_form_data,
                'api_url': self.list_api_url,
                'reference_id': keyword,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }
    
    def _create_item(self, drug_item, page_num):
        """
        æ„å»º LiaoningDrugItem
        :param drug_item: è¯·æ±‚è·å–çš„è¯å“ä¿¡æ¯ (Dict)
        :param page_num: é‡‡é›†é¡µç 
        """
        item = LiaoningDrugItem()
        
        # ç›´æ¥ä½¿ç”¨APIè¿”å›çš„å­—æ®µåï¼ˆé©¼å³°å‘½åï¼‰
        for field_name in item.fields:
            if field_name in ['id', 'collect_time', 'url', 'url_hash', 'page_num']:
                continue  # è·³è¿‡éœ€è¦å•ç‹¬å¤„ç†çš„å­—æ®µ
            item[field_name] = drug_item.get(field_name, '')
        
        # è®¾ç½®URLå­—æ®µ
        item['url'] = f"https://nhsa.drug/{drug_item.get('goodscode', 'unknown')}"
        
        # è®¾ç½®é¡µç 
        item['page_num'] = page_num
        
        # ç”ŸæˆMD5å”¯ä¸€IDå’Œé‡‡é›†æ—¶é—´
        item.generate_md5_id()
        
        return item