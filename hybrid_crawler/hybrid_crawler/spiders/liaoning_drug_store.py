from .base_spiders import BaseRequestSpider
from ..items import HybridCrawlerItem
from ..models.liaoning_drug import LiaoningDrugItem
from ..utils.logger_utils import get_spider_logger
import json
import scrapy
import time
import pandas as pd
import uuid
import requests
from scrapy.http import JsonRequest, FormRequest
import os
from .mixins import SpiderStatusMixin

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
# æ„å»ºExcelæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
excel_path = os.path.join(script_dir, "../../å…³é”®å­—é‡‡é›†(2).xlsx")

class LiaoningDrugSpider(SpiderStatusMixin, BaseRequestSpider):
    """
    è¾½å®è¯åº—æ•°æ®çˆ¬è™«
    ç›®æ ‡: çˆ¬å–è¾½å®åŒ»ä¿å±€è¯åº—ä¿¡æ¯
    API: http://ggzy.ln.gov.cn/yphc/gzcx/
    """
    name = "liaoning_drug_store"

    # è¯å“åˆ—è¡¨API URL
    list_api_url = "https://ggzy.ln.gov.cn/medical"

    # è¡¥é‡‡é…ç½®
    recrawl_config = {
        'table_name': 'drug_hospital_liaoning_test',
        'unique_id': 'goodscode',
    }

    @classmethod
    def fetch_all_ids_from_api(cls, logger=None, stop_check=None):
        """
        è¾½å®çˆ¬è™«æ˜¯åŸºäºå…³é”®è¯çš„ï¼Œéœ€è¦éå†å…³é”®è¯è·å–æ‰€æœ‰æ•°æ®
        è¿”å›: {goodscode: base_info} å­—å…¸
        """
        api_data = {}
        session = requests.Session()

        # åŠ è½½å…³é”®è¯
        try:
            df_name = pd.read_excel(excel_path)
            keywords = df_name.loc[:, "é‡‡é›†å…³é”®å­—"].to_list()
        except Exception as e:
            if logger:
                logger.error(f"å…³é”®è¯æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return api_data

        for keyword in keywords:
            if stop_check and stop_check():
                break

            page_num = 1
            while True:
                if stop_check and stop_check():
                    break
                try:
                    form_data = {
                        "apiName": "GetYPYYCG",
                        "product": keyword,
                        "company": "",
                        "pageNum": str(page_num)
                    }
                    response = session.post(cls.list_api_url, data=form_data, timeout=30)
                    response.raise_for_status()
                    res_json = response.json()

                    data_block = res_json.get("data", {})
                    rows = data_block.get("data", [])
                    total_pages = int(data_block.get("totalPage", 0))

                    for record in rows:
                        goods_code = record.get('goodscode')
                        if goods_code:
                            api_data[goods_code] = record

                    if logger:
                        logger.info(f"è¾½å®APIå…³é”®è¯[{keyword}]ç¬¬{page_num}/{total_pages}é¡µï¼Œè·å–{len(rows)}æ¡")

                    if page_num >= total_pages:
                        break
                    page_num += 1
                except Exception as e:
                    if logger:
                        logger.error(f"è¯·æ±‚è¾½å®APIå¤±è´¥: {e}")
                    break

        return api_data

    @classmethod
    def recrawl_by_ids(cls, missing_data, db_session, logger=None):
        """è¾½å®çˆ¬è™«è¡¥é‡‡ - ç›´æ¥ä¿å­˜ç¼ºå¤±çš„æ•°æ®"""
        from ..models.liaoning_drug import LiaoningDrug
        from datetime import datetime
        import hashlib

        success_count = 0
        for goods_code, drug_info in missing_data.items():
            time.sleep(3)
            try:
                record = LiaoningDrug(
                    goodscode=goods_code,
                    ProductName=drug_info.get('ProductName'),
                    Spec=drug_info.get('Spec'),
                    Manufacturer=drug_info.get('Manufacturer'),
                    collect_time=datetime.now()
                )
                record.md5_id = hashlib.md5(goods_code.encode()).hexdigest()
                db_session.add(record)
                success_count += 1

                if logger:
                    logger.info(f"è¡¥é‡‡ goodscode={goods_code} æˆåŠŸ")
            except Exception as e:
                if logger:
                    logger.error(f"è¡¥é‡‡ goodscode={goods_code} å¤±è´¥: {e}")

        db_session.commit()
        return success_count 

    def __init__(self, recrawl_ids=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.spider_log = get_spider_logger(self.name)
        self.crawl_id = str(uuid.uuid4())

        # è¡¥é‡‡æ¨¡å¼ï¼šåªé‡‡é›†æŒ‡å®šçš„ goodscode
        self.recrawl_ids = set(recrawl_ids.split(',')) if recrawl_ids else None
        self.recrawl_mode = self.recrawl_ids is not None

        # åŠ è½½å…³é”®è¯
        try:
            df_name = pd.read_excel(excel_path)
            self.product_list = df_name.loc[:, "é‡‡é›†å…³é”®å­—"].to_list()
            mode_str = f"è¡¥é‡‡æ¨¡å¼ï¼Œç›®æ ‡ {len(self.recrawl_ids)} æ¡" if self.recrawl_mode else "å…¨é‡é‡‡é›†"
            self.spider_log.info(f"ğŸš€ çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œcrawl_id: {self.crawl_id}ï¼Œæ¨¡å¼: {mode_str}ï¼ŒåŠ è½½å…³é”®è¯: {len(self.product_list)} ä¸ª")
        except Exception as e:
            self.spider_log.error(f"âŒ å…³é”®è¯æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            self.product_list = []

    custom_settings = {
        'CONCURRENT_REQUESTS': 8,
        'DOWNLOAD_DELAY': 3,
        # 'USER_AGENT': Handled by Middleware
        # Pipeline é…ç½®å·²ç§»è‡³å…¨å±€ settings.py
    }

    def start_requests(self):
        """æ„é€ åˆå§‹çš„POSTè¯·æ±‚"""
        self.spider_log.info(f"ğŸ“‹ å¼€å§‹é‡‡é›†ï¼Œå…± {len(self.product_list)} ä¸ªå…³é”®è¯")
        
        for product in self.product_list:
            form_data = {
                "apiName": "GetYPYYCG",
                "product": product,
                "company": "",
                "pageNum": "1" # æ˜¾å¼è½¬ä¸ºå­—ç¬¦ä¸²
            }
            
            self.spider_log.info(f"ğŸ” æ­£åœ¨é‡‡é›†å…³é”®è¯: {product}")
            
            yield FormRequest(
                url=self.list_api_url,
                method='POST',
                formdata=form_data,
                callback=self.parse_logic,
                meta={'form_data': form_data, 'crawl_id': self.crawl_id, 'keyword': product},
                dont_filter=True
            )

    def parse_logic(self, response):
        """å¤„ç†è¯å“åˆ—è¡¨ç¬¬ä¸€é¡µå“åº” + ç”Ÿæˆåç»­é¡µç è¯·æ±‚"""
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta['crawl_id']
        current_form_data = response.meta['form_data']
        keyword = response.meta['keyword']
        
        try:
            res_json = json.loads(response.text)
            
            # è·å–æ•°æ®å’Œåˆ†é¡µä¿¡æ¯
            data_block = res_json.get("data", {})
            rows = data_block.get("data", [])
            total_pages = int(data_block.get("totalPage", 0))
            total_records = int(data_block.get("totalData", 0))
            current_page = int(current_form_data.get("pageNum", 1))
            
            self.spider_log.info(f"ğŸ“„ å…³é”®è¯ [{keyword}] åˆ—è¡¨é¡µé¢ [{current_page}/{total_pages}] - å‘ç° {len(rows)} æ¡è®°å½• (æ€»è®¡: {total_records})")

            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=current_page,
                total_pages=total_pages,
                items_found=len(rows),
                params=current_form_data,
                api_url=self.list_api_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=keyword
            )

            item_count = 0
            # 1. å¤„ç†å½“å‰é¡µçš„æ¯ä¸€æ¡è¯å“æ•°æ®
            for drug_item in rows:
                goods_code = drug_item.get('goodscode')
                # è¡¥é‡‡æ¨¡å¼ï¼šè·³è¿‡ä¸åœ¨ç›®æ ‡åˆ—è¡¨ä¸­çš„è®°å½•
                if self.recrawl_mode:
                    if goods_code not in self.recrawl_ids:
                        continue
                    self.recrawl_ids.discard(goods_code)  # å·²å¤„ç†ï¼Œä»åˆ—è¡¨ç§»é™¤

                yield self._create_item(drug_item, current_page)
                item_count += 1

            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€
            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=current_page,
                total_pages=total_pages,
                items_found=len(rows),
                params=current_form_data,
                api_url=self.list_api_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=keyword,
                items_stored=item_count
            )

            # 2. ç”Ÿæˆå‰©ä½™é¡µç è¯·æ±‚ (ä»ç¬¬2é¡µå¼€å§‹)
            if current_page < total_pages:
                self.spider_log.info(f"ğŸ”„ å‡†å¤‡é‡‡é›†å…³é”®è¯ [{keyword}] åç»­é¡µé¢ (2-{total_pages})")
                
                for next_page in range(2, total_pages + 1):
                    next_form_data = current_form_data.copy()
                    next_form_data['pageNum'] = str(next_page)
                    
                    yield FormRequest(
                        url=self.list_api_url,
                        method='POST',
                        formdata=next_form_data,
                        callback=self.parse_list_page, # ä½¿ç”¨ç‹¬ç«‹å›è°ƒå¤„ç†åç»­é¡µé¢
                        meta={
                            'form_data': next_form_data, 
                            'parent_crawl_id': parent_crawl_id, # åˆ—è¡¨é¡µçš„çˆ¶çº§æ˜¯ keyword çº§çš„ id
                            'keyword': keyword,
                            'page_num': next_page
                        },
                        dont_filter=True
                    )

        except Exception as e:
            self.spider_log.error(f"âŒ åˆ—è¡¨é¡µè§£æå¤±è´¥ (Page 1): {e}", exc_info=True)
            
            yield self.report_error(
                stage='list_page',
                error_msg=e,
                crawl_id=page_crawl_id,
                params=current_form_data,
                api_url=self.list_api_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=keyword
            )

    def parse_list_page(self, response):
        """å¤„ç†åç»­é¡µç çš„å“åº”"""
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta['parent_crawl_id']
        current_form_data = response.meta['form_data']
        keyword = response.meta['keyword']
        page_num = response.meta['page_num']

        try:
            res_json = json.loads(response.text)
            data_block = res_json.get("data", {})
            rows = data_block.get("data", [])
            total_pages = int(data_block.get("totalPage", 0))
            
            self.spider_log.info(f"ğŸ“„ å…³é”®è¯ [{keyword}] åˆ—è¡¨é¡µé¢ [{page_num}/{total_pages}] - å‘ç° {len(rows)} æ¡è®°å½•")
            
            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=page_num,
                total_pages=total_pages,
                items_found=len(rows),
                params=current_form_data,
                api_url=self.list_api_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=keyword
            )
            
            item_count = 0
            for item in rows:
                goods_code = item.get('goodscode')
                # è¡¥é‡‡æ¨¡å¼ï¼šè·³è¿‡ä¸åœ¨ç›®æ ‡åˆ—è¡¨ä¸­çš„è®°å½•
                if self.recrawl_mode:
                    if goods_code not in self.recrawl_ids:
                        continue
                    self.recrawl_ids.discard(goods_code)  # å·²å¤„ç†ï¼Œä»åˆ—è¡¨ç§»é™¤

                yield self._create_item(item, page_num)
                item_count += 1
            
            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=page_num,
                total_pages=total_pages,
                items_found=len(rows),
                params=current_form_data,
                api_url=self.list_api_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=keyword,
                items_stored=item_count
            )
                
        except Exception as e:
            self.spider_log.error(f"âŒ é¡µé¢å¤„ç†å¤±è´¥ (Page {page_num}): {e}", exc_info=True)
            
            yield self.report_error(
                stage='list_page',
                error_msg=e,
                crawl_id=page_crawl_id,
                params=current_form_data,
                api_url=self.list_api_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=keyword
            )
    
    def _create_item(self, drug_item, page_num):
        """
        æ„å»º LiaoningDrugItem
        :param drug_item: è¯·æ±‚è·å–çš„è¯å“ä¿¡æ¯ (Dict)
        :param page_num: é‡‡é›†é¡µç 
        """
        item = LiaoningDrugItem()
        
        # ç›´æ¥ä½¿ç”¨APIè¿”å›çš„å­—æ®µåï¼ˆé©¼å³°å‘½åï¼‰
        for field_name in item.fields:
            if field_name in ['id', 'collect_time', 'url', 'url_hash', 'page_num']:
                continue  # è·³è¿‡éœ€è¦å•ç‹¬å¤„ç†çš„å­—æ®µ
            item[field_name] = drug_item.get(field_name, '')
        
        # è®¾ç½®URLå­—æ®µ
        item['url'] = f"https://nhsa.drug/{drug_item.get('goodscode', 'unknown')}"
        
        # è®¾ç½®é¡µç 
        item['page_num'] = page_num
        
        # ç”ŸæˆMD5å”¯ä¸€IDå’Œé‡‡é›†æ—¶é—´
        item.generate_md5_id()
        
        return item
