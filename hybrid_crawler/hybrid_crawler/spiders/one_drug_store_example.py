from ast import Param
from .base_spiders import BaseRequestSpider
from ..models.ningxia_drug import NingxiaDrugItem
from urllib.parse import urlencode
from ..utils.logger_utils import get_spider_logger
import json
import scrapy
import time
import uuid

# http://ylbzj.hebei.gov.cn/category/162
class NingxiaDrugSpider(BaseRequestSpider):
    """
    å®å¤åŒ»ä¿å±€è¯å“è®¢å•çˆ¬è™«
    ç›®æ ‡: ç›´æ¥è·å–è¯å“è®¢å•åˆ—è¡¨æ•°æ®
    Target: https://nxyp.ylbz.nx.gov.cn
    """
    name = "ningxia_drug_spider"
    
    # æ¥å£åœ°å€
    list_api_url = "https://nxyp.ylbz.nx.gov.cn/cms/recentPurchaseDetail/getRecentPurchaseDetailData.html" 
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.logger = get_spider_logger(self.name)
        self.crawl_id = str(uuid.uuid4())
        self.logger.info(f"ğŸš€ çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œcrawl_id: {self.crawl_id}")

    custom_settings = {
        'CONCURRENT_REQUESTS': 4, # æ ¹æ®æœåŠ¡å™¨å‹åŠ›é€‚å½“è°ƒæ•´
        'DOWNLOAD_DELAY': 5,
        'DEFAULT_REQUEST_HEADERS': {
            'Accept': '*/*',
            'Connection': 'keep-alive',
            'Content-Type': 'application/json',
            'User-Agent': 'PostmanRuntime-ApipostRuntime/1.1.0',
            'prodType': '2'
        },
        # ä½¿ç”¨æ•°æ®ç®¡é“
        'ITEM_PIPELINES': {
            'hybrid_crawler.pipelines.DataCleaningPipeline': 300,        # æ¸…æ´—
            'hybrid_crawler.pipelines.CrawlStatusPipeline': 350,         # çŠ¶æ€ç›‘æ§ (æ–°å¢)
            'hybrid_crawler.pipelines.NingxiaDrugPipeline': 400,         # å…¥åº“
        }
    }

    def start_requests(self):
        """æ„é€ åˆå§‹çš„POSTè¯·æ±‚"""
        payload = {
            "_search": "false",
            "page": 1,
            "rows": 1000,
            "sidx": "",
            "sord": "asc"
        }
        
        self.logger.info(f"ğŸ“‹ å¼€å§‹é‡‡é›†è¯å“è®¢å•åˆ—è¡¨ï¼Œåˆå§‹payload: {json.dumps(payload)}")
        
        # ä¸ŠæŠ¥å¼€å§‹é‡‡é›†çŠ¶æ€
        yield {
            '_status_': True,
            'crawl_id': self.crawl_id,
            'stage': 'start_requests',
            'page_no': 1,
            'params': payload,
            'api_url': self.list_api_url,
            'success': True
        }

        form_data_str = {k: str(v) for k, v in payload.items()}
        yield scrapy.FormRequest(
            url=self.list_api_url,
            method='POST',
            formdata=form_data_str,
            callback=self.parse_logic,
            meta={'payload': payload, 'crawl_id': self.crawl_id}, # ä¼ é€’ payload å’Œ crawl_id
            dont_filter=True
        )

    def parse_logic(self, response):
        """å¤„ç†è¯å“åˆ—è¡¨åˆå§‹å“åº”ï¼šå¤„ç†ç¬¬ä¸€é¡µæ•°æ® + ç”Ÿæˆåç»­é¡µç è¯·æ±‚"""
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta['crawl_id']
        current_payload = response.meta['payload']
        
        try:
            res_json = json.loads(response.text)
            
            total_pages = int(res_json.get("total", 0))
            records = res_json.get("rows", [])
            current = int(res_json.get("page", 1))
            total_records = int(res_json.get("records", 0))

            self.logger.info(f"ğŸ“„ è®¢å•åˆ—è¡¨é¡µé¢ [{current}/{total_pages}] - å‘ç° {len(records)} æ¡è®°å½• (æ€»è®¡: {total_records})")

            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current,
                'total_pages': total_pages,
                'items_found': len(records),
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            item_count = 0
            # 1. å¤„ç†å½“å‰é¡µçš„æ¯ä¸€æ¡è¯å“è®¢å•æ•°æ® -> ç›´æ¥åˆ›å»ºItem
            for order_item in records:
                yield self._create_item(order_item, current)
                item_count += 1

            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current,
                'total_pages': total_pages,
                'items_found': len(records),
                'items_stored': item_count,
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            # 2. ç”Ÿæˆå‰©ä½™é¡µç è¯·æ±‚ (ä»ç¬¬2é¡µå¼€å§‹)
            # æ³¨æ„ï¼šæ­¤å¤„ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰è¯·æ±‚ï¼Œå¹¶å‘é‡ç”± CONCURRENT_REQUESTS æ§åˆ¶
            if current < total_pages:
                self.logger.info(f"ğŸ”„ å‡†å¤‡è°ƒåº¦åç»­é¡µé¢ (2-{total_pages})")
                
                for page in range(2, total_pages + 1):
                    next_payload = current_payload.copy()
                    next_payload['page'] = page
                    
                    form_data_str = {k: str(v) for k, v in next_payload.items()}
                    yield scrapy.FormRequest(
                        url=self.list_api_url,
                        method='POST',
                        formdata=form_data_str,
                        callback=self.parse_list_page,
                        meta={
                            'payload': next_payload, 
                            'parent_crawl_id': parent_crawl_id, # åˆ—è¡¨é¡µçš„çˆ¶çº§æ˜¯ Spider çš„ crawl_id
                            'page_num': page
                        }, 
                        dont_filter=True
                    )

        except Exception as e:
            self.logger.error(f"âŒ åˆ—è¡¨é¡µè§£æå¤±è´¥ (Page 1): {e}", exc_info=True)
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': 1,
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }

    def parse_list_page(self, response):
        """å¤„ç†åç»­è¯å“åˆ—è¡¨é¡µ"""
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta['parent_crawl_id']
        current_payload = response.meta['payload']
        page_num = response.meta['page_num']
        
        try:
            res_json = json.loads(response.text)
            records = res_json.get("rows", [])
            # æ¥å£è¿”å›çš„ page å­—æ®µå¯èƒ½ä¸ºå­—ç¬¦ä¸²
            api_page = int(res_json.get('page', page_num))
            total_pages = int(res_json.get("total", 0))
            
            self.logger.info(f"ğŸ“„ è®¢å•åˆ—è¡¨é¡µé¢ [{api_page}/{total_pages}] - å‘ç° {len(records)} æ¡è®°å½•")
            
            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': api_page,
                'total_pages': total_pages,
                'items_found': len(records),
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }
            
            item_count = 0
            for order_item in records:
                yield self._create_item(order_item, api_page)
                item_count += 1
            
            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': api_page,
                'total_pages': total_pages,
                'items_found': len(records),
                'items_stored': item_count,
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }
                
        except Exception as e:
            self.logger.error(f"âŒ åˆ†é¡µè§£æå¤±è´¥ Page {page_num}: {e}", exc_info=True)
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': page_num,
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }

    def _create_item(self, order_item, page_num=1):
        """
        æ„å»º NingxiaDrugItem
        :param order_item: ä»APIè·å–çš„è®¢å•ä¿¡æ¯ (Dict)
        :param page_num: é‡‡é›†é¡µç 
        """
        item = NingxiaDrugItem()
        
        # 1. è®¾ç½®è®¢å•ä¿¡æ¯å­—æ®µ
        for field_name in item.fields:
            if field_name in ['md5_id', 'collect_time', 'url', 'url_hash', 'page_num']:
                continue  # è·³è¿‡éœ€è¦å•ç‹¬å¤„ç†çš„å­—æ®µ
            if field_name in order_item:
                item[field_name] = order_item[field_name]
        
        # 2. è®¾ç½®URLå­—æ®µ
        item['url'] = f"{self.list_api_url}?page={page_num}"
        
        # 3. è®¾ç½®é¡µç 
        item['page_num'] = page_num
        
        # 4. ç”ŸæˆMD5å”¯ä¸€IDå’Œé‡‡é›†æ—¶é—´
        item.generate_md5_id()
        
        return item