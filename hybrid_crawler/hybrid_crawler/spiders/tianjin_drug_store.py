import scrapy
import json
import random
import string
import uuid
import requests
from ..models.tianjin_drug import TianjinDrugItem
from scrapy.http import JsonRequest
from ..utils.logger_utils import get_spider_logger
import pandas as pd
import os
from .mixins import SpiderStatusMixin

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
# æ„å»ºExcelæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
excel_path = os.path.join(script_dir, "../../å…³é”®å­—é‡‡é›†(2).xlsx")

class TianjinDrugSpider(SpiderStatusMixin, scrapy.Spider):
    """
    å¤©æ´¥å¸‚åŒ»è¯é‡‡è´­ä¸­å¿ƒ - è¯å“åŠé…é€åŒ»é™¢æŸ¥è¯¢
    Target: https://tps.ylbz.tj.gov.cn
    """
    name = "tianjin_drug_spider"

    # æ¥å£åœ°å€
    drug_list_url = "https://tps.ylbz.tj.gov.cn/csb/1.0.0/guideGetMedList"
    hospital_list_url = "https://tps.ylbz.tj.gov.cn/csb/1.0.0/guideGetHosp"

    # è¡¥é‡‡é…ç½®
    recrawl_config = {
        'table_name': 'drug_hospital_tianjin_test',
        'unique_id': 'med_id',
    }

    @staticmethod
    def _get_verification_code():
        """ç”Ÿæˆ4ä½éšæœºå­—æ¯æ•°å­—æ··åˆéªŒè¯ç """
        chars = string.ascii_lowercase + string.digits
        return ''.join(random.choices(chars, k=4))

    @classmethod
    def fetch_all_ids_from_api(cls, logger=None, stop_check=None):
        """
        å¤©æ´¥çˆ¬è™«éœ€è¦éªŒè¯ç ï¼Œéå†å…³é”®è¯è·å–æ‰€æœ‰æ•°æ®
        è¿”å›: {med_id: base_info} å­—å…¸
        """
        api_data = {}
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Content-Type': 'application/json',
        })

        # åŠ è½½å…³é”®è¯
        try:
            df_name = pd.read_excel(excel_path)
            keywords = df_name.loc[:, "é‡‡é›†å…³é”®å­—"].to_list()
        except Exception as e:
            if logger:
                logger.error(f"å…³é”®è¯æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return api_data

        for keyword in keywords:
            if stop_check and stop_check():
                break
            try:
                payload = {
                    "verificationCode": cls._get_verification_code(),
                    "content": keyword
                }
                response = session.post(cls.drug_list_url, json=payload, timeout=30)
                response.raise_for_status()
                res_json = response.json()

                if res_json.get("code") != 200:
                    continue

                data = res_json.get("data", {})
                drug_list = data.get("list", [])

                for drug in drug_list:
                    med_id = drug.get('medid')
                    if med_id:
                        api_data[med_id] = {
                            'med_id': med_id,
                            'gen_name': drug.get('genname'),
                            'prod_name': drug.get('prodname'),
                            'dosform': drug.get('dosform'),
                            'spec': drug.get('spec'),
                            'pac': drug.get('pac'),
                            'prod_entp': drug.get('prodentp'),
                            'source_data': json.dumps(drug, ensure_ascii=False)
                        }

                if logger:
                    logger.info(f"å¤©æ´¥APIå…³é”®è¯[{keyword}]è·å–{len(drug_list)}æ¡")

            except Exception as e:
                if logger:
                    logger.error(f"è¯·æ±‚å¤©æ´¥APIå¤±è´¥: {e}")

        return api_data

    @classmethod
    def recrawl_by_ids(cls, missing_data, db_session, logger=None):
        """æ ¹æ®ç¼ºå¤±çš„ med_id åŠå…¶åŸºç¡€ä¿¡æ¯è°ƒç”¨åŒ»é™¢APIè¿›è¡Œè¡¥é‡‡"""
        from ..models.tianjin_drug import TianjinDrug
        from datetime import datetime
        import hashlib

        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Content-Type': 'application/json',
        })

        success_count = 0
        for med_id, base_info in missing_data.items():
            try:
                hospital_payload = {
                    "verificationCode": cls._get_verification_code(),
                    "genname": base_info.get('gen_name'),
                    "dosform": base_info.get('dosform'),
                    "spec": base_info.get('spec'),
                    "pac": base_info.get('pac')
                }

                resp = session.post(cls.hospital_list_url, json=hospital_payload, timeout=30)
                resp.raise_for_status()
                res_json = resp.json()

                if res_json.get("code") != 200:
                    continue

                data = res_json.get("data", {})
                hosp_list = data.get("list", [])

                if hosp_list:
                    for hosp in hosp_list:
                        record = TianjinDrug(
                            **base_info,
                            has_hospital_record=True,
                            hs_name=hosp.get('hsname'),
                            hs_lav=hosp.get('hslav'),
                            got_time=hosp.get('gottime'),
                            collect_time=datetime.now()
                        )
                        field_values = {'med_id': med_id, 'hs_name': hosp.get('hsname')}
                        record.md5_id = hashlib.md5(
                            json.dumps(field_values, sort_keys=True, ensure_ascii=False).encode()
                        ).hexdigest()
                        db_session.add(record)
                else:
                    record = TianjinDrug(
                        **base_info,
                        has_hospital_record=False,
                        collect_time=datetime.now()
                    )
                    record.md5_id = hashlib.md5(med_id.encode()).hexdigest()
                    db_session.add(record)

                success_count += 1
                if logger:
                    logger.info(f"è¡¥é‡‡ med_id={med_id} æˆåŠŸ")

            except Exception as e:
                if logger:
                    logger.error(f"è¡¥é‡‡ med_id={med_id} å¤±è´¥: {e}")

        db_session.commit()
        return success_count
    
    def __init__(self, recrawl_ids=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.spider_log = get_spider_logger(self.name)
        self.crawl_id = str(uuid.uuid4())

        # è¡¥é‡‡æ¨¡å¼ï¼šåªé‡‡é›†æŒ‡å®šçš„ med_id
        self.recrawl_ids = set(recrawl_ids.split(',')) if recrawl_ids else None
        self.recrawl_mode = self.recrawl_ids is not None

        # åŠ è½½å…³é”®è¯
        try:
            df_name = pd.read_excel(excel_path)
            self.search_contents = df_name.loc[:, "é‡‡é›†å…³é”®å­—"].to_list()
            mode_str = f"è¡¥é‡‡æ¨¡å¼ï¼Œç›®æ ‡ {len(self.recrawl_ids)} æ¡" if self.recrawl_mode else "å…¨é‡é‡‡é›†"
            self.spider_log.info(f"ğŸš€ çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œcrawl_id: {self.crawl_id}ï¼Œæ¨¡å¼: {mode_str}ï¼ŒåŠ è½½å…³é”®è¯: {len(self.search_contents)} ä¸ª")
        except Exception as e:
            self.spider_log.error(f"âŒ å…³é”®è¯æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            self.search_contents = []

    custom_settings = {
        'CONCURRENT_REQUESTS': 3, # ç¨å¾®é™ä½å¹¶å‘ï¼Œé¿å…éªŒè¯ç æ¥å£é£æ§è¿‡ä¸¥
        'DOWNLOAD_DELAY': 3,
        'DEFAULT_REQUEST_HEADERS': {
            'Content-Type': 'application/json',
            'Accept': 'application/json, text/plain, */*',
            'Sec-Fetch-Site': 'same-origin',
            'Accept-Language': 'zh-CN,zh-Hans;q=0.9',
            'Sec-Fetch-Mode': 'cors',
            'Origin': 'https://tps.ylbz.tj.gov.cn',
            # 'User-Agent': Handled by RandomUserAgentMiddleware
            'Referer': 'https://tps.ylbz.tj.gov.cn/drugGuide/tps-local/b/',
            'Sec-Fetch-Dest': 'empty',
            'Priority': 'u=3, i'
        },
        # Pipeline é…ç½®å·²ç§»è‡³å…¨å±€ settings.py
    }

    def get_verification_code(self):
        """ç”Ÿæˆ4ä½éšæœºå­—æ¯æ•°å­—æ··åˆéªŒè¯ç """
        chars = string.ascii_lowercase + string.digits
        return ''.join(random.choices(chars, k=4))

    def start_requests(self):
        """éå†å…³é”®è¯å‘èµ·è¯·æ±‚"""
        total_keywords = len(self.search_contents)
        self.spider_log.info(f"ğŸ“‹ å¼€å§‹é‡‡é›†ï¼Œå…± {total_keywords} ä¸ªå…³é”®è¯")
        
        for index, content in enumerate(self.search_contents):
            payload = {
                "verificationCode": self.get_verification_code(),
                "content": content
            }
            
            yield JsonRequest(
                url=self.drug_list_url,
                method='POST',
                data=payload,
                callback=self.parse_drug_list,
                meta={
                    'keyword': content, 
                    'crawl_id': self.crawl_id, 
                    'payload': payload,
                    'keyword_index': index + 1,
                    'total_keywords': total_keywords
                },
                dont_filter=True
            )

    def parse_drug_list(self, response):
        """è§£æè¯å“åˆ—è¡¨"""
        page_crawl_id = str(uuid.uuid4())
        keyword = response.meta['keyword']
        parent_crawl_id = response.meta['crawl_id']
        current_payload = response.meta['payload']
        
        # ä½¿ç”¨å…³é”®è¯è¿›åº¦ä½œä¸ºä»»åŠ¡è¿›åº¦
        current_page = response.meta['keyword_index']
        total_pages = response.meta['total_keywords']
        
        try:
            res_json = json.loads(response.text)
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if res_json.get("code") != 200:
                error_msg = res_json.get('message', 'Unknown Error')
                self.spider_log.error(f"âŒ å…³é”®è¯ [{keyword}] åˆ—è¡¨APIé”™è¯¯: {error_msg}")
                
                yield self.report_error(
                    stage='list_page',
                    error_msg=error_msg,
                    crawl_id=page_crawl_id,
                    params=current_payload,
                    api_url=self.drug_list_url,
                    parent_crawl_id=parent_crawl_id,
                    reference_id=keyword
                )
                return

            data = res_json.get("data", {})
            drug_list = data.get("list", [])
            
            if not drug_list:
                self.spider_log.info(f"ğŸ“„ å…³é”®è¯ [{keyword}] ({current_page}/{total_pages}) æœªæ‰¾åˆ°è¯å“è®°å½•")
                yield self.report_list_page(
                    crawl_id=page_crawl_id,
                    page_no=current_page,
                    total_pages=total_pages,
                    items_found=0,
                    params=current_payload,
                    api_url=self.drug_list_url,
                    parent_crawl_id=parent_crawl_id,
                    reference_id=keyword
                )
                return

            self.spider_log.info(f"ğŸ“„ å…³é”®è¯ [{keyword}] ({current_page}/{total_pages}) å‘ç° {len(drug_list)} æ¡è¯å“è®°å½•")
            
            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=current_page,
                total_pages=total_pages,
                items_found=len(drug_list),
                params=current_payload,
                api_url=self.drug_list_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=keyword
            )

            item_count = 0
            for drug in drug_list:
                med_id = drug.get('medid')
                # è¡¥é‡‡æ¨¡å¼ï¼šè·³è¿‡ä¸åœ¨ç›®æ ‡åˆ—è¡¨ä¸­çš„è®°å½•
                if self.recrawl_mode:
                    if med_id not in self.recrawl_ids:
                        continue
                    self.recrawl_ids.discard(med_id)  # å·²å¤„ç†ï¼Œä»åˆ—è¡¨ç§»é™¤

                # 1. æå–è¯å“åŸºç¡€ä¿¡æ¯
                base_info = {
                    'med_id': drug.get('medid'),
                    'gen_name': drug.get('genname'),
                    'prod_name': drug.get('prodname'),
                    'dosform': drug.get('dosform'),
                    'spec': drug.get('spec'),
                    'pac': drug.get('pac'),
                    'conv_rat': drug.get('convrat'),
                    'min_sal_unt': drug.get('minSalunt'),
                    'prod_entp': drug.get('prodentp'),
                    'aprv_no': drug.get('aprvno'),
                    'source_data': json.dumps(drug, ensure_ascii=False)
                }

                # 2. æ„å»ºåŒ»é™¢æŸ¥è¯¢å‚æ•°
                hospital_payload = {
                    "verificationCode": self.get_verification_code(),
                    "genname": drug.get('genname'),
                    "dosform": drug.get('dosform'),
                    "spec": drug.get('spec'),
                    "pac": drug.get('pac')
                }

                # å‘èµ·åŒ»é™¢è¯¦æƒ…è¯·æ±‚
                yield JsonRequest(
                    url=self.hospital_list_url,
                    method='POST',
                    data=hospital_payload,
                    callback=self.parse_hospital_list,
                    meta={
                        'base_info': base_info, 
                        'parent_crawl_id': page_crawl_id,
                        'payload': hospital_payload
                    },
                    dont_filter=True
                )
                item_count += 1
            
            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€
            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=current_page,
                total_pages=total_pages,
                items_found=len(drug_list),
                items_stored=item_count,
                params=current_payload,
                api_url=self.drug_list_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=keyword
            )

        except Exception as e:
            self.spider_log.error(f"âŒ è§£æè¯å“åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
            yield self.report_error(
                stage='list_page',
                error_msg=e,
                crawl_id=page_crawl_id,
                params=current_payload,
                api_url=self.drug_list_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=keyword
            )

    def parse_hospital_list(self, response):
        """è§£æåŒ»é™¢åˆ—è¡¨å¹¶ç”Ÿæˆæœ€ç»ˆItem"""
        base_info = response.meta['base_info']
        parent_crawl_id = response.meta['parent_crawl_id']
        current_payload = response.meta['payload']
        detail_crawl_id = str(uuid.uuid4())
        
        try:
            res_json = json.loads(response.text)
            
            if res_json.get("code") != 200:
                msg = res_json.get('message', 'Unknown Error')
                self.spider_log.warning(f"âš ï¸ è¯å“ [{base_info['gen_name']}] åŒ»é™¢APIè­¦å‘Š: {msg}")
                
                # å³ä½¿åŒ»é™¢æ¥å£æŠ¥é”™ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©ä¿å­˜è¯å“åŸºç¡€ä¿¡æ¯
                # ä½†è¿™é‡Œæˆ‘ä»¬è®°å½•é”™è¯¯çŠ¶æ€
                yield self.report_error(
                    stage='detail_page',
                    error_msg=msg,
                    crawl_id=detail_crawl_id,
                    params=current_payload,
                    api_url=self.hospital_list_url,
                    parent_crawl_id=parent_crawl_id,
                    reference_id=base_info.get('med_id')
                )
                return

            data = res_json.get("data", {})
            hosp_list = data.get("list", [])
            
            self.spider_log.info(f"ğŸ¥ è¯å“ [{base_info['gen_name']}] å‘ç° {len(hosp_list)} å®¶åŒ»é™¢")
            
            # ä¼˜åŒ–ï¼šç§»é™¤å†—ä½™çŠ¶æ€ä¸ŠæŠ¥

            item_count = 0
            if hosp_list:
                for hosp in hosp_list:
                    item = TianjinDrugItem()
                    item.update(base_info)
                    
                    # æ³¨å…¥åŒ»é™¢ä¿¡æ¯
                    item['has_hospital_record'] = True
                    item['hs_name'] = hosp.get('hsname')
                    item['hs_lav'] = hosp.get('hslav')
                    item['got_time'] = hosp.get('gottime')
                    
                    item.generate_md5_id()
                    yield item
                    item_count += 1
            else:
                # æ— åŒ»é™¢è®°å½•ï¼Œä»…ä¿å­˜è¯å“ä¿¡æ¯
                item = TianjinDrugItem()
                item.update(base_info)
                item['has_hospital_record'] = False
                item['hs_name'] = None
                item['hs_lav'] = None
                item['got_time'] = None
                
                item.generate_md5_id()
                yield item
                item_count += 1
            
            # æ›´æ–°è¯¦æƒ…é¡µé‡‡é›†çŠ¶æ€
            yield self.report_detail_page(
                crawl_id=detail_crawl_id,
                page_no=1,
                items_found=len(hosp_list),
                items_stored=item_count,
                params=current_payload,
                api_url=self.hospital_list_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=base_info.get('med_id')
            )

        except Exception as e:
            self.spider_log.error(f"âŒ è§£æåŒ»é™¢åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
            yield self.report_error(
                stage='detail_page',
                error_msg=e,
                crawl_id=detail_crawl_id,
                params=current_payload,
                api_url=self.hospital_list_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=base_info.get('med_id')
            )
