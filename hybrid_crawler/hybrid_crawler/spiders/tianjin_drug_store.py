import scrapy
import json
import random
import string
import uuid
from ..models.tianjin_drug import TianjinDrugItem
from scrapy.http import JsonRequest
from ..utils.logger_utils import get_spider_logger
import pandas as pd
import os
from .mixins import SpiderStatusMixin

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
# æ„å»ºExcelæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
excel_path = os.path.join(script_dir, "../../å…³é”®å­—é‡‡é›†(2).xlsx")

class TianjinDrugSpider(SpiderStatusMixin, scrapy.Spider):
    """
    å¤©æ´¥å¸‚åŒ»è¯é‡‡è´­ä¸­å¿ƒ - è¯å“åŠé…é€åŒ»é™¢æŸ¥è¯¢
    Target: https://tps.ylbz.tj.gov.cn
    """
    name = "tianjin_drug_spider"
    
    # æ¥å£åœ°å€
    drug_list_url = "https://tps.ylbz.tj.gov.cn/csb/1.0.0/guideGetMedList"
    hospital_list_url = "https://tps.ylbz.tj.gov.cn/csb/1.0.0/guideGetHosp"
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.spider_log = get_spider_logger(self.name)
        self.crawl_id = str(uuid.uuid4())
        
        # åŠ è½½å…³é”®è¯
        try:
            df_name = pd.read_excel(excel_path)
            self.search_contents = df_name.loc[:, "é‡‡é›†å…³é”®å­—"].to_list()
            self.spider_log.info(f"ğŸš€ çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œcrawl_id: {self.crawl_id}ï¼ŒåŠ è½½å…³é”®è¯: {len(self.search_contents)} ä¸ª")
        except Exception as e:
            self.spider_log.error(f"âŒ å…³é”®è¯æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            self.search_contents = []

    custom_settings = {
        'CONCURRENT_REQUESTS': 3, # ç¨å¾®é™ä½å¹¶å‘ï¼Œé¿å…éªŒè¯ç æ¥å£é£æ§è¿‡ä¸¥
        'DOWNLOAD_DELAY': 3,
        'DEFAULT_REQUEST_HEADERS': {
            'Content-Type': 'application/json',
            'Accept': 'application/json, text/plain, */*',
            'Sec-Fetch-Site': 'same-origin',
            'Accept-Language': 'zh-CN,zh-Hans;q=0.9',
            'Sec-Fetch-Mode': 'cors',
            'Origin': 'https://tps.ylbz.tj.gov.cn',
            # 'User-Agent': Handled by RandomUserAgentMiddleware
            'Referer': 'https://tps.ylbz.tj.gov.cn/drugGuide/tps-local/b/',
            'Sec-Fetch-Dest': 'empty',
            'Priority': 'u=3, i'
        },
        # Pipeline é…ç½®å·²ç§»è‡³å…¨å±€ settings.py
    }

    def get_verification_code(self):
        """ç”Ÿæˆ4ä½éšæœºå­—æ¯æ•°å­—æ··åˆéªŒè¯ç """
        chars = string.ascii_lowercase + string.digits
        return ''.join(random.choices(chars, k=4))

    def start_requests(self):
        """éå†å…³é”®è¯å‘èµ·è¯·æ±‚"""
        self.spider_log.info(f"ğŸ“‹ å¼€å§‹é‡‡é›†ï¼Œå…± {len(self.search_contents)} ä¸ªå…³é”®è¯")
        
        for content in self.search_contents:
            payload = {
                "verificationCode": self.get_verification_code(),
                "content": content
            }
            
            yield JsonRequest(
                url=self.drug_list_url,
                method='POST',
                data=payload,
                callback=self.parse_drug_list,
                meta={'keyword': content, 'crawl_id': self.crawl_id, 'payload': payload},
                dont_filter=True
            )

    def parse_drug_list(self, response):
        """è§£æè¯å“åˆ—è¡¨"""
        page_crawl_id = str(uuid.uuid4())
        keyword = response.meta['keyword']
        parent_crawl_id = response.meta['crawl_id']
        current_payload = response.meta['payload']
        
        try:
            res_json = json.loads(response.text)
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if res_json.get("code") != 200:
                error_msg = res_json.get('message', 'Unknown Error')
                self.spider_log.error(f"âŒ å…³é”®è¯ [{keyword}] åˆ—è¡¨APIé”™è¯¯: {error_msg}")
                
                yield self.report_error(
                    stage='list_page',
                    error_msg=error_msg,
                    crawl_id=page_crawl_id,
                    params=current_payload,
                    api_url=self.drug_list_url,
                    parent_crawl_id=parent_crawl_id,
                    reference_id=keyword
                )
                return

            data = res_json.get("data", {})
            drug_list = data.get("list", [])
            
            if not drug_list:
                self.spider_log.info(f"ğŸ“„ å…³é”®è¯ [{keyword}] æœªæ‰¾åˆ°è¯å“è®°å½•")
                yield self.report_list_page(
                    crawl_id=page_crawl_id,
                    page_no=1,
                    items_found=0,
                    params=current_payload,
                    api_url=self.drug_list_url,
                    parent_crawl_id=parent_crawl_id,
                    reference_id=keyword
                )
                return

            self.spider_log.info(f"ğŸ“„ å…³é”®è¯ [{keyword}] å‘ç° {len(drug_list)} æ¡è¯å“è®°å½•")
            
            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=1,
                items_found=len(drug_list),
                params=current_payload,
                api_url=self.drug_list_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=keyword
            )

            item_count = 0
            for drug in drug_list:
                # 1. æå–è¯å“åŸºç¡€ä¿¡æ¯
                base_info = {
                    'med_id': drug.get('medid'),
                    'gen_name': drug.get('genname'),
                    'prod_name': drug.get('prodname'),
                    'dosform': drug.get('dosform'),
                    'spec': drug.get('spec'),
                    'pac': drug.get('pac'),
                    'conv_rat': drug.get('convrat'),
                    'min_sal_unt': drug.get('minSalunt'),
                    'prod_entp': drug.get('prodentp'),
                    'aprv_no': drug.get('aprvno'),
                    'source_data': json.dumps(drug, ensure_ascii=False)
                }

                # 2. æ„å»ºåŒ»é™¢æŸ¥è¯¢å‚æ•°
                hospital_payload = {
                    "verificationCode": self.get_verification_code(),
                    "genname": drug.get('genname'),
                    "dosform": drug.get('dosform'),
                    "spec": drug.get('spec'),
                    "pac": drug.get('pac')
                }

                # å‘èµ·åŒ»é™¢è¯¦æƒ…è¯·æ±‚
                yield JsonRequest(
                    url=self.hospital_list_url,
                    method='POST',
                    data=hospital_payload,
                    callback=self.parse_hospital_list,
                    meta={
                        'base_info': base_info, 
                        'parent_crawl_id': page_crawl_id,
                        'payload': hospital_payload
                    },
                    dont_filter=True
                )
                item_count += 1
            
            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€
            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=1,
                items_found=len(drug_list),
                items_stored=item_count,
                params=current_payload,
                api_url=self.drug_list_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=keyword
            )

        except Exception as e:
            self.spider_log.error(f"âŒ è§£æè¯å“åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
            yield self.report_error(
                stage='list_page',
                error_msg=e,
                crawl_id=page_crawl_id,
                params=current_payload,
                api_url=self.drug_list_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=keyword
            )

    def parse_hospital_list(self, response):
        """è§£æåŒ»é™¢åˆ—è¡¨å¹¶ç”Ÿæˆæœ€ç»ˆItem"""
        base_info = response.meta['base_info']
        parent_crawl_id = response.meta['parent_crawl_id']
        current_payload = response.meta['payload']
        detail_crawl_id = str(uuid.uuid4())
        
        try:
            res_json = json.loads(response.text)
            
            if res_json.get("code") != 200:
                msg = res_json.get('message', 'Unknown Error')
                self.spider_log.warning(f"âš ï¸ è¯å“ [{base_info['gen_name']}] åŒ»é™¢APIè­¦å‘Š: {msg}")
                
                # å³ä½¿åŒ»é™¢æ¥å£æŠ¥é”™ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©ä¿å­˜è¯å“åŸºç¡€ä¿¡æ¯
                # ä½†è¿™é‡Œæˆ‘ä»¬è®°å½•é”™è¯¯çŠ¶æ€
                yield self.report_error(
                    stage='detail_page',
                    error_msg=msg,
                    crawl_id=detail_crawl_id,
                    params=current_payload,
                    api_url=self.hospital_list_url,
                    parent_crawl_id=parent_crawl_id,
                    reference_id=base_info.get('med_id')
                )
                return

            data = res_json.get("data", {})
            hosp_list = data.get("list", [])
            
            self.spider_log.info(f"ğŸ¥ è¯å“ [{base_info['gen_name']}] å‘ç° {len(hosp_list)} å®¶åŒ»é™¢")
            
            # ä¸ŠæŠ¥è¯¦æƒ…é¡µé‡‡é›†çŠ¶æ€
            yield self.report_detail_page(
                crawl_id=detail_crawl_id,
                page_no=1,
                items_found=len(hosp_list),
                params=current_payload,
                api_url=self.hospital_list_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=base_info.get('med_id')
            )

            item_count = 0
            if hosp_list:
                for hosp in hosp_list:
                    item = TianjinDrugItem()
                    item.update(base_info)
                    
                    # æ³¨å…¥åŒ»é™¢ä¿¡æ¯
                    item['has_hospital_record'] = True
                    item['hs_name'] = hosp.get('hsname')
                    item['hs_lav'] = hosp.get('hslav')
                    item['got_time'] = hosp.get('gottime')
                    
                    item.generate_md5_id()
                    yield item
                    item_count += 1
            else:
                # æ— åŒ»é™¢è®°å½•ï¼Œä»…ä¿å­˜è¯å“ä¿¡æ¯
                item = TianjinDrugItem()
                item.update(base_info)
                item['has_hospital_record'] = False
                item['hs_name'] = None
                item['hs_lav'] = None
                item['got_time'] = None
                
                item.generate_md5_id()
                yield item
                item_count += 1
            
            # æ›´æ–°è¯¦æƒ…é¡µé‡‡é›†çŠ¶æ€
            yield self.report_detail_page(
                crawl_id=detail_crawl_id,
                page_no=1,
                items_found=len(hosp_list),
                items_stored=item_count,
                params=current_payload,
                api_url=self.hospital_list_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=base_info.get('med_id')
            )

        except Exception as e:
            self.spider_log.error(f"âŒ è§£æåŒ»é™¢åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
            yield self.report_error(
                stage='detail_page',
                error_msg=e,
                crawl_id=detail_crawl_id,
                params=current_payload,
                api_url=self.hospital_list_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=base_info.get('med_id')
            )
