import scrapy
import json
import time
import base64
import ddddocr
import uuid
from ..models.shandong_drug import ShandongDrugItem
from scrapy.http import JsonRequest 
import pandas as pd
from ..utils.logger_utils import get_spider_logger
import os

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
# æ„å»ºExcelæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
excel_path = os.path.join(script_dir, "../../å…³é”®å­—é‡‡é›†(2).xlsx")

class ShandongDrugSpider(scrapy.Spider):
    name = "drug_hosipital_shandong"
    
    # æ¥å£ URL
    index_url = "https://ypjc.ybj.shandong.gov.cn/trade/drug/query-of-hanging-directory/index"
    captcha_url = "https://ypjc.ybj.shandong.gov.cn/code/hsaTrade/tps-local/web/gwml/getPicVerCode"
    list_api_url = "https://ypjc.ybj.shandong.gov.cn/code/hsaTrade/tps-local/web/gwml/listDrug"
    hospital_api_url = "https://ypjc.ybj.shandong.gov.cn/code/hsaTrade/tps-local/web/gwml/listHospital"

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.spider_log = get_spider_logger(self.name)
        self.crawl_id = str(uuid.uuid4())
        
        # åˆå§‹åŒ– OCR
        self.ocr = ddddocr.DdddOcr(show_ad=False)
        
        # åŠ è½½å…³é”®è¯
        try:
            df_name = pd.read_excel(excel_path)
            self.product_names = df_name.loc[:, "é‡‡é›†å…³é”®å­—"].to_list()
            self.spider_log.info(f"ğŸš€ çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œcrawl_id: {self.crawl_id}ï¼ŒåŠ è½½å…³é”®è¯: {len(self.product_names)} ä¸ª")
        except Exception as e:
            self.spider_log.error(f"âŒ å…³é”®è¯æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            self.product_names = []

    custom_settings = {
        'CONCURRENT_REQUESTS': 1, # ä¿æŒä½å¹¶å‘ï¼Œé¿å…éªŒè¯ç å°ç¦
        'DOWNLOAD_DELAY': 2,
        'DEFAULT_REQUEST_HEADERS': {
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh-Hans;q=0.9',
            'Connection': 'keep-alive',
            'Content-Type': 'application/json;charset=utf-8',
            'Origin': 'https://ypjc.ybj.shandong.gov.cn',
            'Referer': 'https://ypjc.ybj.shandong.gov.cn/trade/drug/query-of-hanging-directory/index',
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.5 Safari/605.1.15',
            'queryToken': '05ea8b36dcbc4cbf925d1eb65324dd96', 
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'Priority': 'u=3, i'
        },
        'ITEM_PIPELINES': {
            'hybrid_crawler.pipelines.DataCleaningPipeline': 300,        # æ¸…æ´—
            'hybrid_crawler.pipelines.CrawlStatusPipeline': 350,         # çŠ¶æ€ç›‘æ§ (æ–°å¢)
            'hybrid_crawler.pipelines.ShandongDrugPipeline': 400,        # å…¥åº“
        }
    }

    def start_requests(self):
        """ç¬¬ä¸€æ­¥ï¼šè®¿é—®ç´¢å¼•é¡µé¢è·å–å¿…è¦çš„Cookie"""
        self.spider_log.info(f"ğŸ“‹ å¼€å§‹é‡‡é›†ï¼Œé˜Ÿåˆ—ä¸­å…± {len(self.product_names)} ä¸ªå…³é”®è¯")
        yield scrapy.Request(url=self.index_url, method='GET', callback=self.parse_index, dont_filter=True)

    def parse_index(self, response):
        """
        ç¬¬äºŒæ­¥ï¼šå¤„ç†ç´¢å¼•é¡µé¢ï¼Œå¹¶å¼€å§‹ä¸ºæ¯ä¸ªå…³é”®å­—å‘èµ·éªŒè¯ç è¯·æ±‚
        """
        self.spider_log.info("âœ… ç´¢å¼•é¡µé¢è®¿é—®æˆåŠŸï¼Œå¼€å§‹è°ƒåº¦å…³é”®è¯ä»»åŠ¡")
        
        for prod_name in self.product_names:
            timestamp = int(time.time() * 1000)
            url = f"{self.captcha_url}?timestamp={timestamp}"
            
            # ä¸ŠæŠ¥å¼€å§‹é‡‡é›†çŠ¶æ€ (é’ˆå¯¹æ¯ä¸ªå…³é”®è¯)
            # yield {
            #     '_status_': True,
            #     'crawl_id': self.crawl_id,
            #     'stage': 'start_requests',
            #     'page_no': 1,
            #     'params': {'keyword': prod_name},
            #     'api_url': self.list_api_url,
            #     'reference_id': prod_name,
            #     'success': True
            # }
            
            yield JsonRequest(
                url=url, 
                method='GET', 
                callback=self.parse_captcha, 
                meta={
                    'keyword': prod_name, 
                    'retry_count': 0,
                    'crawl_id': self.crawl_id # ä¼ é€’æ ¹ID
                },
                dont_filter=True
            )

    def parse_captcha(self, response):
        """ç¬¬ä¸‰æ­¥ï¼šè¯†åˆ«éªŒè¯ç å¹¶å‘èµ·å•æ¬¡è¯å“åˆ—è¡¨æŸ¥è¯¢"""
        current_keyword = response.meta.get('keyword')
        retry_payload = response.meta.get('retry_payload') 
        parent_crawl_id = response.meta.get('crawl_id')
        
        try:
            res_json = json.loads(response.text)
            if not res_json.get("success"):
                self.spider_log.error(f"âŒ [{current_keyword}] éªŒè¯ç æ¥å£æŠ¥é”™: {response.text}")
                return

            data = res_json.get("data", {})
            base64_str = data.get("base64Str", "")
            random_str = data.get("randomStr", "")
            resp_text = data.get("text", "") 

            if not base64_str:
                self.spider_log.error(f"âŒ [{current_keyword}] æœªæ‰¾åˆ°éªŒè¯ç å›¾ç‰‡æ•°æ®")
                return

            # ddddocr è¯†åˆ«
            img_bytes = base64.b64decode(base64_str.split(',')[-1])
            code_result = self.ocr.classification(img_bytes)
            
            self.spider_log.debug(f"ğŸ”¢ [{current_keyword}] éªŒè¯ç è¯†åˆ«ç»“æœ: {code_result}")

            # æ„é€ è¯·æ±‚ Payload
            if retry_payload:
                payload = retry_payload
                payload.update({
                    "randomStr": random_str,
                    "text": resp_text,
                    "code": code_result
                })
            else:
                payload = {
                    "current": 1,
                    "size": 100,
                    "randomStr": random_str,
                    "text": resp_text, 
                    "prodCode": "",
                    "prodName": current_keyword,
                    "prodentpName": "",
                    "purchaseType": "",
                    "queryType": "1",
                    "code": code_result
                }

            yield JsonRequest(
                url=self.list_api_url,
                method='POST',
                data=payload,
                callback=self.parse_list,
                meta={
                    'payload': payload,
                    'keyword': current_keyword,
                    'retry_count': response.meta.get('retry_count', 0),
                    'parent_crawl_id': parent_crawl_id
                },
                dont_filter=True
            )

        except Exception as e:
            self.spider_log.error(f"âŒ [{current_keyword}] è§£æéªŒè¯ç å“åº”å¼‚å¸¸: {e}", exc_info=True)

    def parse_list(self, response):
        """ç¬¬å››æ­¥ï¼šè§£æè¯å“åˆ—è¡¨"""
        page_crawl_id = str(uuid.uuid4())
        current_keyword = response.meta.get('keyword', 'Unknown')
        parent_crawl_id = response.meta.get('parent_crawl_id')
        current_payload = response.meta.get('payload')
        
        try:
            res_json = json.loads(response.text)
            
            if not res_json.get("success"):
                error_code = res_json.get("code")
                # æ£€æŸ¥æ˜¯å¦ä¸ºéªŒè¯ç é”™è¯¯ï¼ˆcode=160003ï¼‰
                if error_code == 160003:
                    retry_count = response.meta.get('retry_count', 0)
                    max_retries = 5
                    
                    if retry_count < max_retries:
                        self.spider_log.warning(f"âš ï¸ [{current_keyword}] éªŒè¯ç é”™è¯¯ï¼Œå‡†å¤‡é‡è¯• ({retry_count + 1}/{max_retries})")
                        
                        timestamp = int(time.time() * 1000)
                        captcha_url = f"{self.captcha_url}?timestamp={timestamp}"
                        
                        yield JsonRequest(
                            url=captcha_url,
                            method='GET',
                            callback=self.parse_captcha,
                            meta={
                                'keyword': current_keyword,
                                'retry_count': retry_count + 1,
                                'retry_payload': current_payload,
                                'crawl_id': parent_crawl_id # ä¿æŒ ID ä¼ é€’
                            },
                            dont_filter=True
                        )
                        return
                    else:
                        self.spider_log.error(f"âŒ [{current_keyword}] éªŒè¯ç é”™è¯¯æ¬¡æ•°è¿‡å¤šï¼Œæ”¾å¼ƒè¯¥è¯")
                        return
                else:
                    self.spider_log.warning(f"âŒ [{current_keyword}] åˆ—è¡¨è¯·æ±‚å¼‚å¸¸: {res_json.get('msg', 'Unknown')}")
                    
                    yield {
                        '_status_': True,
                        'crawl_id': page_crawl_id,
                        'stage': 'list_page',
                        'page_no': current_payload.get('current'),
                        'params': current_payload,
                        'api_url': self.list_api_url,
                        'reference_id': current_keyword,
                        'success': False,
                        'error_message': res_json.get('msg', 'Unknown Error'),
                        'parent_crawl_id': parent_crawl_id
                    }
                    return

            # --- æ­£å¸¸æ•°æ®å¤„ç†é€»è¾‘ ---
            data_block = res_json.get("data", {})
            records = data_block.get("records", [])
            current_page = data_block.get("current", 1)
            total_pages = data_block.get("pages", 0)
            total_records = data_block.get("total", 0)

            self.spider_log.info(f"ğŸ“„ å…³é”®è¯ [{current_keyword}] åˆ—è¡¨é¡µé¢ [{current_page}/{total_pages}] - å‘ç° {len(records)} æ¡è®°å½• (æ€»è®¡: {total_records})")

            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current_page,
                'total_pages': total_pages,
                'items_found': len(records),
                'params': current_payload,
                'api_url': self.list_api_url,
                'reference_id': current_keyword,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            item_count = 0
            for record in records:
                base_info = {
                    'prodCode': record.get('prodCode'),
                    'prodName': record.get('prodName'),
                    'prodentpName': record.get('prodentpName'),
                    'spec': record.get('prodSpec'),
                    'pac': record.get('prodPac'),
                    'price': record.get('pubonlnPricStr'),
                    'aprvno': record.get('aprvno'),
                    'manufacture_name': record.get('marketPermitHolder') or record.get('scqyName'),
                    'public_time': record.get('optTime'),
                    'source_data': json.dumps(record, ensure_ascii=False)
                }
                
                pubonln_id = record.get('pubonlnId')

                if pubonln_id:
                    hospital_payload = {
                        "current": 1,
                        "size": 50, 
                        "randomStr": "",
                        "text": "",
                        "medinsName": "",
                        "basicFlag": "",
                        "queryType": "0",
                        "code": "",
                        "procureCatalogId": pubonln_id
                    }
                    
                    yield JsonRequest(
                        url=self.hospital_api_url,
                        method='POST',
                        data=hospital_payload,
                        callback=self.parse_hospital,
                        meta={
                            'base_info': base_info, 
                            'payload': hospital_payload,
                            'keyword': current_keyword,
                            'parent_crawl_id': page_crawl_id,
                            'prod_code': base_info['prodCode']
                        },
                        dont_filter=True
                    )
                    item_count += 1
                else:
                    item = ShandongDrugItem()
                    for k, v in base_info.items():
                        item[k] = v
                    item['has_hospital_record'] = False
                    item.generate_md5_id()
                    yield item
                    item_count += 1

            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current_page,
                'total_pages': total_pages,
                'items_found': len(records),
                'items_stored': item_count,
                'params': current_payload,
                'api_url': self.list_api_url,
                'reference_id': current_keyword,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            # ç¿»é¡µé€»è¾‘
            if current_page < total_pages:
                self.spider_log.info(f"ğŸ”„ å‡†å¤‡é‡‡é›†å…³é”®è¯ [{current_keyword}] ä¸‹ä¸€é¡µ [{current_page + 1}/{total_pages}]")
                next_page = current_page + 1
                new_payload = current_payload.copy()
                new_payload['current'] = next_page
                
                # å‡è®¾ç¿»é¡µSessionä¿æŒï¼Œæ— éœ€é‡æ–°éªŒè¯ç 
                yield JsonRequest(
                    url=self.list_api_url,
                    method='POST',
                    data=new_payload,
                    callback=self.parse_list,
                    meta={
                        'payload': new_payload,
                        'keyword': current_keyword,
                        'parent_crawl_id': parent_crawl_id 
                    },
                    dont_filter=True
                )

        except Exception as e:
            self.spider_log.error(f"âŒ [{current_keyword}] è§£æè¯å“åˆ—è¡¨é¡µå¼‚å¸¸: {e}", exc_info=True)
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current_payload.get('current'),
                'params': current_payload,
                'api_url': self.list_api_url,
                'reference_id': current_keyword,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }

    def parse_hospital(self, response):
        """è§£æåŒ»é™¢è¯¦æƒ…"""
        base_info = response.meta['base_info']
        current_payload = response.meta['payload']
        parent_crawl_id = response.meta['parent_crawl_id']
        prod_code = response.meta.get('prod_code')
        keyword = response.meta.get('keyword')
        detail_crawl_id = str(uuid.uuid4())
        
        try:
            res_json = json.loads(response.text)
            
            if not res_json.get("success"):
                msg = res_json.get('msg', 'Unknown Error')
                self.spider_log.warning(f"âš ï¸ åŒ»é™¢æ¥å£è¯·æ±‚å¤±è´¥: {msg}")
                
                yield {
                    '_status_': True,
                    'crawl_id': detail_crawl_id,
                    'stage': 'detail_page',
                    'page_no': current_payload['current'],
                    'params': current_payload,
                    'api_url': self.hospital_api_url,
                    'reference_id': prod_code,
                    'success': False,
                    'error_message': msg,
                    'parent_crawl_id': parent_crawl_id
                }
                return

            data = res_json.get("data", {})
            records = data.get("records", [])
            current_page = data.get("current", 1)
            total_pages = data.get("pages", 0)
            
            self.spider_log.info(f"ğŸ¥ è¯å“ [{base_info['prodName']}] è¯¦æƒ…é¡µ [{current_page}/{total_pages}] - å‘ç° {len(records)} å®¶åŒ»é™¢")
            
            # ä¸ŠæŠ¥è¯¦æƒ…é¡µé‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': detail_crawl_id,
                'stage': 'detail_page',
                'page_no': current_page,
                'total_pages': total_pages,
                'items_found': len(records),
                'params': current_payload,
                'api_url': self.hospital_api_url,
                'reference_id': prod_code,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            item_count = 0
            if not records:
                # æ— åŒ»é™¢è®°å½•ï¼Œä»…ä¿å­˜åŸºç¡€ä¿¡æ¯
                item = ShandongDrugItem()
                for k, v in base_info.items():
                    item[k] = v
                item['has_hospital_record'] = False
                item.generate_md5_id()
                yield item
                item_count += 1
            else:
                for hosp in records:
                    item = ShandongDrugItem()
                    for k, v in base_info.items():
                        item[k] = v
                    item['has_hospital_record'] = True
                    item['hospitalName'] = hosp.get('hospitalName')
                    item['hospitalId'] = hosp.get('hospitalId')
                    item['cityName'] = hosp.get('cityName')
                    item['cotyName'] = hosp.get('cotyName')
                    item['admdvsName'] = hosp.get('admdvsName')
                    item['drugPurchasePropertyStr'] = hosp.get('drugPurchasePropertyStr')
                    item['userName'] = hosp.get('userName')
                    item['admdvs'] = hosp.get('admdvs')
                    item.generate_md5_id()
                    yield item
                    item_count += 1

            # æ›´æ–°è¯¦æƒ…é¡µé‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': detail_crawl_id,
                'stage': 'detail_page',
                'page_no': current_page,
                'total_pages': total_pages,
                'items_found': len(records),
                'items_stored': item_count,
                'params': current_payload,
                'api_url': self.hospital_api_url,
                'reference_id': prod_code,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            if current_page < total_pages:
                next_page = current_page + 1
                new_payload = current_payload.copy()
                new_payload['current'] = next_page
                yield JsonRequest(
                    url=self.hospital_api_url,
                    method='POST',
                    data=new_payload,
                    callback=self.parse_hospital,
                    meta={
                        'base_info': base_info,
                        'payload': new_payload,
                        'keyword': keyword,
                        'parent_crawl_id': parent_crawl_id,
                        'prod_code': prod_code
                    },
                    dont_filter=True
                )
        except Exception as e:
            self.spider_log.error(f"âŒ è§£æåŒ»é™¢è¯¦æƒ…é¡µå¼‚å¸¸: {e}", exc_info=True)
            yield {
                '_status_': True,
                'crawl_id': detail_crawl_id,
                'stage': 'detail_page',
                'page_no': current_payload.get('current'),
                'params': current_payload,
                'api_url': self.hospital_api_url,
                'reference_id': prod_code,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }