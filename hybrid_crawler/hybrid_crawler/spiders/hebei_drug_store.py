from .base_spiders import BaseRequestSpider
from ..models.hebei_drug import HebeiDrugItem
from ..utils.logger_utils import get_spider_logger
from urllib.parse import urlencode
import json
import scrapy
import time
import uuid
import requests
from .mixins import SpiderStatusMixin

# http://ylbzj.hebei.gov.cn/category/162
class HebeiDrugSpider(SpiderStatusMixin, BaseRequestSpider):
    """
    æ²³åŒ—åŒ»ä¿å±€è¯å“åŠé‡‡è´­åŒ»é™¢çˆ¬è™«
    ç›®æ ‡: å…ˆè·å–è¯å“åˆ—è¡¨ï¼Œå†æ ¹æ® prodCode è·å–é‡‡è´­è¯¥è¯å“çš„åŒ»é™¢ä¿¡æ¯
    """
    name = "hebei_drug_spider"

    # API Endpoints
    list_api_url = "https://ylbzj.hebei.gov.cn/templates/default_pc/syyypqxjzcg/queryPubonlnDrudInfoList"
    hospital_api_url = "https://ylbzj.hebei.gov.cn/templates/default_pc/syyypqxjzcg/queryProcurementMedinsList"

    # è¡¥é‡‡é…ç½®
    recrawl_config = {
        'table_name': 'drug_hospital_hebei_test',
        'unique_id': 'prodCode',
    }

    @classmethod
    def fetch_all_ids_from_api(cls, logger=None, stop_check=None):
        """ä»å®˜ç½‘ API è·å–æ‰€æœ‰ prodCode åŠå…¶åŸºç¡€ä¿¡æ¯"""
        api_data = {}
        current = 1
        page_size = 1000
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
            'prodType': '2'
        })

        while True:
            if stop_check and stop_check():
                break
            try:
                params = {"pageNo": current, "pageSize": page_size, "prodName": "", "prodentpName": ""}
                response = session.get(cls.list_api_url, params=params, timeout=30)
                response.raise_for_status()
                res_json = response.json()

                data_block = res_json.get("data", {})
                records = data_block.get("list", [])
                total_pages = int(data_block.get("pages", 0))
                current_page = int(data_block.get("pageNo", 1))

                for record in records:
                    prod_code = record.get('prodCode')
                    if prod_code:
                        api_data[prod_code] = record  # ä¿å­˜å®Œæ•´è®°å½•ä½œä¸º base_info

                if logger:
                    logger.info(f"æ²³åŒ—APIç¬¬{current_page}/{total_pages}é¡µï¼Œè·å–{len(records)}æ¡")

                if current_page >= total_pages:
                    break
                current += 1
            except Exception as e:
                if logger:
                    logger.error(f"è¯·æ±‚æ²³åŒ—APIå¤±è´¥: {e}")
                break

        return api_data

    @classmethod
    def recrawl_by_ids(cls, missing_data, db_session, logger=None):
        """æ ¹æ®ç¼ºå¤±çš„ prodCode åŠå…¶åŸºç¡€ä¿¡æ¯ç›´æ¥è°ƒç”¨è¯¦æƒ…APIè¿›è¡Œè¡¥é‡‡"""
        from ..models.hebei_drug import HebeiDrug
        from datetime import datetime
        import hashlib

        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
            'prodType': '2'
        })

        success_count = 0
        for prod_code, drug_info in missing_data.items():
            try:
                prodentp_code = drug_info.get("prodentpCode")
                if not prodentp_code:
                    continue

                params = {
                    "pageNo": 1, "pageSize": 1000,
                    "prodCode": prod_code, "prodEntpCode": prodentp_code, "isPublicHospitals": ""
                }
                resp = session.get(cls.hospital_api_url, params=params, timeout=30)
                resp.raise_for_status()
                res_json = resp.json()

                hospital_list = res_json.get("list", []) or []

                record = HebeiDrug(
                    prodCode=prod_code,
                    prodName=drug_info.get('prodName'),
                    prodentpName=drug_info.get('prodentpName'),
                    prodentpCode=prodentp_code,
                    hospital_purchases=hospital_list,
                    collect_time=datetime.now()
                )
                record.md5_id = hashlib.md5(prod_code.encode()).hexdigest()
                db_session.add(record)

                success_count += 1
                if logger:
                    logger.info(f"è¡¥é‡‡ prodCode={prod_code} æˆåŠŸï¼ŒåŒ»é™¢æ•°: {len(hospital_list)}")

            except Exception as e:
                if logger:
                    logger.error(f"è¡¥é‡‡ prodCode={prod_code} å¤±è´¥: {e}")

        db_session.commit()
        return success_count

    # å­˜å‚¨cookie
    cookies = {}
    
    def __init__(self, recrawl_ids=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.spider_log = get_spider_logger(self.name)
        self.crawl_id = str(uuid.uuid4())
        # è¡¥é‡‡æ¨¡å¼ï¼šåªé‡‡é›†æŒ‡å®šçš„ prodCode
        self.recrawl_ids = set(recrawl_ids.split(',')) if recrawl_ids else None
        self.recrawl_mode = self.recrawl_ids is not None
        mode_str = f"è¡¥é‡‡æ¨¡å¼ï¼Œç›®æ ‡ {len(self.recrawl_ids)} æ¡" if self.recrawl_mode else "å…¨é‡é‡‡é›†"
        self.spider_log.info(f"ğŸš€ çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œcrawl_id: {self.crawl_id}ï¼Œæ¨¡å¼: {mode_str}")
    

    custom_settings = {
        'CONCURRENT_REQUESTS': 4, # æ ¹æ®æœåŠ¡å™¨å‹åŠ›é€‚å½“è°ƒæ•´
        'DOWNLOAD_DELAY': 5,
        'DEFAULT_REQUEST_HEADERS': {
            'Accept': '*/*',
            'Connection': 'keep-alive',
            'Content-Type': 'application/json',
            # 'User-Agent': Handled by RandomUserAgentMiddleware
            'prodType': '2'
        },
        # Pipeline é…ç½®å·²ç§»è‡³å…¨å±€ settings.py
    }

    def start_requests(self):
        """æ„é€ åˆå§‹çš„GETè¯·æ±‚"""
        payload = {
            "pageNo": 1,
            "pageSize": 1000, 
            "prodName": "",
            "prodentpName": ""
        }
        query_string = urlencode(payload)
        full_url = f"{self.list_api_url}?{query_string}"
        
        self.spider_log.info(f"ğŸ“‹ å¼€å§‹é‡‡é›†è¯å“åˆ—è¡¨ï¼Œåˆå§‹payload: {json.dumps(payload)}")
        
        # å‘èµ·ç¬¬ä¸€é¡µè¯·æ±‚ï¼ˆä¸éœ€è¦cookieï¼‰
        yield scrapy.Request(
            url=full_url,
            method='GET',
            callback=self.parse_logic,
            meta={'payload': payload, 'crawl_id': self.crawl_id},
            dont_filter=True
        )

    def parse_logic(self, response):
        """å¤„ç†è¯å“åˆ—è¡¨åˆå§‹å“åº”ï¼šå¤„ç†ç¬¬ä¸€é¡µæ•°æ® + ç”Ÿæˆåç»­é¡µç è¯·æ±‚"""
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta['crawl_id']
        current_payload = response.meta['payload']
        
        try:
            # æ›´æ–°cookies
            if response.headers.getlist('Set-Cookie'):
                self._update_cookies(response)
            
            res_json = json.loads(response.text)
            
            data_block = res_json.get("data", {})
            total_pages = int(data_block.get("pages", 0))
            records = data_block.get("list", [])
            current = data_block.get("pageNo", 1)
            page_size = data_block.get("pageSize", 1000)

            self.spider_log.info(f"ğŸ“„ åˆ—è¡¨é¡µé¢ [{current}/{total_pages}] - å‘ç° {len(records)} æ¡è¯å“è®°å½•")
            
            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=current,
                total_pages=total_pages,
                items_found=len(records),
                params=current_payload,
                api_url=self.list_api_url,
                parent_crawl_id=parent_crawl_id,
                page_size=page_size
            )

            item_count = 0
            # 1. å¤„ç†å½“å‰é¡µçš„æ¯ä¸€æ¡è¯å“æ•°æ® -> å‘èµ·è¯¦æƒ…è¯·æ±‚
            for drug_item in records:
                prod_code = drug_item.get("prodCode")
                # è¡¥é‡‡æ¨¡å¼ï¼šè·³è¿‡ä¸åœ¨ç›®æ ‡åˆ—è¡¨ä¸­çš„è®°å½•
                if self.recrawl_mode:
                    if prod_code not in self.recrawl_ids:
                        continue
                    self.recrawl_ids.discard(prod_code)  # å·²å¤„ç†ï¼Œä»åˆ—è¡¨ç§»é™¤

                # ä¼ å…¥ page_crawl_id ä½œä¸º parent_crawl_id
                for request in self._request_hospital_detail(drug_item, current, page_crawl_id):
                    yield request
                    item_count += 1

            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€ï¼Œè®°å½•è§¦å‘çš„è¯¦æƒ…é¡µè¯·æ±‚æ•°é‡
            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=current,
                total_pages=total_pages,
                items_found=len(records),
                params=current_payload,
                api_url=self.list_api_url,
                parent_crawl_id=parent_crawl_id,
                page_size=page_size,
                items_stored=item_count
            )

            # 2. ç”Ÿæˆå‰©ä½™é¡µç è¯·æ±‚ (ä»ç¬¬2é¡µå¼€å§‹)
            if current < total_pages:
                self.spider_log.info(f"ğŸ”„ å‡†å¤‡é‡‡é›†ä¸‹ä¸€é¡µåˆ—è¡¨ [{current + 1}/{total_pages}]")
                for page in range(2, total_pages + 1):
                    next_payload = current_payload.copy()
                    next_payload['pageNo'] = page
                    
                    query_string = urlencode(next_payload)
                    full_url = f"{self.list_api_url}?{query_string}"
                    
                    yield scrapy.Request(
                        url=full_url,
                        method='GET',
                        callback=self.parse_list_page,
                        meta={
                            'page_num': page, 
                            'payload': next_payload, 
                            'parent_crawl_id': parent_crawl_id # åˆ—è¡¨é¡µçš„çˆ¶çº§æ˜¯ root crawl_id
                        },
                        dont_filter=True
                    )

        except Exception as e:
            self.spider_log.error(f"âŒ åˆ—è¡¨é¡µé¢è§£æå¤±è´¥ (Page 1): {e}", exc_info=True)
            
            yield self.report_error(
                stage='list_page',
                error_msg=e,
                crawl_id=page_crawl_id,
                params=current_payload,
                api_url=self.list_api_url,
                parent_crawl_id=parent_crawl_id
            )

    def parse_list_page(self, response):
        """å¤„ç†åç»­è¯å“åˆ—è¡¨é¡µ"""
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta.get('parent_crawl_id')
        page_num = response.meta.get('page_num', 1)
        current_payload = response.meta.get('payload', {})
        
        try:
            if response.headers.getlist('Set-Cookie'):
                self._update_cookies(response)
            
            res_json = json.loads(response.text)
            data_block = res_json.get("data", {})
            records = data_block.get("list", [])
            total_pages = data_block.get("pages", 0)
            page_size = data_block.get("pageSize", 1000)
            
            self.spider_log.info(f"ğŸ“„ åˆ—è¡¨é¡µé¢ [{page_num}/{total_pages}] - å‘ç° {len(records)} æ¡è¯å“è®°å½•")
            
            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=page_num,
                total_pages=total_pages,
                items_found=len(records),
                params=current_payload,
                api_url=self.list_api_url,
                parent_crawl_id=parent_crawl_id,
                page_size=page_size
            )
            
            item_count = 0
            for drug_item in records:
                prod_code = drug_item.get("prodCode")
                # è¡¥é‡‡æ¨¡å¼ï¼šè·³è¿‡ä¸åœ¨ç›®æ ‡åˆ—è¡¨ä¸­çš„è®°å½•
                if self.recrawl_mode:
                    if prod_code not in self.recrawl_ids:
                        continue
                    self.recrawl_ids.discard(prod_code)  # å·²å¤„ç†ï¼Œä»åˆ—è¡¨ç§»é™¤

                for request in self._request_hospital_detail(drug_item, page_num, page_crawl_id):
                    yield request
                    item_count += 1
            
            yield self.report_list_page(
                crawl_id=page_crawl_id,
                page_no=page_num,
                total_pages=total_pages,
                items_found=len(records),
                params=current_payload,
                api_url=self.list_api_url,
                parent_crawl_id=parent_crawl_id,
                page_size=page_size,
                items_stored=item_count
            )
                
        except Exception as e:
            self.spider_log.error(f"âŒ åˆ†é¡µè§£æå¤±è´¥ Page {page_num}: {e}", exc_info=True)
            
            yield self.report_error(
                stage='list_page',
                error_msg=e,
                crawl_id=page_crawl_id,
                params=current_payload,
                api_url=self.list_api_url,
                parent_crawl_id=parent_crawl_id
            )

    def _request_hospital_detail(self, drug_item, page_num, parent_crawl_id):
        """æ„é€ è·å–åŒ»é™¢è¯¦æƒ…çš„è¯·æ±‚ï¼Œå°† drug_item ä¼ é€’ä¸‹å»"""
        prodentp_code = drug_item.get("prodentpCode")
        prod_code = drug_item.get("prodCode")
        
        if not prodentp_code:
            self.spider_log.warning(f"âš ï¸ ç¼ºå°‘ prodentpCodeï¼Œè·³è¿‡è¯¦æƒ…æŸ¥è¯¢: {drug_item.get('prodName')}")
            return

        # æ„é€ è¯¦æƒ…é¡µ Payload
        payload = {
            "pageNo": 1,
            "pageSize": 1000,
            "prodCode": prod_code,
            "prodEntpCode": prodentp_code,
            "isPublicHospitals": ""
        }
        query_string = urlencode(payload)
        full_url = f"{self.hospital_api_url}?{query_string}"
        
        # è¿™é‡Œçš„ meta éå¸¸é‡è¦ï¼Œç”¨æ¥ä¼ é€’ç¬¬ä¸€æ­¥è·å–çš„ info å’Œé¡µç 
        yield scrapy.Request(
            url=full_url,
            method='GET',
            callback=self.parse_detail,
            meta={
                'drug_info': drug_item, 
                'page_num': page_num,
                'parent_crawl_id': parent_crawl_id,
                'payload': payload
            }, 
            cookies=self.cookies if self.cookies else None,
            dont_filter=True
        )

    def parse_detail(self, response):
        """å¤„ç†åŒ»é™¢è¯¦æƒ…å“åº”ï¼Œåˆå¹¶æ•°æ®å¹¶ç”Ÿæˆ Item"""
        drug_info = response.meta['drug_info']
        page_num = response.meta.get('page_num', 1)
        parent_crawl_id = response.meta['parent_crawl_id']
        current_payload = response.meta['payload']
        detail_crawl_id = str(uuid.uuid4())
        
        try:
            # å°è¯•æ›´æ–°cookies (éƒ¨åˆ†ç«™ç‚¹è¯¦æƒ…é¡µä¹Ÿä¼šset-cookie)
            if response.headers.getlist('Set-Cookie'):
                self._update_cookies(response)
                
            res_json = json.loads(response.text)
            
            hospital_list = res_json.get("list", [])
            # å…¼å®¹å¯èƒ½çš„ null æˆ–ä¸åŒç»“æ„
            if hospital_list is None:
                hospital_list = []
            
            self.spider_log.info(f"ğŸ¥ è¯å“ [{drug_info.get('prodName')}] è¯¦æƒ…é¡µ - å‘ç° {len(hospital_list)} å®¶åŒ»é™¢è®°å½•")
            
            # ä¼˜åŒ–ï¼šç§»é™¤å†—ä½™çŠ¶æ€ä¸ŠæŠ¥

            # 3. åˆ›å»ºåˆå¹¶åçš„æ•°æ® Item
            item = self._create_item(drug_info, hospital_list, page_num)
            yield item
            
            # æ›´æ–°çŠ¶æ€ï¼Œç¡®è®¤å…¥åº“ 1 æ¡ (èšåˆäº†æ‰€æœ‰åŒ»é™¢ä¿¡æ¯)
            yield self.report_detail_page(
                crawl_id=detail_crawl_id,
                page_no=page_num,
                items_found=len(hospital_list),
                params=current_payload,
                api_url=self.hospital_api_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=drug_info.get('prodCode'),
                items_stored=1,
                total_pages=1
            )

        except Exception as e:
            self.spider_log.error(f"âŒ è¯¦æƒ…é¡µè§£æå¤±è´¥: {e} | URL: {response.url}", exc_info=True)
            
            yield self.report_error(
                stage='detail_page',
                error_msg=e,
                crawl_id=detail_crawl_id,
                params=current_payload,
                api_url=self.hospital_api_url,
                parent_crawl_id=parent_crawl_id,
                reference_id=drug_info.get('prodCode')
            )

    def _create_item(self, drug_info, hospital_list, page_num=1):
        """
        æ„å»º HebeiDrugItem
        """
        item = HebeiDrugItem()
        
        prodentp_code = drug_info.get("prodentpCode")
        prod_code = drug_info.get("prodCode")
        
        # 1. è®¾ç½®è¯å“åŸºç¡€ä¿¡æ¯
        for field_name in item.fields:
            if field_name in ['md5_id', 'collect_time', 'url', 'url_hash', 'hospital_purchases', 'page_num']:
                continue
            if field_name in drug_info:
                item[field_name] = drug_info[field_name]
        
        # 2. è®¾ç½®åŒ»é™¢é‡‡è´­ä¿¡æ¯
        item['hospital_purchases'] = hospital_list
        
        # 3. è®¾ç½®URLå­—æ®µ
        item['url'] = f"{self.hospital_api_url}?pageNo=1&pageSize=1000&prodCode={prod_code}&prodEntpCode={prodentp_code}"
        
        # 4. è®¾ç½®é¡µç 
        item['page_num'] = page_num
        
        # 5. ç”ŸæˆMD5å”¯ä¸€ID
        item.generate_md5_id()
        
        return item
        
    def _update_cookies(self, response):
        """
        ä»å“åº”ä¸­æå–å¹¶æ›´æ–°cookies
        """
        try:
            for cookie_header in response.headers.getlist('Set-Cookie'):
                cookie_str = cookie_header.decode('utf-8')
                if '=' in cookie_str:
                    cookie_parts = cookie_str.split(';')[0].split('=')
                    if len(cookie_parts) >= 2:
                        cookie_name = cookie_parts[0].strip()
                        cookie_value = '='.join(cookie_parts[1:]).strip()
                        self.cookies[cookie_name] = cookie_value
            # self.spider_log.debug(f"æ›´æ–°åçš„cookies: {self.cookies}")
        except Exception as e:
            self.spider_log.warning(f"Cookies æ›´æ–°å¤±è´¥: {e}")
