from ast import Param
from .base_spiders import BaseRequestSpider
from ..models.hebei_drug import HebeiDrugItem
from ..utils.logger_utils import get_spider_logger
from urllib.parse import urlencode
import json
import scrapy
import time
import uuid

# http://ylbzj.hebei.gov.cn/category/162
class HebeiDrugSpider(BaseRequestSpider):
    """
    æ²³åŒ—åŒ»ä¿å±€è¯å“åŠé‡‡è´­åŒ»é™¢çˆ¬è™«
    ç›®æ ‡: å…ˆè·å–è¯å“åˆ—è¡¨ï¼Œå†æ ¹æ® prodCode è·å–é‡‡è´­è¯¥è¯å“çš„åŒ»é™¢ä¿¡æ¯
    """
    name = "hebei_drug_spider"
    
    # API Endpoints
    list_api_url = "https://ylbzj.hebei.gov.cn/templates/default_pc/syyypqxjzcg/queryPubonlnDrudInfoList" 
    hospital_api_url = "https://ylbzj.hebei.gov.cn/templates/default_pc/syyypqxjzcg/queryProcurementMedinsList"
    
    # å­˜å‚¨cookie
    cookies = {}
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.spider_log = get_spider_logger(self.name)
        self.crawl_id = str(uuid.uuid4())
        self.spider_log.info(f"ğŸš€ çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œcrawl_id: {self.crawl_id}")
    

    custom_settings = {
        'CONCURRENT_REQUESTS': 4, # æ ¹æ®æœåŠ¡å™¨å‹åŠ›é€‚å½“è°ƒæ•´
        'DOWNLOAD_DELAY': 5,
        'DEFAULT_REQUEST_HEADERS': {
            'Accept': '*/*',
            'Connection': 'keep-alive',
            'Content-Type': 'application/json',
            'User-Agent': 'PostmanRuntime-ApipostRuntime/1.1.0',
            'prodType': '2'
        },
        'ITEM_PIPELINES': {
            'hybrid_crawler.pipelines.DataCleaningPipeline': 300,        # æ¸…æ´—
            'hybrid_crawler.pipelines.CrawlStatusPipeline': 350,         # çŠ¶æ€ç›‘æ§ (æ–°å¢)
            'hybrid_crawler.pipelines.HebeiDrugPipeline': 400,           # å…¥åº“
        }
    }

    def start_requests(self):
        """æ„é€ åˆå§‹çš„GETè¯·æ±‚"""
        payload = {
            "pageNo": 1,
            "pageSize": 1000, 
            "prodName": "",
            "prodentpName": ""
        }
        query_string = urlencode(payload)
        full_url = f"{self.list_api_url}?{query_string}"
        
        self.spider_log.info(f"ğŸ“‹ å¼€å§‹é‡‡é›†è¯å“åˆ—è¡¨ï¼Œåˆå§‹payload: {json.dumps(payload)}")
        
        # ä¸ŠæŠ¥å¼€å§‹é‡‡é›†çŠ¶æ€
        # yield {
        #     '_status_': True,
        #     'crawl_id': self.crawl_id,
        #     'stage': 'start_requests',
        #     'page_no': 1,
        #     'params': payload,
        #     'api_url': self.list_api_url,
        #     'success': True
        # }
        
        # å‘èµ·ç¬¬ä¸€é¡µè¯·æ±‚ï¼ˆä¸éœ€è¦cookieï¼‰
        yield scrapy.Request(
            url=full_url,
            method='GET',
            callback=self.parse_logic,
            meta={'payload': payload, 'crawl_id': self.crawl_id},
            dont_filter=True
        )

    def parse_logic(self, response):
        """å¤„ç†è¯å“åˆ—è¡¨åˆå§‹å“åº”ï¼šå¤„ç†ç¬¬ä¸€é¡µæ•°æ® + ç”Ÿæˆåç»­é¡µç è¯·æ±‚"""
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta['crawl_id']
        current_payload = response.meta['payload']
        
        try:
            # æ›´æ–°cookies
            if response.headers.getlist('Set-Cookie'):
                self._update_cookies(response)
            
            res_json = json.loads(response.text)
            
            data_block = res_json.get("data", {})
            total_pages = int(data_block.get("pages", 0))
            records = data_block.get("list", [])
            current = data_block.get("pageNo", 1)
            page_size = data_block.get("pageSize", 1000)

            self.spider_log.info(f"ğŸ“„ åˆ—è¡¨é¡µé¢ [{current}/{total_pages}] - å‘ç° {len(records)} æ¡è¯å“è®°å½•")
            
            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current,
                'total_pages': total_pages,
                'page_size': page_size,
                'items_found': len(records),
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            item_count = 0
            # 1. å¤„ç†å½“å‰é¡µçš„æ¯ä¸€æ¡è¯å“æ•°æ® -> å‘èµ·è¯¦æƒ…è¯·æ±‚
            for drug_item in records:
                # ä¼ å…¥ page_crawl_id ä½œä¸º parent_crawl_id
                for request in self._request_hospital_detail(drug_item, current, page_crawl_id):
                    yield request
                    item_count += 1

            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€ï¼Œè®°å½•è§¦å‘çš„è¯¦æƒ…é¡µè¯·æ±‚æ•°é‡
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': current,
                'total_pages': total_pages,
                'items_found': len(records),
                'items_stored': item_count, 
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            # 2. ç”Ÿæˆå‰©ä½™é¡µç è¯·æ±‚ (ä»ç¬¬2é¡µå¼€å§‹)
            if current < total_pages:
                self.spider_log.info(f"ğŸ”„ å‡†å¤‡é‡‡é›†ä¸‹ä¸€é¡µåˆ—è¡¨ [{current + 1}/{total_pages}]")
                for page in range(2, total_pages + 1):
                    next_payload = current_payload.copy()
                    next_payload['pageNo'] = page
                    
                    query_string = urlencode(next_payload)
                    full_url = f"{self.list_api_url}?{query_string}"
                    
                    yield scrapy.Request(
                        url=full_url,
                        method='GET',
                        callback=self.parse_list_page,
                        meta={
                            'page_num': page, 
                            'payload': next_payload, 
                            'parent_crawl_id': parent_crawl_id # åˆ—è¡¨é¡µçš„çˆ¶çº§æ˜¯ root crawl_id
                        },
                        dont_filter=True
                    )

        except Exception as e:
            self.spider_log.error(f"âŒ åˆ—è¡¨é¡µé¢è§£æå¤±è´¥ (Page 1): {e}", exc_info=True)
            
            # ä¸ŠæŠ¥å¼‚å¸¸çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': 1,
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }

    def parse_list_page(self, response):
        """å¤„ç†åç»­è¯å“åˆ—è¡¨é¡µ"""
        page_crawl_id = str(uuid.uuid4())
        parent_crawl_id = response.meta.get('parent_crawl_id')
        page_num = response.meta.get('page_num', 1)
        current_payload = response.meta.get('payload', {})
        
        try:
            if response.headers.getlist('Set-Cookie'):
                self._update_cookies(response)
            
            res_json = json.loads(response.text)
            data_block = res_json.get("data", {})
            records = data_block.get("list", [])
            total_pages = data_block.get("pages", 0)
            page_size = data_block.get("pageSize", 1000)
            
            self.spider_log.info(f"ğŸ“„ åˆ—è¡¨é¡µé¢ [{page_num}/{total_pages}] - å‘ç° {len(records)} æ¡è¯å“è®°å½•")
            
            # ä¸ŠæŠ¥é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': page_num,
                'total_pages': total_pages,
                'page_size': page_size,
                'items_found': len(records),
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }
            
            item_count = 0
            for drug_item in records:
                for request in self._request_hospital_detail(drug_item, page_num, page_crawl_id):
                    yield request
                    item_count += 1
            
            # æ›´æ–°é¡µé¢é‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': page_num,
                'total_pages': total_pages,
                'items_found': len(records),
                'items_stored': item_count,
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }
                
        except Exception as e:
            self.spider_log.error(f"âŒ åˆ†é¡µè§£æå¤±è´¥ Page {page_num}: {e}", exc_info=True)
            
            yield {
                '_status_': True,
                'crawl_id': page_crawl_id,
                'stage': 'list_page',
                'page_no': page_num,
                'params': current_payload,
                'api_url': self.list_api_url,
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }

    def _request_hospital_detail(self, drug_item, page_num, parent_crawl_id):
        """æ„é€ è·å–åŒ»é™¢è¯¦æƒ…çš„è¯·æ±‚ï¼Œå°† drug_item ä¼ é€’ä¸‹å»"""
        prodentp_code = drug_item.get("prodentpCode")
        prod_code = drug_item.get("prodCode")
        
        if not prodentp_code:
            self.spider_log.warning(f"âš ï¸ ç¼ºå°‘ prodentpCodeï¼Œè·³è¿‡è¯¦æƒ…æŸ¥è¯¢: {drug_item.get('prodName')}")
            return

        # æ„é€ è¯¦æƒ…é¡µ Payload
        payload = {
            "pageNo": 1,
            "pageSize": 1000,
            "prodCode": prod_code,
            "prodEntpCode": prodentp_code,
            "isPublicHospitals": ""
        }
        query_string = urlencode(payload)
        full_url = f"{self.hospital_api_url}?{query_string}"
        
        # è¿™é‡Œçš„ meta éå¸¸é‡è¦ï¼Œç”¨æ¥ä¼ é€’ç¬¬ä¸€æ­¥è·å–çš„ info å’Œé¡µç 
        yield scrapy.Request(
            url=full_url,
            method='GET',
            callback=self.parse_detail,
            meta={
                'drug_info': drug_item, 
                'page_num': page_num,
                'parent_crawl_id': parent_crawl_id,
                'payload': payload
            }, 
            cookies=self.cookies if self.cookies else None,
            dont_filter=True
        )

    def parse_detail(self, response):
        """å¤„ç†åŒ»é™¢è¯¦æƒ…å“åº”ï¼Œåˆå¹¶æ•°æ®å¹¶ç”Ÿæˆ Item"""
        drug_info = response.meta['drug_info']
        page_num = response.meta.get('page_num', 1)
        parent_crawl_id = response.meta['parent_crawl_id']
        current_payload = response.meta['payload']
        detail_crawl_id = str(uuid.uuid4())
        
        try:
            # å°è¯•æ›´æ–°cookies (éƒ¨åˆ†ç«™ç‚¹è¯¦æƒ…é¡µä¹Ÿä¼šset-cookie)
            if response.headers.getlist('Set-Cookie'):
                self._update_cookies(response)
                
            res_json = json.loads(response.text)
            
            hospital_list = res_json.get("list", [])
            # å…¼å®¹å¯èƒ½çš„ null æˆ–ä¸åŒç»“æ„
            if hospital_list is None:
                hospital_list = []
            
            self.spider_log.info(f"ğŸ¥ è¯å“ [{drug_info.get('prodName')}] è¯¦æƒ…é¡µ - å‘ç° {len(hospital_list)} å®¶åŒ»é™¢è®°å½•")
            
            # ä¸ŠæŠ¥è¯¦æƒ…é¡µé‡‡é›†çŠ¶æ€
            yield {
                '_status_': True,
                'crawl_id': detail_crawl_id,
                'stage': 'detail_page',
                'page_no': page_num, # è¯¦æƒ…é¡µæ²¡æœ‰åˆ†é¡µï¼Œæ²¿ç”¨åˆ—è¡¨é¡µç 
                'items_found': len(hospital_list),
                'params': current_payload,
                'api_url': self.hospital_api_url,
                'reference_id': drug_info.get('prodCode'),
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

            # 3. åˆ›å»ºåˆå¹¶åçš„æ•°æ® Item
            item = self._create_item(drug_info, hospital_list, page_num)
            yield item
            
            # æ›´æ–°çŠ¶æ€ï¼Œç¡®è®¤å…¥åº“ 1 æ¡ (èšåˆäº†æ‰€æœ‰åŒ»é™¢ä¿¡æ¯)
            yield {
                '_status_': True,
                'crawl_id': detail_crawl_id,
                'stage': 'detail_page',
                'items_stored': 1,
                'params': current_payload,
                'api_url': self.hospital_api_url,
                'reference_id': drug_info.get('prodCode'),
                'success': True,
                'parent_crawl_id': parent_crawl_id
            }

        except Exception as e:
            self.spider_log.error(f"âŒ è¯¦æƒ…é¡µè§£æå¤±è´¥: {e} | URL: {response.url}", exc_info=True)
            
            yield {
                '_status_': True,
                'crawl_id': detail_crawl_id,
                'stage': 'detail_page',
                'params': current_payload,
                'api_url': self.hospital_api_url,
                'reference_id': drug_info.get('prodCode'),
                'success': False,
                'error_message': str(e),
                'parent_crawl_id': parent_crawl_id
            }

    def _create_item(self, drug_info, hospital_list, page_num=1):
        """
        æ„å»º HebeiDrugItem
        """
        item = HebeiDrugItem()
        
        prodentp_code = drug_info.get("prodentpCode")
        prod_code = drug_info.get("prodCode")
        
        # 1. è®¾ç½®è¯å“åŸºç¡€ä¿¡æ¯
        for field_name in item.fields:
            if field_name in ['md5_id', 'collect_time', 'url', 'url_hash', 'hospital_purchases', 'page_num']:
                continue
            if field_name in drug_info:
                item[field_name] = drug_info[field_name]
        
        # 2. è®¾ç½®åŒ»é™¢é‡‡è´­ä¿¡æ¯
        item['hospital_purchases'] = hospital_list
        
        # 3. è®¾ç½®URLå­—æ®µ
        item['url'] = f"{self.hospital_api_url}?pageNo=1&pageSize=1000&prodCode={prod_code}&prodEntpCode={prodentp_code}"
        
        # 4. è®¾ç½®é¡µç 
        item['page_num'] = page_num
        
        # 5. ç”ŸæˆMD5å”¯ä¸€ID
        item.generate_md5_id()
        
        return item
        
    def _update_cookies(self, response):
        """
        ä»å“åº”ä¸­æå–å¹¶æ›´æ–°cookies
        """
        try:
            for cookie_header in response.headers.getlist('Set-Cookie'):
                cookie_str = cookie_header.decode('utf-8')
                if '=' in cookie_str:
                    cookie_parts = cookie_str.split(';')[0].split('=')
                    if len(cookie_parts) >= 2:
                        cookie_name = cookie_parts[0].strip()
                        cookie_value = '='.join(cookie_parts[1:]).strip()
                        self.cookies[cookie_name] = cookie_value
            # self.spider_log.debug(f"æ›´æ–°åçš„cookies: {self.cookies}")
        except Exception as e:
            self.spider_log.warning(f"Cookies æ›´æ–°å¤±è´¥: {e}")