import os
import sys
import time
import subprocess
import signal
import psutil
import re
from typing import List, Optional
from collections import deque
from fastapi import FastAPI, Request, HTTPException, BackgroundTasks
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse, JSONResponse
from pydantic import BaseModel
from sqlalchemy import create_engine, desc, func
from sqlalchemy.orm import sessionmaker

# ==========================================
# è·¯å¾„é…ç½®
# ==========================================
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
template_path = os.path.join(current_dir, "templates")
log_dir = os.path.join(project_root, "logs") # ç»Ÿä¸€æ—¥å¿—ç›®å½•

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
os.makedirs(log_dir, exist_ok=True)

if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ==========================================
# æ¨¡å—å¯¼å…¥
# ==========================================
try:
    from hybrid_crawler.models import Base, init_db, SessionLocal
    from hybrid_crawler.models.crawl_status import CrawlStatus
    from hybrid_crawler.models.spider_progress import SpiderProgress # æ–°å¢
    from run import SPIDER_MAP
except ImportError as e:
    print(f"âš ï¸ å¯¼å…¥è­¦å‘Š: {e}")
    SPIDER_MAP = {}
    SessionLocal = None
    CrawlStatus = None
    SpiderProgress = None

# è¡¥é‡‡æ¨¡å—å¯¼å…¥
try:
    import sys
    sys.path.append(os.path.dirname(project_root))
    from recrawl_checker import check_all_spiders, recrawl_spider, SPIDER_MAPPING as RECRAWL_SPIDER_MAP
except ImportError as e:
    print(f"âš ï¸ è¡¥é‡‡æ¨¡å—å¯¼å…¥è­¦å‘Š: {e}")
    RECRAWL_SPIDER_MAP = {}
    check_all_spiders = None
    recrawl_spider = None

app = FastAPI(title="Crawler Command Center")
templates = Jinja2Templates(directory=template_path)

# å†…å­˜ä¸­ç»´æŠ¤è¿è¡Œçš„è¿›ç¨‹
RUNNING_PROCESSES = {}

class SpiderTask(BaseModel):
    spiders: List[str]

# ==========================================
# æ™ºèƒ½æ—¥å¿—åˆ†æå™¨
# ==========================================
class LogParser:
    """è§£ææ—¥å¿—ä»¥æå–è¿›åº¦ä¿¡æ¯"""
    
    @staticmethod
    def parse_progress(spider_name, log_content):
        """
        æ ¹æ®ä¸åŒçš„çˆ¬è™«æ—¥å¿—æ¨¡å¼ï¼Œæå–è¿›åº¦
        è¿”å›: (progress_percent, current, total, status_text)
        """
        if not log_content:
            return 0, 0, 0, "Waiting for logs..."

        lines = log_content.split('\n')
        last_lines = lines[-50:] # åªåˆ†ææœ€è¿‘50è¡Œï¼Œæé«˜æ•ˆç‡
        full_text = "\n".join(lines) # å…¨æ–‡ç”¨äºæœç´¢åˆå§‹åŒ–ä¿¡æ¯

        # æ¨¡å¼1: ç¦å»ºè¯åº— (åˆ†é¡µæ¨¡å¼ [1/33])
        # Log: ğŸ“„ è¯å“åˆ—è¡¨é¡µé¢ [1/33]
        if "fujian" in spider_name or "åˆ†é¡µ" in log_content:
            # æœç´¢æ€»é¡µæ•° (ä»åå¾€å‰æ‰¾æœ€æ–°çš„è¿›åº¦)
            for line in reversed(last_lines):
                match = re.search(r'\[(\d+)/(\d+)\]', line)
                if match:
                    current, total = map(int, match.groups())
                    if total > 0:
                        return round((current / total) * 100, 1), current, total, f"Page {current}/{total}"
        
        # æ¨¡å¼2: æµ·å—è¯åº— (å…³é”®è¯æ¨¡å¼)
        # Log: åŠ è½½å…³é”®è¯: 146 ä¸ª ... æ­£åœ¨é‡‡é›†å…³é”®è¯: ç‰‡
        if "hainan" in spider_name:
            # 1. æ‰¾æ€»æ•°
            total = 0
            total_match = re.search(r'åŠ è½½å…³é”®è¯[:ï¼š]\s*(\d+)', full_text)
            if total_match:
                total = int(total_match.group(1))
            
            # 2. æ‰¾å½“å‰è¿›åº¦ (ç»Ÿè®¡"æ­£åœ¨é‡‡é›†å…³é”®è¯"å‡ºç°çš„æ¬¡æ•°)
            # æ³¨æ„ï¼šè¿™ç§æ–¹å¼åœ¨æ—¥å¿—è¢«æˆªæ–­æ—¶å¯èƒ½ä¸å‡†ï¼Œä½†åœ¨ tail æ¨¡å¼ä¸‹æˆ‘ä»¬å°½é‡è¯»å–è¶³å¤Ÿå¤š
            # æ›´å‡†ç¡®çš„æ–¹æ³•æ˜¯æ‰¾æœ€åä¸€æ¬¡å‡ºç°çš„å…³é”®è¯ç´¢å¼•ï¼Œä½†è¿™é‡Œç®€åŒ–å¤„ç†
            if total > 0:
                # ç»Ÿè®¡å·²å®Œæˆçš„å…³é”®è¯æ•°é‡ï¼ˆç®€å•é€šè¿‡æ—¥å¿—è¡Œæ•°ä¼°ç®—ï¼Œæˆ–è€…éœ€åœ¨æ—¥å¿—ä¸­æ‰“å°è¿›åº¦ç´¢å¼•ï¼‰
                # å‡è®¾æ—¥å¿—é‡Œæ¯å¤„ç†ä¸€ä¸ªå…³é”®è¯ä¼šæ‰“å° "æ­£åœ¨é‡‡é›†å…³é”®è¯"
                current = len(re.findall(r'æ­£åœ¨é‡‡é›†å…³é”®è¯', full_text))
                # ä¿®æ­£ï¼šé˜²æ­¢ current > total (é‡è¯•å¯èƒ½å¯¼è‡´æ—¥å¿—é‡å¤)
                current = min(current, total)
                return round((current / total) * 100, 1), current, total, f"Keyword {current}/{total}"

        # æ¨¡å¼3: é€šç”¨æ¨¡å¼ (æ ¹æ® Scraped items ä¼°ç®—ï¼Œæˆ–è€…æ— æ³•è®¡ç®—)
        return 0, 0, 0, "Running..."

# ==========================================
# API æ¥å£
# ==========================================

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/api/spiders")
async def get_spiders():
    """è·å–çˆ¬è™«åˆ—è¡¨åŠç®€è¦çŠ¶æ€"""
    spiders_list = []
    db = SessionLocal() if SessionLocal else None
    
    try:
        for name in SPIDER_MAP.keys():
            status = "stopped"
            pid = None
            
            # 1. æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
            if name in RUNNING_PROCESSES:
                proc = RUNNING_PROCESSES[name]
                if proc.poll() is None:
                    status = "running"
                    pid = proc.pid
                else:
                    del RUNNING_PROCESSES[name]
            
            # 2. ä» DB è·å–æœ€åä¸€æ¬¡è¿è¡Œç»Ÿè®¡
            last_stats = {}
            if db and CrawlStatus:
                latest_log = db.query(CrawlStatus).filter(
                    CrawlStatus.spider_name == name
                ).order_by(desc(CrawlStatus.start_time)).first()
                if latest_log:
                    last_stats = {
                        "items": latest_log.items_stored,
                        "last_run": latest_log.start_time.strftime("%Y-%m-%d %H:%M") if latest_log.start_time else "-"
                    }
            
            # 3. å°è¯•ä»å®æ—¶è¿›åº¦è¡¨è·å–æ›´å‡†ç¡®çš„çŠ¶æ€
            if db and SpiderProgress:
                progress = db.query(SpiderProgress).filter_by(spider_name=name).first()
                if progress:
                     # å¦‚æœè¿›ç¨‹åœ¨è·‘ï¼Œä½† DB æ˜¾ç¤º errorï¼Œå¯èƒ½éœ€è¦æ³¨æ„
                     if status == "running" and progress.status == "error":
                         status = "warning"
                     # å¦‚æœ DB æ˜¾ç¤º running ä½†è¿›ç¨‹æ²¡äº†ï¼Œé‚£æ˜¯æ„å¤–é€€å‡º
                     elif status == "stopped" and progress.status == "running":
                         # è¿™é‡Œå¯ä»¥å°è¯•é‡ç½® DB çŠ¶æ€ï¼Œæˆ–è€…æ˜¾ç¤º "dead"
                         pass
                     
                     if progress.progress_percent > 0:
                         last_stats["progress"] = f"{progress.progress_percent}%"

            spiders_list.append({
                "name": name, 
                "status": status, 
                "pid": pid,
                "stats": last_stats
            })
    finally:
        if db: db.close()
    return {"spiders": spiders_list}

@app.post("/api/start")
async def start_spiders(task: SpiderTask):
    """å¯åŠ¨çˆ¬è™«ï¼Œé‡å®šå‘æ—¥å¿—åˆ°æ–‡ä»¶"""
    started = []
    python_executable = sys.executable
    script_path = os.path.join(project_root, 'run.py')
    
    for name in task.spiders:
        if name in RUNNING_PROCESSES and RUNNING_PROCESSES[name].poll() is None:
            continue
            
        # å®šä¹‰ä¸“å±æ—¥å¿—æ–‡ä»¶
        spider_log_file = os.path.join(log_dir, f"{name}.log")
        
        try:
            # æ‰“å¼€æ—¥å¿—æ–‡ä»¶å¥æŸ„ (wæ¨¡å¼è¦†ç›–æ—§æ—¥å¿—ï¼Œæˆ–è€…aæ¨¡å¼è¿½åŠ ï¼Œå»ºè®®wæ–¹ä¾¿çœ‹å•æ¬¡è¿›åº¦)
            log_out = open(spider_log_file, "w", encoding="utf-8")
            
            proc = subprocess.Popen(
                [python_executable, script_path, name],
                cwd=project_root,
                stdout=log_out, # æ ‡å‡†è¾“å‡ºé‡å®šå‘åˆ°æ–‡ä»¶
                stderr=subprocess.STDOUT, # é”™è¯¯è¾“å‡ºåˆå¹¶åˆ°æ ‡å‡†è¾“å‡º
                encoding='utf-8' # ä»…åœ¨ text=True æ—¶æœ‰æ•ˆï¼Œè¿™é‡Œç›´æ¥ç”± file handle å¤„ç†
            )
            RUNNING_PROCESSES[name] = proc
            started.append(name)
        except Exception as e:
            print(f"âŒ Start Error {name}: {e}")
            
    return {"status": "ok", "started": started}

@app.post("/api/stop")
async def stop_spider(task: SpiderTask):
    stopped = []
    for name in task.spiders:
        if name in RUNNING_PROCESSES:
            proc = RUNNING_PROCESSES[name]
            # å°è¯•æ€æ‰è¿›ç¨‹æ ‘
            try:
                parent = psutil.Process(proc.pid)
                for child in parent.children(recursive=True):
                    child.kill()
                parent.kill()
            except: pass
            del RUNNING_PROCESSES[name]
            stopped.append(name)
    return {"status": "ok", "stopped": stopped}

@app.get("/api/spider/{name}/monitor")
async def get_spider_monitor(name: str):
    """è·å–å•ä¸ªçˆ¬è™«çš„å®æ—¶ç›‘æ§æ•°æ®ï¼ˆæ—¥å¿—+è¿›åº¦ï¼‰"""
    log_file = os.path.join(log_dir, f"{name}.log")
    
    # 1. è¯»å–æ—¥å¿— (è¯»å–æœ€å 10KB) - ä¿æŒä¸å˜ï¼Œç”¨äº Debug
    log_content = ""
    if os.path.exists(log_file):
        try:
            with open(log_file, 'rb') as f:
                f.seek(0, 2)
                file_size = f.tell()
                read_size = 1024 * 10
                if file_size > read_size:
                    f.seek(file_size - read_size)
                else:
                    f.seek(0)
                log_content = f.read().decode('utf-8', errors='ignore')
        except Exception as e:
            log_content = f"Error reading log: {e}"
    else:
        log_content = "Waiting for logs... (Log file not created yet)"

    # 2. ä» DB è·å–ç²¾å‡†è¿›åº¦
    progress = 0
    current = 0
    total = 0
    status_text = "Initializing..."
    last_status_detail = {}
    task_tree = []
    
    if SessionLocal and SpiderProgress:
        db = SessionLocal()
        try:
            # è¿›åº¦ä¿¡æ¯
            sp = db.query(SpiderProgress).filter_by(spider_name=name).first()
            if sp:
                progress = sp.progress_percent
                current = sp.completed_tasks
                total = sp.total_tasks
                status_text = sp.current_item or sp.status
            
            # --- æ„å»ºä»»åŠ¡æ ‘ (Task Tree) ---
            if CrawlStatus:
                # è·å–æœ€è¿‘çš„ 100 æ¡è®°å½•
                recent_logs = db.query(CrawlStatus).filter_by(spider_name=name)\
                    .order_by(desc(CrawlStatus.id)).limit(200).all()
                
                # 1. å°†è®°å½•æŒ‰ parent_crawl_id åˆ†ç»„
                nodes = {}
                children_map = {}
                
                for log in recent_logs:
                    # ç®€åŒ–èŠ‚ç‚¹ä¿¡æ¯
                    node = {
                        "id": log.crawl_id,
                        "parent_id": log.parent_crawl_id,
                        "stage": log.stage,
                        "status": "success" if log.success else "error",
                        "progress": f"{log.page_no}/{log.total_pages}" if log.total_pages > 0 else f"{log.page_no}",
                        "info": f"Found: {log.items_found} | Stored: {log.items_stored}",
                        "timestamp": log.start_time.strftime("%H:%M:%S") if log.start_time else "",
                        "error": log.error_message,
                        "children": []
                    }
                    nodes[log.crawl_id] = node
                    
                    pid = log.parent_crawl_id
                    if pid:
                        if pid not in children_map: children_map[pid] = []
                        children_map[pid].append(node)
                
                # 2. ç»„è£…æ ‘ (è‡ªåº•å‘ä¸Šæˆ–è€…è‡ªé¡¶å‘ä¸‹)
                # ç”±äºæˆ‘ä»¬åªæŸ¥äº†æœ€è¿‘ N æ¡ï¼Œå¯èƒ½æ‰¾ä¸åˆ° Rootï¼Œæ‰€ä»¥æˆ‘ä»¬å°†æ‰€æœ‰ parent_id åœ¨æœ¬æ¬¡æŸ¥è¯¢ä¸­æ‰¾ä¸åˆ°çš„èŠ‚ç‚¹è§†ä¸º "Visible Root"
                
                visible_roots = []
                for log in recent_logs:
                    node = nodes[log.crawl_id]
                    # å¦‚æœæœ‰å­èŠ‚ç‚¹ï¼ŒæŒ‚è½½ä¸Šå»
                    if log.crawl_id in children_map:
                        node['children'] = children_map[log.crawl_id]
                    
                    # åˆ¤æ–­æ˜¯å¦ä¸ºå½“å‰è§†å›¾çš„æ ¹
                    # å¦‚æœæ²¡æœ‰ parent_idï¼Œæˆ–è€… parent_id ä¸åœ¨æœ¬æ¬¡æŸ¥å‡ºæ¥çš„èŠ‚ç‚¹ä¸­
                    if not log.parent_crawl_id or log.parent_crawl_id not in nodes:
                        visible_roots.append(node)
                
                # å»é‡ (å› ä¸º recent_logs æ˜¯æŒ‰ ID å€’åºçš„ï¼Œæˆ‘ä»¬å¯èƒ½é‡å¤æ·»åŠ äº†)
                # è¿™é‡Œçš„é€»è¾‘æœ‰ç‚¹ä¹±ï¼Œç®€åŒ–ä¸€ä¸‹ï¼š
                # æˆ‘ä»¬åªéå† visible_rootsï¼Œä½†æ˜¯ visible_roots å¯èƒ½åŒ…å«åŒä¸€ä¸ªæ ‘çš„å¤šä¸ªåˆ†æ”¯ï¼ˆå¦‚æœæ ¹èŠ‚ç‚¹å¤ªè€æ²¡æŸ¥å‡ºæ¥ï¼‰
                # ä¸ºäº†å±•ç¤ºç¾è§‚ï¼Œæˆ‘ä»¬åªå–æœ€é¡¶å±‚çš„
                
                unique_roots = {}
                for r in visible_roots:
                    if r['id'] not in unique_roots:
                        unique_roots[r['id']] = r
                
                task_tree = list(unique_roots.values())
            
            # åˆ†å±‚çŠ¶æ€ä¿¡æ¯ (ä¿ç•™æ—§é€»è¾‘ä»¥å…¼å®¹)
            if CrawlStatus:
                # 1. åˆ—è¡¨å±‚ (List Page) - ä¸»ä»»åŠ¡
                latest_list = db.query(CrawlStatus).filter_by(spider_name=name, stage='list_page')\
                    .order_by(desc(CrawlStatus.id)).first()
                
                if latest_list:
                    last_status_detail['list_layer'] = {
                        "api_url": latest_list.api_url,
                        "params": latest_list.params,
                        "items_found": latest_list.items_found,
                        "items_stored": latest_list.items_stored,
                        "page_no": latest_list.page_no,
                        "total_pages": latest_list.total_pages,
                        "timestamp": latest_list.start_time.strftime("%H:%M:%S") if latest_list.start_time else ""
                    }
                
                # 2. è¯¦æƒ…å±‚ (Detail Page) - å­ä»»åŠ¡
                latest_detail = db.query(CrawlStatus).filter_by(spider_name=name, stage='detail_page')\
                    .order_by(desc(CrawlStatus.id)).first()
                    
                if latest_detail:
                    last_status_detail['detail_layer'] = {
                        "api_url": latest_detail.api_url,
                        "params": latest_detail.params,
                        "items_found": latest_detail.items_found,
                        "items_stored": latest_detail.items_stored,
                        "page_no": latest_detail.page_no,
                        "total_pages": latest_detail.total_pages,
                        "error_message": latest_detail.error_message,
                        "timestamp": latest_detail.start_time.strftime("%H:%M:%S") if latest_detail.start_time else ""
                    }
                
                # å…¼å®¹æ—§é€»è¾‘çš„ fallback (å¦‚æœåªæŸ¥åˆ°ä¸€æ¡ï¼Œæˆ–è€…ä½œä¸ºæ€»ä½“æ¦‚è§ˆ)
                if not latest_list and not latest_detail:
                     latest_any = db.query(CrawlStatus).filter_by(spider_name=name).order_by(desc(CrawlStatus.id)).first()
                     if latest_any:
                         last_status_detail['general'] = {
                             "stage": latest_any.stage,
                             "api_url": latest_any.api_url,
                             "items_stored": latest_any.items_stored
                         }

        except Exception as e:
            status_text = f"DB Error: {str(e)}"
        finally:
            db.close()
    else:
        # Fallback to log parser if DB not available
        progress, current, total, status_text = LogParser.parse_progress(name, log_content)
    
    # 3. åˆ¤æ–­è¿è¡ŒçŠ¶æ€
    is_running = False
    if name in RUNNING_PROCESSES and RUNNING_PROCESSES[name].poll() is None:
        is_running = True
        
    return {
        "name": name,
        "is_running": is_running,
        "progress": progress,
        "current_step": current,
        "total_steps": total,
        "status_text": status_text,
        "detail": last_status_detail,
        "task_tree": task_tree, # æ–°å¢ä»»åŠ¡æ ‘
        "logs": log_content
    }

@app.get("/api/dashboard/stats")
async def get_stats():
    # ç®€åŒ–çš„ç»Ÿè®¡æ¥å£
    if not SessionLocal: return {}
    db = SessionLocal()
    try:
        total = db.query(func.sum(CrawlStatus.items_stored)).scalar() or 0
        runs = db.query(func.count(CrawlStatus.crawl_id)).scalar() or 0
        return {"total_items": total, "total_runs": runs, "chart_data": []}
    finally:
        db.close()

@app.post("/api/db/reset")
async def reset_db():
    if not SessionLocal: raise HTTPException(500, "No DB")
    try:
        from hybrid_crawler.models import engine, Base
        Base.metadata.drop_all(bind=engine)
        Base.metadata.create_all(bind=engine)
        return {"status": "ok"}
    except Exception as e:
        raise HTTPException(500, str(e))

@app.get("/api/recrawl/check")
async def check_recrawl_status():
    """æ£€æŸ¥æ‰€æœ‰çˆ¬è™«çš„ç¼ºå¤±æƒ…å†µ"""
    if not check_all_spiders:
        return {"status": "error", "message": "è¡¥é‡‡æ¨¡å—æœªæ­£ç¡®åŠ è½½"}
    
    try:
        report = check_all_spiders()
        return {"status": "ok", "report": report}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/api/recrawl/check/{spider_name}")
async def check_single_recrawl(spider_name: str, background_tasks: BackgroundTasks):
    """æ£€æŸ¥ç‰¹å®šçˆ¬è™«çš„ç¼ºå¤±æƒ…å†µ"""
    if not check_all_spiders or spider_name not in RECRAWL_SPIDER_MAP:
        return {"status": "error", "message": "æ— æ•ˆçš„çˆ¬è™«åç§°"}
    
    # ç›´æ¥è¿”å›ï¼Œå®é™…æ£€æŸ¥åœ¨åå°æ‰§è¡Œ
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæœºåˆ¶æ¥æŸ¥è¯¢æ£€æŸ¥ç»“æœï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬æš‚æ—¶ç›´æ¥æ‰§è¡Œ
    # å¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œåº”è¯¥ä½¿ç”¨ä»»åŠ¡é˜Ÿåˆ—å’Œç»“æœæŸ¥è¯¢æœºåˆ¶
    try:
        from recrawl_checker import BaseRecrawler
        crawler = RECRAWL_SPIDER_MAP[spider_name]()
        missing_ids = crawler.find_missing()
        crawler.close()
        
        return {
            "status": "ok", 
            "spider_name": spider_name,
            "missing_count": len(missing_ids),
            "missing_ids": list(missing_ids)[:10]  # åªè¿”å›å‰10ä¸ªç¤ºä¾‹
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/api/recrawl/start/{spider_name}")
async def start_recrawl(spider_name: str, background_tasks: BackgroundTasks):
    """å¼€å§‹ç‰¹å®šçˆ¬è™«çš„è¡¥é‡‡"""
    if not recrawl_spider or spider_name not in RECRAWL_SPIDER_MAP:
        return {"status": "error", "message": "æ— æ•ˆçš„çˆ¬è™«åç§°"}
    
    try:
        # å¼‚æ­¥æ‰§è¡Œè¡¥é‡‡ï¼Œé¿å…é˜»å¡API
        background_tasks.add_task(recrawl_spider, spider_name)
        return {"status": "ok", "message": f"å·²å¼€å§‹{spider_name}çˆ¬è™«çš„è¡¥é‡‡ä»»åŠ¡"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/api/recrawl/start-all")
async def start_all_recrawl(background_tasks: BackgroundTasks):
    """ä¸€é”®æ£€æŸ¥å¹¶è¡¥å……é‡‡é›†æ‰€æœ‰çˆ¬è™«"""
    if not check_all_spiders or not recrawl_spider:
        return {"status": "error", "message": "è¡¥é‡‡æ¨¡å—æœªæ­£ç¡®åŠ è½½"}
    
    async def run_all_recrawl():
        """å¼‚æ­¥æ‰§è¡Œæ‰€æœ‰çˆ¬è™«çš„æ£€æŸ¥å’Œè¡¥é‡‡"""
        from recrawl_checker import BaseRecrawler
        
        for spider_name, crawler_class in RECRAWL_SPIDER_MAP.items():
            try:
                # å…ˆæ£€æŸ¥ç¼ºå¤±æƒ…å†µ
                crawler = crawler_class()
                missing_ids = crawler.find_missing()
                crawler.close()
                
                # å¦‚æœæœ‰ç¼ºå¤±æ•°æ®ï¼Œæ‰§è¡Œè¡¥é‡‡
                if missing_ids:
                    recrawl_spider(spider_name)
            except Exception as e:
                print(f"å¤„ç†{spider_name}çˆ¬è™«æ—¶å‡ºé”™: {e}")
    
    try:
        # å¼‚æ­¥æ‰§è¡Œæ‰€æœ‰çˆ¬è™«çš„è¡¥é‡‡
        background_tasks.add_task(run_all_recrawl)
        return {"status": "ok", "message": "å·²å¼€å§‹æ‰€æœ‰çˆ¬è™«çš„ä¸€é”®æ£€æŸ¥å’Œè¡¥é‡‡ä»»åŠ¡"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ Dashboard running on port 5210")
    uvicorn.run(app, host="0.0.0.0", port=5210)