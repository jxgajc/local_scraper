import os
import sys
import time
import subprocess
import signal
import psutil
import re
from typing import List, Optional
from collections import deque
from fastapi import FastAPI, Request, HTTPException, BackgroundTasks
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse, JSONResponse
from pydantic import BaseModel
from sqlalchemy import create_engine, desc, func
from sqlalchemy.orm import sessionmaker

# ==========================================
# è·¯å¾„é…ç½®
# ==========================================
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
template_path = os.path.join(current_dir, "templates")
log_dir = os.path.join(project_root, "logs") # ç»Ÿä¸€æ—¥å¿—ç›®å½•

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
os.makedirs(log_dir, exist_ok=True)

if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ==========================================
# æ¨¡å—å¯¼å…¥
# ==========================================
try:
    from hybrid_crawler.models import Base, init_db, SessionLocal
    from hybrid_crawler.models.crawl_status import CrawlStatus
    from hybrid_crawler.models.spider_progress import SpiderProgress # æ–°å¢
    from hybrid_crawler.recrawl.manager import RecrawlManager  # æ–°å¢
    from run import SPIDER_MAP
except ImportError as e:
    print(f"âš ï¸ å¯¼å…¥è­¦å‘Š: {e}")
    SPIDER_MAP = {}
    SessionLocal = None
    CrawlStatus = None
    SpiderProgress = None
    RecrawlManager = None

app = FastAPI(title="Crawler Command Center")
templates = Jinja2Templates(directory=template_path)

# å†…å­˜ä¸­ç»´æŠ¤è¿è¡Œçš„è¿›ç¨‹
RUNNING_PROCESSES = {}
# å†…å­˜ä¸­ç»´æŠ¤è¿è¡Œçš„è¡¥é‡‡ä»»åŠ¡ (spider_name -> crawler_instance)
RECRAWL_TASKS = {}
# å¼‚æ­¥ä»»åŠ¡çŠ¶æ€å­˜å‚¨ (task_id -> status_dict)
ASYNC_TASK_STATUS = {}

class SpiderTask(BaseModel):
    spiders: List[str]

class RecrawlRequest(BaseModel):
    missing_ids: Optional[List[str]] = None

# ==========================================
# æ™ºèƒ½æ—¥å¿—åˆ†æå™¨
# ==========================================
class LogParser:
    """è§£ææ—¥å¿—ä»¥æå–è¿›åº¦ä¿¡æ¯"""
    
    @staticmethod
    def parse_progress(spider_name, log_content):
        """
        æ ¹æ®ä¸åŒçš„çˆ¬è™«æ—¥å¿—æ¨¡å¼ï¼Œæå–è¿›åº¦
        è¿”å›: (progress_percent, current, total, status_text)
        """
        if not log_content:
            return 0, 0, 0, "Waiting for logs..."

        lines = log_content.split('\n')
        last_lines = lines[-50:] # åªåˆ†ææœ€è¿‘50è¡Œï¼Œæé«˜æ•ˆç‡
        full_text = "\n".join(lines) # å…¨æ–‡ç”¨äºæœç´¢åˆå§‹åŒ–ä¿¡æ¯

        # æ¨¡å¼1: ç¦å»ºè¯åº— (åˆ†é¡µæ¨¡å¼ [1/33])
        # Log: ğŸ“„ è¯å“åˆ—è¡¨é¡µé¢ [1/33]
        if "fujian" in spider_name or "åˆ†é¡µ" in log_content:
            # æœç´¢æ€»é¡µæ•° (ä»åå¾€å‰æ‰¾æœ€æ–°çš„è¿›åº¦)
            for line in reversed(last_lines):
                match = re.search(r'\[(\d+)/(\d+)\]', line)
                if match:
                    current, total = map(int, match.groups())
                    if total > 0:
                        return round((current / total) * 100, 1), current, total, f"Page {current}/{total}"
        
        # æ¨¡å¼2: æµ·å—è¯åº— (å…³é”®è¯æ¨¡å¼)
        # Log: åŠ è½½å…³é”®è¯: 146 ä¸ª ... æ­£åœ¨é‡‡é›†å…³é”®è¯: ç‰‡
        if "hainan" in spider_name:
            # 1. æ‰¾æ€»æ•°
            total = 0
            total_match = re.search(r'åŠ è½½å…³é”®è¯[:ï¼š]\s*(\d+)', full_text)
            if total_match:
                total = int(total_match.group(1))
            
            # 2. æ‰¾å½“å‰è¿›åº¦ (ç»Ÿè®¡"æ­£åœ¨é‡‡é›†å…³é”®è¯"å‡ºç°çš„æ¬¡æ•°)
            # æ³¨æ„ï¼šè¿™ç§æ–¹å¼åœ¨æ—¥å¿—è¢«æˆªæ–­æ—¶å¯èƒ½ä¸å‡†ï¼Œä½†åœ¨ tail æ¨¡å¼ä¸‹æˆ‘ä»¬å°½é‡è¯»å–è¶³å¤Ÿå¤š
            # æ›´å‡†ç¡®çš„æ–¹æ³•æ˜¯æ‰¾æœ€åä¸€æ¬¡å‡ºç°çš„å…³é”®è¯ç´¢å¼•ï¼Œä½†è¿™é‡Œç®€åŒ–å¤„ç†
            if total > 0:
                # ç»Ÿè®¡å·²å®Œæˆçš„å…³é”®è¯æ•°é‡ï¼ˆç®€å•é€šè¿‡æ—¥å¿—è¡Œæ•°ä¼°ç®—ï¼Œæˆ–è€…éœ€åœ¨æ—¥å¿—ä¸­æ‰“å°è¿›åº¦ç´¢å¼•ï¼‰
                # å‡è®¾æ—¥å¿—é‡Œæ¯å¤„ç†ä¸€ä¸ªå…³é”®è¯ä¼šæ‰“å° "æ­£åœ¨é‡‡é›†å…³é”®è¯"
                current = len(re.findall(r'æ­£åœ¨é‡‡é›†å…³é”®è¯', full_text))
                # ä¿®æ­£ï¼šé˜²æ­¢ current > total (é‡è¯•å¯èƒ½å¯¼è‡´æ—¥å¿—é‡å¤)
                current = min(current, total)
                return round((current / total) * 100, 1), current, total, f"Keyword {current}/{total}"

        # æ¨¡å¼3: é€šç”¨æ¨¡å¼ (æ ¹æ® Scraped items ä¼°ç®—ï¼Œæˆ–è€…æ— æ³•è®¡ç®—)
        return 0, 0, 0, "Running..."

# ==========================================
# API æ¥å£
# ==========================================

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/api/spiders")
async def get_spiders():
    """è·å–çˆ¬è™«åˆ—è¡¨åŠç®€è¦çŠ¶æ€"""
    spiders_list = []
    db = SessionLocal() if SessionLocal else None
    
    try:
        for name in SPIDER_MAP.keys():
            status = "stopped"
            pid = None
            
            # 1. æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
            if name in RUNNING_PROCESSES:
                proc = RUNNING_PROCESSES[name]
                if proc.poll() is None:
                    status = "running"
                    pid = proc.pid
                else:
                    del RUNNING_PROCESSES[name]
            
            # 2. ä» DB è·å–æœ€åä¸€æ¬¡è¿è¡Œç»Ÿè®¡
            last_stats = {}
            if db and CrawlStatus:
                latest_log = db.query(CrawlStatus).filter(
                    CrawlStatus.spider_name == name
                ).order_by(desc(CrawlStatus.start_time)).first()
                if latest_log:
                    last_stats = {
                        "items": latest_log.items_stored,
                        "last_run": latest_log.start_time.strftime("%Y-%m-%d %H:%M") if latest_log.start_time else "-"
                    }
            
            # 3. å°è¯•ä»å®æ—¶è¿›åº¦è¡¨è·å–æ›´å‡†ç¡®çš„çŠ¶æ€
            if db and SpiderProgress:
                progress = db.query(SpiderProgress).filter_by(spider_name=name).first()
                if progress:
                     # å¦‚æœè¿›ç¨‹åœ¨è·‘ï¼Œä½† DB æ˜¾ç¤º errorï¼Œå¯èƒ½éœ€è¦æ³¨æ„
                     if status == "running" and progress.status == "error":
                         status = "warning"
                     # å¦‚æœ DB æ˜¾ç¤º running ä½†è¿›ç¨‹æ²¡äº†ï¼Œé‚£æ˜¯æ„å¤–é€€å‡º
                     elif status == "stopped" and progress.status == "running":
                         # è¿™é‡Œå¯ä»¥å°è¯•é‡ç½® DB çŠ¶æ€ï¼Œæˆ–è€…æ˜¾ç¤º "dead"
                         pass
                     
                     if progress.progress_percent > 0:
                         last_stats["progress"] = f"{progress.progress_percent}%"

            spiders_list.append({
                "name": name, 
                "status": status, 
                "pid": pid,
                "stats": last_stats
            })
    finally:
        if db: db.close()
    return {"spiders": spiders_list}

@app.post("/api/start")
async def start_spiders(task: SpiderTask):
    """å¯åŠ¨çˆ¬è™«ï¼Œé‡å®šå‘æ—¥å¿—åˆ°æ–‡ä»¶"""
    started = []
    python_executable = sys.executable
    script_path = os.path.join(project_root, 'run.py')
    
    for name in task.spiders:
        if name in RUNNING_PROCESSES and RUNNING_PROCESSES[name].poll() is None:
            continue
            
        # å®šä¹‰ä¸“å±æ—¥å¿—æ–‡ä»¶
        spider_log_file = os.path.join(log_dir, f"{name}.log")
        
        try:
            # æ‰“å¼€æ—¥å¿—æ–‡ä»¶å¥æŸ„ (wæ¨¡å¼è¦†ç›–æ—§æ—¥å¿—ï¼Œæˆ–è€…aæ¨¡å¼è¿½åŠ ï¼Œå»ºè®®wæ–¹ä¾¿çœ‹å•æ¬¡è¿›åº¦)
            log_out = open(spider_log_file, "w", encoding="utf-8")
            
            proc = subprocess.Popen(
                [python_executable, script_path, name],
                cwd=project_root,
                stdout=log_out, # æ ‡å‡†è¾“å‡ºé‡å®šå‘åˆ°æ–‡ä»¶
                stderr=subprocess.STDOUT, # é”™è¯¯è¾“å‡ºåˆå¹¶åˆ°æ ‡å‡†è¾“å‡º
                encoding='utf-8' # ä»…åœ¨ text=True æ—¶æœ‰æ•ˆï¼Œè¿™é‡Œç›´æ¥ç”± file handle å¤„ç†
            )
            RUNNING_PROCESSES[name] = proc
            started.append(name)
        except Exception as e:
            print(f"âŒ Start Error {name}: {e}")
            
    return {"status": "ok", "started": started}

@app.post("/api/stop")
async def stop_spider(task: SpiderTask):
    stopped = []
    for name in task.spiders:
        if name in RUNNING_PROCESSES:
            proc = RUNNING_PROCESSES[name]
            # å°è¯•æ€æ‰è¿›ç¨‹æ ‘
            try:
                parent = psutil.Process(proc.pid)
                for child in parent.children(recursive=True):
                    child.kill()
                parent.kill()
            except: pass
            del RUNNING_PROCESSES[name]
            stopped.append(name)
    return {"status": "ok", "stopped": stopped}

@app.get("/api/spider/{name}/monitor")
async def get_spider_monitor(name: str):
    """è·å–å•ä¸ªçˆ¬è™«çš„å®æ—¶ç›‘æ§æ•°æ®ï¼ˆæ—¥å¿—+è¿›åº¦ï¼‰"""
    log_file = os.path.join(log_dir, f"{name}.log")
    
    # 1. è¯»å–æ—¥å¿— (è¯»å–æœ€å 10KB) - ä¿æŒä¸å˜ï¼Œç”¨äº Debug
    log_content = ""
    if os.path.exists(log_file):
        try:
            with open(log_file, 'rb') as f:
                f.seek(0, 2)
                file_size = f.tell()
                read_size = 1024 * 10
                if file_size > read_size:
                    f.seek(file_size - read_size)
                else:
                    f.seek(0)
                log_content = f.read().decode('utf-8', errors='ignore')
        except Exception as e:
            log_content = f"Error reading log: {e}"
    else:
        log_content = "Waiting for logs... (Log file not created yet)"

    # 2. ä» DB è·å–ç²¾å‡†è¿›åº¦
    progress = 0
    current = 0
    total = 0
    status_text = "Initializing..."
    last_status_detail = {}
    task_tree = []
    
    if SessionLocal and SpiderProgress:
        db = SessionLocal()
        try:
            # è¿›åº¦ä¿¡æ¯
            sp = db.query(SpiderProgress).filter_by(spider_name=name).first()
            if sp:
                progress = sp.progress_percent
                current = sp.completed_tasks
                total = sp.total_tasks
                status_text = sp.current_item or sp.status
            
            # --- æ„å»ºä»»åŠ¡æ ‘ (Task Tree) ---
            if CrawlStatus:
                # è·å–æœ€è¿‘çš„ 100 æ¡è®°å½•
                recent_logs = db.query(CrawlStatus).filter_by(spider_name=name)\
                    .order_by(desc(CrawlStatus.id)).limit(200).all()
                
                # 1. å°†è®°å½•æŒ‰ parent_crawl_id åˆ†ç»„
                nodes = {}
                children_map = {}
                
                for log in recent_logs:
                    # ç®€åŒ–èŠ‚ç‚¹ä¿¡æ¯
                    node = {
                        "id": log.crawl_id,
                        "parent_id": log.parent_crawl_id,
                        "stage": log.stage,
                        "status": "success" if log.success else "error",
                        "progress": f"{log.page_no}/{log.total_pages}" if log.total_pages > 0 else f"{log.page_no}",
                        "info": f"Found: {log.items_found} | Stored: {log.items_stored}",
                        "timestamp": log.start_time.strftime("%H:%M:%S") if log.start_time else "",
                        "error": log.error_message,
                        "children": []
                    }
                    nodes[log.crawl_id] = node
                    
                    pid = log.parent_crawl_id
                    if pid:
                        if pid not in children_map: children_map[pid] = []
                        children_map[pid].append(node)
                
                # 2. ç»„è£…æ ‘ (è‡ªåº•å‘ä¸Šæˆ–è€…è‡ªé¡¶å‘ä¸‹)
                # ç”±äºæˆ‘ä»¬åªæŸ¥äº†æœ€è¿‘ N æ¡ï¼Œå¯èƒ½æ‰¾ä¸åˆ° Rootï¼Œæ‰€ä»¥æˆ‘ä»¬å°†æ‰€æœ‰ parent_id åœ¨æœ¬æ¬¡æŸ¥è¯¢ä¸­æ‰¾ä¸åˆ°çš„èŠ‚ç‚¹è§†ä¸º "Visible Root"
                
                visible_roots = []
                for log in recent_logs:
                    node = nodes[log.crawl_id]
                    # å¦‚æœæœ‰å­èŠ‚ç‚¹ï¼ŒæŒ‚è½½ä¸Šå»
                    if log.crawl_id in children_map:
                        node['children'] = children_map[log.crawl_id]
                    
                    # åˆ¤æ–­æ˜¯å¦ä¸ºå½“å‰è§†å›¾çš„æ ¹
                    # å¦‚æœæ²¡æœ‰ parent_idï¼Œæˆ–è€… parent_id ä¸åœ¨æœ¬æ¬¡æŸ¥å‡ºæ¥çš„èŠ‚ç‚¹ä¸­
                    if not log.parent_crawl_id or log.parent_crawl_id not in nodes:
                        visible_roots.append(node)
                
                # å»é‡ (å› ä¸º recent_logs æ˜¯æŒ‰ ID å€’åºçš„ï¼Œæˆ‘ä»¬å¯èƒ½é‡å¤æ·»åŠ äº†)
                # è¿™é‡Œçš„é€»è¾‘æœ‰ç‚¹ä¹±ï¼Œç®€åŒ–ä¸€ä¸‹ï¼š
                # æˆ‘ä»¬åªéå† visible_rootsï¼Œä½†æ˜¯ visible_roots å¯èƒ½åŒ…å«åŒä¸€ä¸ªæ ‘çš„å¤šä¸ªåˆ†æ”¯ï¼ˆå¦‚æœæ ¹èŠ‚ç‚¹å¤ªè€æ²¡æŸ¥å‡ºæ¥ï¼‰
                # ä¸ºäº†å±•ç¤ºç¾è§‚ï¼Œæˆ‘ä»¬åªå–æœ€é¡¶å±‚çš„
                
                unique_roots = {}
                for r in visible_roots:
                    if r['id'] not in unique_roots:
                        unique_roots[r['id']] = r
                
                task_tree = list(unique_roots.values())
            
            # åˆ†å±‚çŠ¶æ€ä¿¡æ¯ (ä¿ç•™æ—§é€»è¾‘ä»¥å…¼å®¹)
            if CrawlStatus:
                # 1. åˆ—è¡¨å±‚ (List Page) - ä¸»ä»»åŠ¡
                latest_list = db.query(CrawlStatus).filter_by(spider_name=name, stage='list_page')\
                    .order_by(desc(CrawlStatus.id)).first()
                
                if latest_list:
                    last_status_detail['list_layer'] = {
                        "api_url": latest_list.api_url,
                        "params": latest_list.params,
                        "items_found": latest_list.items_found,
                        "items_stored": latest_list.items_stored,
                        "page_no": latest_list.page_no,
                        "total_pages": latest_list.total_pages,
                        "timestamp": latest_list.start_time.strftime("%H:%M:%S") if latest_list.start_time else ""
                    }
                
                # 2. è¯¦æƒ…å±‚ (Detail Page) - å­ä»»åŠ¡
                latest_detail = db.query(CrawlStatus).filter_by(spider_name=name, stage='detail_page')\
                    .order_by(desc(CrawlStatus.id)).first()
                    
                if latest_detail:
                    last_status_detail['detail_layer'] = {
                        "api_url": latest_detail.api_url,
                        "params": latest_detail.params,
                        "items_found": latest_detail.items_found,
                        "items_stored": latest_detail.items_stored,
                        "page_no": latest_detail.page_no,
                        "total_pages": latest_detail.total_pages,
                        "error_message": latest_detail.error_message,
                        "timestamp": latest_detail.start_time.strftime("%H:%M:%S") if latest_detail.start_time else ""
                    }
                
                # å…¼å®¹æ—§é€»è¾‘çš„ fallback (å¦‚æœåªæŸ¥åˆ°ä¸€æ¡ï¼Œæˆ–è€…ä½œä¸ºæ€»ä½“æ¦‚è§ˆ)
                if not latest_list and not latest_detail:
                     latest_any = db.query(CrawlStatus).filter_by(spider_name=name).order_by(desc(CrawlStatus.id)).first()
                     if latest_any:
                         last_status_detail['general'] = {
                             "stage": latest_any.stage,
                             "api_url": latest_any.api_url,
                             "items_stored": latest_any.items_stored
                         }

        except Exception as e:
            status_text = f"DB Error: {str(e)}"
        finally:
            db.close()
    else:
        # Fallback to log parser if DB not available
        progress, current, total, status_text = LogParser.parse_progress(name, log_content)
    
    # 3. åˆ¤æ–­è¿è¡ŒçŠ¶æ€
    is_running = False
    if name in RUNNING_PROCESSES and RUNNING_PROCESSES[name].poll() is None:
        is_running = True
        
    return {
        "name": name,
        "is_running": is_running,
        "progress": progress,
        "current_step": current,
        "total_steps": total,
        "status_text": status_text,
        "detail": last_status_detail,
        "task_tree": task_tree, # æ–°å¢ä»»åŠ¡æ ‘
        "logs": log_content
    }

@app.get("/api/dashboard/stats")
async def get_stats():
    # ç®€åŒ–çš„ç»Ÿè®¡æ¥å£
    if not SessionLocal: return {}
    db = SessionLocal()
    try:
        total = db.query(func.sum(CrawlStatus.items_stored)).scalar() or 0
        runs = db.query(func.count(CrawlStatus.crawl_id)).scalar() or 0
        return {"total_items": total, "total_runs": runs, "chart_data": []}
    finally:
        db.close()

@app.post("/api/db/reset")
async def reset_db():
    if not SessionLocal: raise HTTPException(500, "No DB")
    try:
        from hybrid_crawler.models import engine, Base
        Base.metadata.drop_all(bind=engine)
        Base.metadata.create_all(bind=engine)
        return {"status": "ok"}
    except Exception as e:
        raise HTTPException(500, str(e))

@app.get("/api/recrawl/check")
async def check_recrawl_status():
    """æ£€æŸ¥æ‰€æœ‰çˆ¬è™«çš„ç¼ºå¤±æƒ…å†µ"""
    if not RecrawlManager:
        return {"status": "error", "message": "RecrawlManager not available"}

    report = {}
    for name in RecrawlManager.list_spiders():
        try:
            missing_data = RecrawlManager.find_missing(name)
            report[name] = {
                "spider_name": name,
                "missing_count": len(missing_data),
                "missing_ids": list(missing_data.keys())[:10]
            }
        except Exception as e:
            report[name] = {"error": str(e)}

    return {"status": "ok", "report": report}

@app.post("/api/recrawl/stop")
async def stop_recrawl(task: SpiderTask):
    """åœæ­¢è¡¥é‡‡ä»»åŠ¡"""
    stopped = []
    
    # æ”¯æŒ "all" å…³é”®å­—
    targets = task.spiders
    if "all" in targets:
        targets = list(RECRAWL_TASKS.keys())
    
    for name in targets:
        if name in RECRAWL_TASKS:
            try:
                # Scrapy spider object doesn't have stop(), but we can define it or just ignore
                # Since these are running in memory/background tasks, we can't easily kill them
                # unless we implemented a flag check in the loop (which we did in mixin via stop_check, but here we run in background thread)
                # For now, we just remove from tasks map
                del RECRAWL_TASKS[name]
                stopped.append(name)
            except Exception as e:
                print(f"Failed to stop {name}: {e}")
                
    return {"status": "ok", "stopped": stopped}

# å…¨å±€å˜é‡ï¼šå­˜å‚¨æ£€æŸ¥ç»“æœï¼ˆç¼ºå¤±çš„IDåˆ—è¡¨ï¼‰
RECRAWL_MISSING_IDS = {}  # {spider_name: set(missing_ids)}

@app.get("/api/recrawl/check/{spider_name}")
async def check_single_recrawl(spider_name: str, background_tasks: BackgroundTasks):
    """æ£€æŸ¥ç‰¹å®šçˆ¬è™«çš„ç¼ºå¤±æƒ…å†µ (å¼‚æ­¥æ‰§è¡Œ)"""
    global RECRAWL_MISSING_IDS, ASYNC_TASK_STATUS

    if not RecrawlManager:
        return {"status": "error", "message": "RecrawlManager not available"}

    if spider_name not in RecrawlManager.list_spiders():
        return {"status": "error", "message": "æ— æ•ˆçš„çˆ¬è™«åç§°"}

    if spider_name in RECRAWL_TASKS:
        return {"status": "error", "message": "è¯¥çˆ¬è™«æ­£åœ¨æ‰§è¡Œæ£€æŸ¥æˆ–è¡¥é‡‡ä»»åŠ¡"}

    task_id = f"check_{spider_name}_{int(time.time())}"
    ASYNC_TASK_STATUS[task_id] = {
        "type": "check",
        "spider_name": spider_name,
        "status": "running",
        "message": "æ­£åœ¨æ£€æŸ¥ç¼ºå¤±æ•°æ®...",
        "missing_count": 0
    }

    def run_check():
        try:
            RECRAWL_TASKS[spider_name] = True

            missing_ids = RecrawlManager.find_missing(spider_name)
            missing_count = len(missing_ids) if missing_ids else 0

            # ä¿å­˜æ£€æŸ¥ç»“æœï¼Œä¾›è¡¥é‡‡æ—¶ä½¿ç”¨
            RECRAWL_MISSING_IDS[spider_name] = missing_ids

            # å°† ID åˆ—è¡¨è½¬æ¢ä¸º listï¼Œæ–¹ä¾¿ JSON åºåˆ—åŒ–
            missing_ids_list = list(missing_ids) if missing_ids else []
            preview_ids = missing_ids_list[:50]  # é¢„è§ˆå‰50ä¸ª

            ASYNC_TASK_STATUS[task_id] = {
                "type": "check",
                "spider_name": spider_name,
                "status": "completed",
                "message": f"å‘ç° {missing_count} æ¡ç¼ºå¤±æ•°æ®",
                "missing_count": missing_count,
                "missing_ids": preview_ids,
                "has_more": len(missing_ids_list) > 50
            }
        except Exception as e:
            import traceback
            traceback.print_exc()
            ASYNC_TASK_STATUS[task_id] = {
                "type": "check",
                "spider_name": spider_name,
                "status": "error",
                "message": str(e),
                "missing_count": 0
            }
        finally:
            if spider_name in RECRAWL_TASKS:
                del RECRAWL_TASKS[spider_name]

    background_tasks.add_task(run_check)
    return {"status": "ok", "task_id": task_id, "message": "æ£€æŸ¥ä»»åŠ¡å·²å¯åŠ¨"}


@app.get("/api/recrawl/task/{task_id}")
async def get_task_status(task_id: str):
    """è·å–å¼‚æ­¥ä»»åŠ¡çŠ¶æ€åŠæ—¥å¿—"""
    if task_id not in ASYNC_TASK_STATUS:
        return {"status": "error", "message": "ä»»åŠ¡ä¸å­˜åœ¨"}

    task_info = ASYNC_TASK_STATUS[task_id]
    spider_name = task_info.get("spider_name", "")

    # è¯»å–æ—¥å¿—æ–‡ä»¶
    logs = ""
    if spider_name:
        log_file = os.path.join(project_root, "logs", f"{spider_name}.log")
        if os.path.exists(log_file):
            try:
                with open(log_file, 'rb') as f:
                    f.seek(0, 2)
                    file_size = f.tell()
                    read_size = 1024 * 20  # è¯»å–æœ€å20KB
                    if file_size > read_size:
                        f.seek(file_size - read_size)
                    else:
                        f.seek(0)
                    logs = f.read().decode('utf-8', errors='ignore')
            except Exception as e:
                logs = f"è¯»å–æ—¥å¿—å¤±è´¥: {e}"

    return {"status": "ok", "task": task_info, "logs": logs}


@app.get("/api/recrawl/tasks")
async def get_all_tasks():
    """è·å–æ‰€æœ‰è¿è¡Œä¸­çš„è¡¥é‡‡ä»»åŠ¡çŠ¶æ€"""
    running_tasks = {k: v for k, v in ASYNC_TASK_STATUS.items() if v.get("status") == "running"}
    active_crawlers = list(RECRAWL_TASKS.keys())
    return {
        "status": "ok",
        "running_tasks": running_tasks,
        "active_crawlers": active_crawlers
    }


@app.post("/api/recrawl/start/{spider_name}")
async def start_recrawl(spider_name: str, background_tasks: BackgroundTasks, body: RecrawlRequest = None):
    """å¼€å§‹ç‰¹å®šçˆ¬è™«çš„è¡¥é‡‡ - ä½¿ç”¨æ£€æŸ¥æ—¶ä¿å­˜çš„ç¼ºå¤±ID (å¼‚æ­¥æ‰§è¡Œ)"""
    global RECRAWL_MISSING_IDS, ASYNC_TASK_STATUS

    if not RecrawlManager:
        return {"status": "error", "message": "RecrawlManager not available"}

    if spider_name not in RecrawlManager.list_spiders():
        return {"status": "error", "message": "æ— æ•ˆçš„çˆ¬è™«åç§°"}

    if spider_name in RECRAWL_TASKS:
        return {"status": "error", "message": "è¯¥çˆ¬è™«æ­£åœ¨æ‰§è¡Œä»»åŠ¡"}

    # ç¡®å®šè¦è¡¥é‡‡çš„ID
    missing_ids = None

    # 1. ä¼˜å…ˆä½¿ç”¨è¯·æ±‚ä½“ä¸­æä¾›çš„ ID
    if body and body.missing_ids:
        missing_ids = body.missing_ids
        print(f"[{spider_name}] ä½¿ç”¨ç”¨æˆ·æä¾›çš„ {len(missing_ids)} ä¸ªIDè¿›è¡Œè¡¥é‡‡")
    # 2. å…¶æ¬¡ä½¿ç”¨åç«¯ç¼“å­˜çš„ ID
    elif spider_name in RECRAWL_MISSING_IDS and RECRAWL_MISSING_IDS[spider_name]:
        missing_ids = RECRAWL_MISSING_IDS[spider_name]
        print(f"[{spider_name}] ä½¿ç”¨ç¼“å­˜çš„ {len(missing_ids)} ä¸ªIDè¿›è¡Œè¡¥é‡‡")
    else:
        return {"status": "error", "message": "è¯·å…ˆæ‰§è¡Œæ£€æŸ¥ç¼ºå¤±æ•°æ®ï¼Œæˆ–ç›´æ¥æä¾› ID åˆ—è¡¨"}

    task_id = f"recrawl_{spider_name}_{int(time.time())}"
    total_count = len(missing_ids) if missing_ids else 0

    ASYNC_TASK_STATUS[task_id] = {
        "type": "recrawl",
        "spider_name": spider_name,
        "status": "running",
        "message": f"æ­£åœ¨è¡¥é‡‡ {total_count} æ¡æ•°æ®...",
        "total": total_count
    }

    def run_recrawl():
        try:
            RECRAWL_TASKS[spider_name] = True

            collected = RecrawlManager.recrawl(spider_name, missing_ids)

            # è¡¥é‡‡å®Œæˆåæ¸…é™¤ä¿å­˜çš„ç¼ºå¤±ID
            RECRAWL_MISSING_IDS.pop(spider_name, None)

            ASYNC_TASK_STATUS[task_id] = {
                "type": "recrawl",
                "spider_name": spider_name,
                "status": "completed",
                "message": f"è¡¥é‡‡å®Œæˆï¼ŒæˆåŠŸ {collected} æ¡",
                "total": total_count,
                "collected": collected
            }

        except Exception as e:
            ASYNC_TASK_STATUS[task_id] = {
                "type": "recrawl",
                "spider_name": spider_name,
                "status": "error",
                "message": str(e),
                "total": total_count
            }
        finally:
            if spider_name in RECRAWL_TASKS:
                del RECRAWL_TASKS[spider_name]

    background_tasks.add_task(run_recrawl)
    return {"status": "ok", "task_id": task_id, "message": f"å·²å¼€å§‹{spider_name}çˆ¬è™«çš„è¡¥é‡‡ä»»åŠ¡ï¼Œå…±{total_count}æ¡"}

@app.post("/api/recrawl/start-all")
async def start_all_recrawl(background_tasks: BackgroundTasks):
    """ä¸€é”®æ£€æŸ¥å¹¶è¡¥å……é‡‡é›†æ‰€æœ‰çˆ¬è™« (å¼‚æ­¥æ‰§è¡Œ)"""
    global ASYNC_TASK_STATUS

    if not RecrawlManager:
        return {"status": "error", "message": "RecrawlManager not available"}

    task_id = f"recrawl_all_{int(time.time())}"
    ASYNC_TASK_STATUS[task_id] = {
        "type": "recrawl_all",
        "status": "running",
        "message": "æ­£åœ¨æ‰§è¡Œä¸€é”®æ£€æŸ¥å¹¶è¡¥é‡‡...",
        "completed": [],
        "failed": []
    }

    def run_all_recrawl():
        """æ‰§è¡Œæ‰€æœ‰çˆ¬è™«çš„æ£€æŸ¥å’Œè¡¥é‡‡"""
        completed = []
        failed = []

        for spider_name in RecrawlManager.list_spiders():
            try:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä»»åŠ¡åœ¨è¿è¡Œ
                if spider_name in RECRAWL_TASKS:
                    continue

                # æ›´æ–°çŠ¶æ€
                ASYNC_TASK_STATUS[task_id]["message"] = f"æ­£åœ¨å¤„ç† {spider_name}..."

                RECRAWL_TASKS[spider_name] = True

                # æ‰§è¡Œå®Œæ•´çš„è¡¥é‡‡æµç¨‹
                count = RecrawlManager.full_recrawl(spider_name)
                completed.append({"spider": spider_name, "count": count})

            except Exception as e:
                failed.append({"spider": spider_name, "error": str(e)})
            finally:
                if spider_name in RECRAWL_TASKS:
                    del RECRAWL_TASKS[spider_name]

        ASYNC_TASK_STATUS[task_id] = {
            "type": "recrawl_all",
            "status": "completed",
            "message": f"å®Œæˆ {len(completed)} ä¸ªï¼Œå¤±è´¥ {len(failed)} ä¸ª",
            "completed": completed,
            "failed": failed
        }

    background_tasks.add_task(run_all_recrawl)
    return {"status": "ok", "task_id": task_id, "message": "å·²å¼€å§‹æ‰€æœ‰çˆ¬è™«çš„ä¸€é”®æ£€æŸ¥å’Œè¡¥é‡‡ä»»åŠ¡"}

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ Dashboard running on port 5210")
    uvicorn.run(app, host="0.0.0.0", port=5210)
