import time
import random
import logging
from scrapy.downloadermiddlewares.retry import RetryMiddleware
from twisted.internet.error import (
    ConnectionRefusedError, DNSLookupError, TimeoutError, TCPTimedOutError
)
from .exceptions import CrawlerNetworkError, ElementNotFoundError, BrowserCrashError

logger = logging.getLogger(__name__)

class StrategyRoutingMiddleware:
    """
    ã€ç­–ç•¥è·¯ç”±ä¸­é—´ä»¶ã€‘
    ä½œç”¨ï¼šåœ¨è¯·æ±‚å‘å‡ºå‰ï¼Œæ ¹æ® Request çš„ meta æ ‡è®°ï¼Œå†³å®šæ˜¯å¦å¯ç”¨ Playwrightã€‚
    """
    def process_request(self, request, spider):
        request_type = request.meta.get('request_type', 'http')
        
        # å¦‚æœæ ‡è®°ä¸º playwrightï¼Œåˆ™æ¿€æ´» scrapy-playwright æ’ä»¶çš„å‚æ•°
        if request_type == 'playwright':
            request.meta['playwright'] = True
            request.meta['dont_merge_cookies'] = True # æµè§ˆå™¨è‡ªå·±ç®¡ç† Cookieï¼Œä¸ä½¿ç”¨ Scrapy çš„ CookieJar
        return None

class SmartRetryMiddleware(RetryMiddleware):
    """
    ã€æ™ºèƒ½é‡è¯•ä¸­é—´ä»¶ã€‘
    ä½œç”¨ï¼šæ›¿ä»£é»˜è®¤çš„ RetryMiddlewareï¼Œå®ç°åˆ†çº§é‡è¯•ç­–ç•¥ã€‚
    """
    NETWORK_ERRORS = (ConnectionRefusedError, DNSLookupError, TimeoutError, TCPTimedOutError, CrawlerNetworkError)
    LOGIC_ERRORS = (ElementNotFoundError, BrowserCrashError)

    def process_exception(self, request, exception, spider):
        retry_times = request.meta.get('retry_times', 0) + 1
        max_retries = self.max_retry_times

        if retry_times > max_retries:
            logger.error(f"âŒ æ”¾å¼ƒè¯·æ±‚ {request.url}: è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°")
            return None

        # ç­–ç•¥ 1: ç½‘ç»œé”™è¯¯ -> æŒ‡æ•°é€€é¿ (Wait time = 2^(n-1))
        if isinstance(exception, self.NETWORK_ERRORS):
            delay = 2 ** (retry_times - 1)
            logger.warning(f"âš ï¸ ç½‘ç»œæ³¢åŠ¨ ({exception}), {delay}s åé‡è¯•: {request.url}")
            # æ”¹è‰¯ï¼šä¸è¦ä½¿ç”¨ time.sleep(delay)ï¼Œè¿™ä¼šé˜»å¡ reactorã€‚
            # æ­£ç¡®åšæ³•æ˜¯è®¾ç½® download_delayï¼Œè®© Scrapy çš„è°ƒåº¦å™¨å»ç­‰å¾…ã€‚
            new_request = self._retry(request, exception, spider)
            if new_request:
                new_request.meta['download_delay'] = delay
            return new_request
            
        # ç­–ç•¥ 2: é€»è¾‘/æ¸²æŸ“é”™è¯¯ -> å‡€å®¤é‡è¯• (Clean Slate)
        elif isinstance(exception, self.LOGIC_ERRORS):
            logger.warning(f"ğŸ”„ é€»è¾‘é”™è¯¯ ({exception}), è§¦å‘å‡€å®¤é‡è¯•: {request.url}")
            # å…³é”®ï¼šæ ‡è®° clean_slateï¼Œå‘Šè¯‰ Spider åœ¨ä¸‹æ¬¡è¯·æ±‚æ—¶é”€æ¯ Context
            request.meta['clean_slate'] = True 
            request.dont_filter = True
            return self._retry(request, exception, spider)

        return super().process_exception(request, exception, spider)

class RandomUserAgentMiddleware:
    """
    ã€éšæœº User-Agent ä¸­é—´ä»¶ã€‘
    æ¯æ¬¡è¯·æ±‚è‡ªåŠ¨éšæœºåˆ‡æ¢ User-Agentï¼Œé™ä½ç‰¹å¾æŒ‡çº¹ã€‚
    """
    def __init__(self, settings):
        self.ua_list = settings.get('USER_AGENT_LIST', [])
        # Fallback list if settings is empty
        if not self.ua_list:
            self.ua_list = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0',
            ]

    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.settings)

    def process_request(self, request, spider):
        # åªæœ‰åœ¨ headers ä¸­æ²¡æœ‰è®¾ç½® User-Agent æ—¶æ‰æ·»åŠ ï¼Œé¿å…è¦†ç›– Spider ç‰¹å®šçš„è®¾ç½®
        if not request.headers.get('User-Agent'):
            ua = random.choice(self.ua_list)
            if ua:
                request.headers.setdefault('User-Agent', ua)
                # logger.debug(f"User-Agent set to: {ua}")
