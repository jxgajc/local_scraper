import os

BOT_NAME = 'hybrid_crawler'
SPIDER_MODULES = ['hybrid_crawler.spiders']
NEWSPIDER_MODULE = 'hybrid_crawler.spiders'

# =============================================================================
# æ•°æ®åº“é…ç½®
# =============================================================================
# ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å–ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
DATABASE_URL = os.getenv('DATABASE_URL', 'mysql+pymysql://xf:xf666@192.168.0.141:3306/spiderweb')
# å°†å…¶æ³¨å…¥åˆ°ç¯å¢ƒå˜é‡ä¸­ï¼Œä»¥ä¾¿ models æ¨¡å—ï¼ˆé Scrapy ä¸Šä¸‹æ–‡ï¼‰ä¹Ÿèƒ½è·å–
os.environ['DATABASE_URL'] = DATABASE_URL

# =============================================================================
# æ ¸å¿ƒå¹¶å‘é…ç½®
# =============================================================================
CONCURRENT_REQUESTS = 32
DOWNLOAD_DELAY = 0

# ã€å…³é”®ä¼˜åŒ–ã€‘å¢åŠ çº¿ç¨‹æ± å¤§å°ï¼Œé˜²æ­¢æ•°æ®åº“ I/O è€—å°½çº¿ç¨‹å¯¼è‡´æ­»é”
# é»˜è®¤åªæœ‰ 10ï¼Œå¯¹äºå¹¶å‘ 32 çš„çˆ¬è™«æ¥è¯´å¤ªå°
REACTOR_THREADPOOL_MAXSIZE = 50

# ã€å…³é”®ä¼˜åŒ–ã€‘è®¾ç½®ä¸‹è½½è¶…æ—¶ï¼Œé˜²æ­¢åä»£ç†æˆ–æ…¢å“åº”å¡ä½ Slot
DOWNLOAD_TIMEOUT = 15

# =============================================================================
# è‡ªåŠ¨é™é€Ÿé…ç½® (AutoThrottle)
# =============================================================================
# å¯ç”¨è‡ªåŠ¨é™é€Ÿï¼Œæ ¹æ®è´Ÿè½½åŠ¨æ€è°ƒæ•´å»¶è¿Ÿ
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 0.5
AUTOTHROTTLE_MAX_DELAY = 60
AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0 # ä¿æŒæ¯ä¸ªè¿œç¨‹æœåŠ¡å™¨å¹³å‡ 1 ä¸ªå¹¶å‘è¯·æ±‚ (é…åˆ CONCURRENT_REQUESTS å…¨å±€é™åˆ¶)
# è°ƒè¯•æ—¶å¯å¼€å¯
# AUTOTHROTTLE_DEBUG = True

# =============================================================================
# é‡è¯•é…ç½®
# =============================================================================
RETRY_ENABLED = True # ç¡®ä¿åŸºç¡€é…ç½®å¼€å¯
RETRY_TIMES = 3      # é‡è¯• 3 æ¬¡

# =============================================================================
# ä¸­é—´ä»¶ç®¡é“é…ç½®
# =============================================================================
DOWNLOADER_MIDDLEWARES = {
    'hybrid_crawler.middlewares.StrategyRoutingMiddleware': 100, # è·¯ç”±ç­–ç•¥
    'hybrid_crawler.middlewares.SmartRetryMiddleware': 550,      # æ™ºèƒ½é‡è¯•
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,  # ç¦ç”¨é»˜è®¤é‡è¯•
}

ITEM_PIPELINES = {
    'hybrid_crawler.pipelines.DataCleaningPipeline': 300,        # æ¸…æ´—
    'hybrid_crawler.pipelines.CrawlStatusPipeline': 350,         # é‡‡é›†çŠ¶æ€è®°å½•
    'hybrid_crawler.pipelines.AsyncBatchWritePipeline': 400,     # å…¥åº“
}

# =============================================================================
# å¼‚æ­¥å†™å…¥ç¼“å†²é…ç½®
# =============================================================================
BUFFER_THRESHOLD = 500  # ç§¯æ”’ 50 æ¡å†™å…¥ä¸€æ¬¡
BUFFER_TIMEOUT_SEC = 1.5 # æˆ–æœ€é•¿ç­‰å¾… 1.5 ç§’å†™å…¥ä¸€æ¬¡

# =============================================================================
# Playwright ä¸“ç”¨é…ç½®
# =============================================================================
# try:
#     from scrapy_playwright.handler import ScrapyPlaywrightDownloadHandler
#     DOWNLOAD_HANDLERS = {
#         "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
#         "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
#     }
#     # å¿…é¡»ä½¿ç”¨ Asyncio ååº”å †
#     TWISTED_REACTOR = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"
#     # ğŸ’¡ Debug: å°† headless æ”¹ä¸º False å¯çœ‹åˆ°æµè§ˆå™¨
#     PLAYWRIGHT_LAUNCH_OPTIONS = {
#         'headless': True,
#         'args': ['--no-sandbox', '--disable-dev-shm-usage', '--disable-gpu'],
#         'timeout': 30000, # å¯åŠ¨è¶…æ—¶æ—¶é—´
#     }
# except ImportError:
#     # å¦‚æœæ— æ³•å¯¼å…¥ scrapy_playwrightï¼Œä½¿ç”¨é»˜è®¤çš„ä¸‹è½½å¤„ç†å™¨
#     DOWNLOAD_HANDLERS = {}
#     # ä¸éœ€è¦è®¾ç½® Asyncio ååº”å †
#     pass

LOG_LEVEL = 'INFO'
# æ—¥å¿—é…ç½®
LOG_ENABLED = True
LOG_ENCODING = 'utf-8'
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'
LOG_DATEFORMAT = '%Y-%m-%d %H:%M:%S'

# æ—¥å¿—ä¿å­˜è·¯å¾„
script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
log_dir = os.path.join(script_dir, 'log')
os.makedirs(log_dir, exist_ok=True)

# ç¦ç”¨ Scrapy é»˜è®¤çš„æ–‡ä»¶æ—¥å¿—ï¼Œä½¿ç”¨æˆ‘ä»¬çš„è‡ªå®šä¹‰æ—¥å¿—ç®¡ç†å™¨
LOG_FILE = None

# æ—¥å¿—å¤„ç†å™¨é…ç½®
LOG_STDOUT = True

# ä¸ºä¸åŒæ¨¡å—è®¾ç½®æ—¥å¿—çº§åˆ«
LOG_LEVELS = {
    'scrapy': 'WARNING',
    'twisted': 'WARNING',
    'hybrid_crawler': 'INFO',
}

