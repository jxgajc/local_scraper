import logging
import time
import hashlib
from twisted.internet import threads
from itemadapter import ItemAdapter
from sqlalchemy.sql import func # æ–°å¢
from .models import SessionLocal, init_db
from .models.crawl_data import CrawlData
from .models.crawl_status import CrawlStatus
from .models.spider_progress import SpiderProgress # æ–°å¢
from .models.fujian_drug import FujianDrug
from .models.hainan_drug import HainanDrug
from .models.hebei_drug import HebeiDrug
from .models.liaoning_drug import LiaoningDrug
from .models.ningxia_drug import NingxiaDrug
from .models.guangdong_drug import GuangdongDrug
from .models.tianjin_drug import TianjinDrug
from .models.shandong_drug import ShandongDrug

from .models.nhsa_drug import NhsaDrug
from .exceptions import DataValidationError

logger = logging.getLogger(__name__)

class DataCleaningPipeline:
    """æ•°æ®æ¸…æ´—ä¸æ ¡éªŒå±‚"""
    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        
        # 1. å¿…å¡«é¡¹æ ¡éªŒ
        # if not adapter.get('url'):
        #     raise DataValidationError("Drop item: Missing URL")
            
        # # 2. ç”ŸæˆæŒ‡çº¹
        # if not adapter.get('url_hash'):
        #     url = adapter.get('url')
        #     adapter['url_hash'] = hashlib.md5(url.encode('utf-8')).hexdigest()
            
        # 3. åŸºç¡€æ¸…æ´— (å»é™¤é¦–å°¾ç©ºæ ¼)
        for k, v in adapter.items():
            if isinstance(v, str):
                adapter[k] = v.strip()
                
        return item

import logging
import time
from twisted.internet import threads, defer
from sqlalchemy.orm import sessionmaker
# from my_project.models import CrawlData, engine  # å¯¼å…¥ä½ çš„æ¨¡å‹

logger = logging.getLogger(__name__)

class AsyncBatchWritePipeline:
    """
    ã€æœ€ç»ˆæ”¹è‰¯ç‰ˆ - å¼‚æ­¥æ‰¹é‡å†™å…¥å±‚ã€‘
    ç‰¹æ€§ï¼š
    1. æ— é”è®¾è®¡ï¼šåˆ©ç”¨ Twisted çº¿ç¨‹æ± ç®¡ç†å¹¶å‘ï¼Œé¿å… Buffer çˆ†ä»“ã€‚
    2. è‡ªåŠ¨æ¸…ç†ï¼šåŠ¨æ€è¿½è¸ªæ´»è·ƒä»»åŠ¡ï¼Œæ— å†…å­˜æ³„æ¼ã€‚
    3. ä¼˜é›…é€€å‡ºï¼šclose_spider ä½¿ç”¨ DeferredList åŸç”Ÿç­‰å¾…ï¼Œå½»åº•å‘Šåˆ« time.sleepã€‚
    """
    
    def __init__(self, buffer_size=50, timeout=2):
        self.buffer = []
        self.buffer_size = buffer_size
        self.timeout = timeout
        self.last_flush_time = time.time()
        
        # æ•°æ®åº“ Session å·¥å‚
        self.session_maker = SessionLocal 
        
        # ã€å…³é”®æ”¹è‰¯ã€‘ä½¿ç”¨ set ä»…å­˜å‚¨å½“å‰æ´»è·ƒçš„å¼‚æ­¥ä»»åŠ¡
        # ä»»åŠ¡å®Œæˆåä¼šè‡ªåŠ¨ä»ä¸­ç§»é™¤
        self.active_tasks = set()

    @classmethod
    def from_crawler(cls, crawler):
        # å»ºè®®ï¼šinit_db() æœ€å¥½æ”¾åœ¨ Spider çš„ start_requests æˆ– main ä¸­ï¼Œè€Œä¸æ˜¯è¿™é‡Œ
        # init_db() 
        settings = crawler.settings
        return cls(
            buffer_size=settings.getint('BUFFER_THRESHOLD', 100),
            timeout=settings.getfloat('BUFFER_TIMEOUT_SEC', 2.0)
        )

    def process_item(self, item, spider):
        # 1. å¦‚æœ item ä¸º Noneï¼Œé€šå¸¸æ— éœ€å¤„ç†ï¼Œç›´æ¥è¿”å›
        if item is None:
            return item

        # 2. æ·»åŠ åˆ° Buffer
        self.buffer.append(item)

        # 3. æ£€æŸ¥æ˜¯å¦æ»¡è¶³å†™å…¥æ¡ä»¶ (æ•°é‡é˜ˆå€¼ æˆ– æ—¶é—´é˜ˆå€¼)
        # æ³¨æ„ï¼šè¿™é‡Œå»æ‰äº† is_flushing çš„åˆ¤æ–­ã€‚
        # åŸå› ï¼šå¦‚æœå†™å…¥æ…¢è€Œçˆ¬è™«å¿«ï¼Œé˜»å¡ flush ä¼šå¯¼è‡´ buffer æ— é™è†¨èƒ€æ’‘çˆ†å†…å­˜ã€‚
        # Twisted çš„çº¿ç¨‹æ± ä¼šè‡ªåŠ¨æ’é˜Ÿå¤„ç† flush ä»»åŠ¡ï¼Œæ¯”æˆ‘ä»¬åœ¨å†…å­˜å›¤ç§¯æ•°æ®æ›´å®‰å…¨ã€‚
        if self._should_flush():
            self._trigger_flush()

        return item

    def _should_flush(self):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ·æ–°"""
        # åªæœ‰å½“ buffer æœ‰æ•°æ®æ—¶æ‰æ£€æŸ¥æ—¶é—´
        has_data = len(self.buffer) > 0
        time_expired = (time.time() - self.last_flush_time) >= self.timeout
        size_reached = len(self.buffer) >= self.buffer_size
        
        return size_reached or (has_data and time_expired)

    def _trigger_flush(self):
        """è§¦å‘å¼‚æ­¥å†™å…¥ä»»åŠ¡"""
        # 1. ç«‹å³åˆ‡ç‰‡å–å‡ºæ•°æ®ï¼Œæ¸…ç©º Buffer (åŸå­æ“ä½œ)
        items_to_write = self.buffer
        self.buffer = [] # æŒ‡å‘æ–°åˆ—è¡¨
        self.last_flush_time = time.time()

        if not items_to_write:
            return

        # 2. å‘èµ·å¼‚æ­¥ä»»åŠ¡
        logger.debug(f"ğŸš€ è§¦å‘å¼‚æ­¥å†™å…¥: {len(items_to_write)} æ¡")
        df = threads.deferToThread(self._flush_buffer, items_to_write)
        
        # 3. ã€å…³é”®ã€‘è¿½è¸ªä»»åŠ¡
        self.active_tasks.add(df)
        
        # 4. ã€å…³é”®ã€‘æ·»åŠ å›è°ƒï¼šä»»åŠ¡ç»“æŸ(æ— è®ºæˆåŠŸå¤±è´¥)åï¼Œä»é›†åˆä¸­ç§»é™¤è‡ªå·±
        # ä½¿ç”¨ addBoth ç¡®ä¿å³ä½¿æŠ¥é”™ä¹Ÿèƒ½æ¸…ç†
        df.addBoth(self._cleanup_task, df)
        
        # 5. æ·»åŠ é”™è¯¯æ—¥å¿—å›è°ƒ
        df.addErrback(self._log_error)

    def _cleanup_task(self, result, df):
        """ä»»åŠ¡å®Œæˆåçš„æ¸…ç†å›è°ƒ"""
        self.active_tasks.discard(df)
        return result

    def _log_error(self, failure):
        """é”™è¯¯æ—¥å¿—å›è°ƒ"""
        logger.error(f"ğŸ”¥ å¼‚æ­¥å†™å…¥ä¸¥é‡å¼‚å¸¸: {failure.getErrorMessage()}")
        return failure

    @defer.inlineCallbacks
    def close_spider(self, spider):
        """
        ã€æœ€ç»ˆæ”¹è‰¯ç‰ˆå…³é—­é€»è¾‘ã€‘
        """
        logger.info(f"â³ çˆ¬è™«å…³é—­ä¸­... å‰©ä½™ Buffer: {len(self.buffer)} | è¿›è¡Œä¸­ä»»åŠ¡: {len(self.active_tasks)}")
        
        # 1. å¦‚æœ Buffer é‡Œè¿˜æœ‰æ²¡å†™å®Œçš„ï¼Œå‘èµ·æœ€åä¸€æ¬¡å¼‚æ­¥å†™å…¥
        if self.buffer:
            self._trigger_flush()
        
        # 2. ã€æ ¸å¿ƒã€‘ç­‰å¾…æ‰€æœ‰æ´»è·ƒä»»åŠ¡å®Œæˆ
        # DeferredList ä¼šç­‰å¾…åˆ—è¡¨é‡Œæ‰€æœ‰çš„ Deferred å˜ä¸º called çŠ¶æ€
        if self.active_tasks:
            yield defer.DeferredList(list(self.active_tasks))
            
        logger.info("âœ… Pipeline å…³é—­å®Œæˆï¼šæ‰€æœ‰æ•°æ®å·²å®‰å…¨è½åº“ã€‚")

    # --- ä»¥ä¸‹ä¸šåŠ¡é€»è¾‘ä¿æŒä¸å˜ ---

    def _get_model_class(self, item):
        return CrawlData

    def _create_orm_object(self, item, model_class):
        if not item: return None
        model_fields = [c.key for c in model_class.__table__.columns]
        item_data = {k: v for k, v in item.items() if k in model_fields}
        return model_class(**item_data)

    def _flush_buffer(self, items):
        """æ‰§è¡Œæ•°æ®åº“å†™å…¥ï¼ˆè¿è¡Œåœ¨çº¿ç¨‹æ± ä¸­ï¼‰"""
        session = self.session_maker()
        try:
            orm_objects = []
            for item in items:
                # å†æ¬¡è¿‡æ»¤ï¼Œç¡®ä¿å®‰å…¨
                if item:
                    model = self._get_model_class(item)
                    obj = self._create_orm_object(item, model)
                    orm_objects.append(obj)
            
            if not orm_objects: return

            session.add_all(orm_objects)
            session.commit()
            logger.info(f"ğŸ’¾ æ‰¹é‡å†™å…¥æˆåŠŸ: {len(orm_objects)} æ¡")
            
        except Exception as e:
            session.rollback()
            logger.error(f"âš ï¸ æ‰¹é‡å†™å…¥å¤±è´¥: {e}ï¼Œæ­£åœ¨é™çº§ä¸ºé€æ¡å†™å…¥...")
            self._fallback_single_write(session, orm_objects)
        finally:
            session.close()

    def _fallback_single_write(self, session, objects):
        count = 0
        for obj in objects:
            try:
                session.merge(obj)
                session.commit()
                count += 1
            except Exception as e:
                session.rollback()
                logger.error(f"âŒ å•æ¡å†™å…¥ä¸¢å¼ƒ: {e}")
        logger.info(f"ğŸ†— é™çº§å¤„ç†å®Œæˆ: æŒ½å› {count}/{len(objects)} æ¡")

    # å¦‚æœéœ€è¦è‡ªå®šä¹‰å­—æ®µæ˜ å°„ï¼Œå¯ä»¥é‡å†™ _create_orm_object æ–¹æ³•
    # def _create_orm_object(self, item, model_class):
    #     # è‡ªå®šä¹‰æ˜ å°„é€»è¾‘
    #     return model_class(
    #         store_name=item.get('name'),
    #         address=item.get('addr'),
    #         contact=item.get('phone'),
    #         # å…¶ä»–å­—æ®µæ˜ å°„
    #     )

class HainanDrugPipeline(AsyncBatchWritePipeline):
    """æµ·å—è¯åº—æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return HainanDrug

class FujianDrugPipeline(AsyncBatchWritePipeline):
    """ç¦å»ºè¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return FujianDrug

    # å­—æ®µæ˜ å°„å·²ç»åœ¨çˆ¬è™«çš„_create_itemæ–¹æ³•ä¸­å®Œæˆï¼Œè¿™é‡Œå¯ä»¥ä½¿ç”¨é»˜è®¤çš„æ˜ å°„
    # å¦‚æœéœ€è¦é¢å¤–çš„å­—æ®µè½¬æ¢ï¼Œå¯ä»¥é‡å†™ _create_orm_object æ–¹æ³•
class HebeiDrugPipeline(AsyncBatchWritePipeline):
    """æ²³åŒ—è¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return HebeiDrug

class LiaoningDrugPipeline(AsyncBatchWritePipeline):
    """è¾½å®è¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return LiaoningDrug

class NingxiaDrugPipeline(AsyncBatchWritePipeline):
    """ç¦å»ºè¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return NingxiaDrug

class GuangdongDrugPipeline(AsyncBatchWritePipeline):
    """å¹¿ä¸œè¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return GuangdongDrug

class TianjinDrugPipeline(AsyncBatchWritePipeline):
    """å¹¿ä¸œè¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return TianjinDrug

class NhsaDrugPipeline(AsyncBatchWritePipeline):
    """å›½å®¶åŒ»ä¿è¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return NhsaDrug

class ShandongDrugPipeline(AsyncBatchWritePipeline):
    """å›½å®¶åŒ»ä¿è¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return ShandongDrug
    # å­—æ®µæ˜ å°„å·²ç»åœ¨çˆ¬è™«çš„_create_itemæ–¹æ³•ä¸­å®Œæˆï¼Œè¿™é‡Œå¯ä»¥ä½¿ç”¨é»˜è®¤çš„æ˜ å°„
    # å¦‚æœéœ€è¦é¢å¤–çš„å­—æ®µè½¬æ¢ï¼Œå¯ä»¥é‡å†™ _create_orm_object æ–¹æ³•


class CrawlStatusPipeline:
    """
    çˆ¬è™«çŠ¶æ€è®°å½•ç®¡é“
    ç”¨äºè®°å½•æ¯ä¸ªçˆ¬è™«çš„é‡‡é›†è¿‡ç¨‹å’Œå‚æ•°ï¼Œç”¨äºæ•°æ®å®Œæ•´æ€§éªŒè¯
    æ¥æ”¶ç‰¹æ®Šçš„çŠ¶æ€itemï¼Œæ ¼å¼ä¸º {'_status_': True, ...}
    
    ã€æ”¹è‰¯ç‰ˆã€‘ï¼šä½¿ç”¨ deferToThread å¼‚æ­¥å†™å…¥ï¼Œé¿å…é˜»å¡ Reactor
    """
    
    def process_item(self, item, spider):
        # æ£€æŸ¥æ˜¯å¦ä¸ºçŠ¶æ€è®°å½•item
        if isinstance(item, dict) and item.get('_status_'):
            # è¿”å› Deferredï¼ŒScrapy ä¼šç­‰å¾…å…¶å®Œæˆ
            return threads.deferToThread(self._save_status, item, spider)
        return item
    
    def _save_status(self, status_item, spider):
        """ä¿å­˜é‡‡é›†çŠ¶æ€ (è¿è¡Œåœ¨çº¿ç¨‹æ± ä¸­)"""
        # æ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹çš„ Session
        session = SessionLocal()
        try:
            # 1. ä¿å­˜å†å²å®¡è®¡æ—¥å¿— (Append Only)
            status = CrawlStatus(
                spider_name=status_item.get('spider_name', spider.name),
                crawl_id=status_item.get('crawl_id'),
                stage=status_item.get('stage'),
                page_no=status_item.get('page_no', 1),
                total_pages=status_item.get('total_pages', 0),
                page_size=status_item.get('page_size', 0),
                items_found=status_item.get('items_found', 0),
                items_stored=status_item.get('items_stored', 0),
                params=status_item.get('params'),
                api_url=status_item.get('api_url'),
                success=status_item.get('success', True),
                error_message=status_item.get('error_message'),
                parent_crawl_id=status_item.get('parent_crawl_id'),
                reference_id=status_item.get('reference_id')
            )
            session.add(status)
            
            # 2. æ›´æ–°å®æ—¶è¿›åº¦ (Upsert)
            self._update_progress(session, status_item, spider)
            
            session.commit()
        except Exception as e:
            session.rollback()
            logger.error(f"âŒ ä¿å­˜é‡‡é›†çŠ¶æ€å¤±è´¥: {e}")
        finally:
            session.close()
            
        # å¿…é¡»è¿”å› item ä»¥ä¾›åç»­ Pipeline ä½¿ç”¨
        return status_item
        
    def _update_progress(self, session, item, spider):
        """æ›´æ–°çˆ¬è™«å®æ—¶è¿›åº¦è¡¨"""
        try:
            spider_name = item.get('spider_name', spider.name)
            
            # å°è¯•æŸ¥è¯¢ç°æœ‰è®°å½•
            progress = session.query(SpiderProgress).filter_by(spider_name=spider_name).first()
            if not progress:
                progress = SpiderProgress(spider_name=spider_name)
                session.add(progress)
            
            # æ›´æ–°å­—æ®µ
            progress.run_id = item.get('crawl_id', 'unknown')
            progress.status = 'running' if item.get('success', True) else 'error'
            
            # è®¡ç®—è¿›åº¦
            page = item.get('page_no', 1)
            total = item.get('total_pages', 0)
            
            progress.total_tasks = total
            progress.completed_tasks = page
            if total > 0:
                progress.progress_percent = round((page / total) * 100, 2)
                
            progress.current_stage = item.get('stage', 'unknown')
            progress.items_scraped = session.query(CrawlStatus).filter_by(spider_name=spider_name).with_entities(func.sum(CrawlStatus.items_stored)).scalar() or 0
            
            # æ„é€ åˆ†å±‚æè¿°ä¿¡æ¯
            desc = f"Stage: {progress.current_stage}"
            if total > 0:
                desc += f" | Page {page}/{total}"
            if item.get('error_message'):
                desc += f" | Error: {item.get('error_message')}"
            progress.current_item = desc
            
        except Exception as e:
            logger.error(f"âš ï¸ æ›´æ–°å®æ—¶è¿›åº¦å¤±è´¥: {e}")

    def close_spider(self, spider):
        pass