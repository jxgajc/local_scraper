import logging
import time
from twisted.internet import threads, defer
from itemadapter import ItemAdapter
from sqlalchemy.dialects.mysql import insert
from sqlalchemy.sql import func
from .models import SessionLocal
from .models.crawl_status import CrawlStatus
from .models.spider_progress import SpiderProgress
from .models.crawl_data import CrawlData # Fallback
from .exceptions import DataValidationError

logger = logging.getLogger(__name__)

class DataCleaningPipeline:
    """æ•°æ®æ¸…æ´—ä¸æ ¡éªŒå±‚"""
    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        
        # åŸºç¡€æ¸…æ´— (å»é™¤é¦–å°¾ç©ºæ ¼)
        for k, v in adapter.items():
            if isinstance(v, str):
                adapter[k] = v.strip()
                
        return item

class UniversalBatchWritePipeline:
    """
    ã€é€šç”¨å¼‚æ­¥æ‰¹é‡å†™å…¥ç®¡é“ã€‘
    æ ¹æ®é…ç½®é€‰æ‹©å­˜å‚¨åç«¯ (MySQL/ES)ï¼Œå¹¶æ‰§è¡Œæ‰¹é‡å†™å…¥ã€‚
    """
    
    def __init__(self, settings):
        self.buffer = []
        self.buffer_size = settings.getint('BUFFER_THRESHOLD', 500)
        self.timeout = settings.getfloat('BUFFER_TIMEOUT_SEC', 1.5)
        self.last_flush_time = time.time()
        
        # ä½¿ç”¨ set ä»…å­˜å‚¨å½“å‰æ´»è·ƒçš„å¼‚æ­¥ä»»åŠ¡
        self.active_tasks = set()
        
        # åˆå§‹åŒ–å­˜å‚¨åç«¯
        backend_type = settings.get('STORAGE_BACKEND', 'mysql').lower()
        logger.info(f"Initializing Storage Backend: {backend_type}")
        
        if backend_type == 'elasticsearch':
            from .storage.elasticsearch import ElasticsearchStorage
            self.storage = ElasticsearchStorage(
                hosts=settings.get('ES_HOSTS', ['http://localhost:9200']),
                user=settings.get('ES_USER'),
                password=settings.get('ES_PASSWORD'),
                index_prefix=settings.get('ES_INDEX_PREFIX', 'drug_store')
            )
        else:
            from .storage.mysql import MySQLStorage
            self.storage = MySQLStorage()

    @classmethod
    def from_crawler(cls, crawler):
        return cls(settings=crawler.settings)

    def process_item(self, item, spider):
        # 1. è¿‡æ»¤ None æˆ– çŠ¶æ€ Item
        if item is None or isinstance(item, dict):
            # å¦‚æœæ˜¯å­—å…¸ï¼ˆé€šå¸¸æ˜¯çŠ¶æ€Itemï¼‰ï¼Œäº¤ç”±ä¸‹ä¸€ä¸ªPipelineå¤„ç†
            return item

        # 2. æ·»åŠ åˆ° Buffer
        self.buffer.append(item)

        # 3. æ£€æŸ¥æ˜¯å¦æ»¡è¶³å†™å…¥æ¡ä»¶
        if self._should_flush():
            self._trigger_flush()

        return item

    def _should_flush(self):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ·æ–°"""
        has_data = len(self.buffer) > 0
        time_expired = (time.time() - self.last_flush_time) >= self.timeout
        size_reached = len(self.buffer) >= self.buffer_size
        return size_reached or (has_data and time_expired)

    def _trigger_flush(self):
        """è§¦å‘å¼‚æ­¥å†™å…¥ä»»åŠ¡"""
        items_to_write = self.buffer
        self.buffer = [] # æŒ‡å‘æ–°åˆ—è¡¨
        self.last_flush_time = time.time()

        if not items_to_write:
            return

        logger.debug(f"ğŸš€ è§¦å‘å¼‚æ­¥å†™å…¥: {len(items_to_write)} æ¡")
        df = threads.deferToThread(self._flush_buffer, items_to_write)
        
        self.active_tasks.add(df)
        df.addBoth(self._cleanup_task, df)
        df.addErrback(self._log_error)

    def _cleanup_task(self, result, df):
        """ä»»åŠ¡å®Œæˆåçš„æ¸…ç†å›è°ƒ"""
        self.active_tasks.discard(df)
        return result

    def _log_error(self, failure):
        """é”™è¯¯æ—¥å¿—å›è°ƒ"""
        logger.error(f"ğŸ”¥ å¼‚æ­¥å†™å…¥ä¸¥é‡å¼‚å¸¸: {failure.getErrorMessage()}")
        return failure

    @defer.inlineCallbacks
    def close_spider(self, spider):
        """ä¼˜é›…å…³é—­"""
        logger.info(f"â³ çˆ¬è™«å…³é—­ä¸­... å‰©ä½™ Buffer: {len(self.buffer)} | è¿›è¡Œä¸­ä»»åŠ¡: {len(self.active_tasks)}")
        
        if self.buffer:
            self._trigger_flush()
        
        if self.active_tasks:
            yield defer.DeferredList(list(self.active_tasks))
            
        logger.info("âœ… Pipeline å…³é—­å®Œæˆï¼šæ‰€æœ‰æ•°æ®å·²å®‰å…¨è½åº“ã€‚")

    def _flush_buffer(self, items):
        """æ‰§è¡Œæ•°æ®åº“å†™å…¥ï¼ˆè¿è¡Œåœ¨çº¿ç¨‹æ± ä¸­ï¼‰"""
        try:
            count = self.storage.save_batch(items)
            logger.info(f"ğŸ’¾ æ‰¹é‡å†™å…¥æˆåŠŸ: {count} æ¡ (æ–°å¢)")
        except Exception as e:
            logger.error(f"âš ï¸ æ‰¹é‡å†™å…¥å¤±è´¥: {e}")


class CrawlStatusPipeline:
    """
    çˆ¬è™«çŠ¶æ€è®°å½•ç®¡é“
    """
    
    def process_item(self, item, spider):
        # æ£€æŸ¥æ˜¯å¦ä¸ºçŠ¶æ€è®°å½•item
        if isinstance(item, dict) and item.get('_status_'):
            return threads.deferToThread(self._save_status, item, spider)
        return item
    
    def _save_status(self, status_item, spider):
        """ä¿å­˜é‡‡é›†çŠ¶æ€ (è¿è¡Œåœ¨çº¿ç¨‹æ± ä¸­)"""
        session = SessionLocal()
        try:
            status = CrawlStatus(
                spider_name=status_item.get('spider_name', spider.name),
                crawl_id=status_item.get('crawl_id'),
                stage=status_item.get('stage'),
                page_no=status_item.get('page_no', 1),
                total_pages=status_item.get('total_pages', 0),
                page_size=status_item.get('page_size', 0),
                items_found=status_item.get('items_found', 0),
                items_stored=status_item.get('items_stored', 0),
                params=status_item.get('params'),
                api_url=status_item.get('api_url'),
                success=status_item.get('success', True),
                error_message=status_item.get('error_message'),
                parent_crawl_id=status_item.get('parent_crawl_id'),
                reference_id=status_item.get('reference_id')
            )
            session.add(status)
            self._update_progress(session, status_item, spider)
            session.commit()
        except Exception as e:
            session.rollback()
            logger.error(f"âŒ ä¿å­˜é‡‡é›†çŠ¶æ€å¤±è´¥: {e}")
        finally:
            session.close()
        return status_item
        
    def _update_progress(self, session, item, spider):
        """
        æ›´æ–°çˆ¬è™«å®æ—¶è¿›åº¦è¡¨
        ä½¿ç”¨ MySQL åŸç”Ÿ Upsert (INSERT ... ON DUPLICATE KEY UPDATE) 
        å½»åº•è§£å†³å¹¶å‘ä¸‹çš„å”¯ä¸€é”®å†²çªå’Œäº‹åŠ¡å›æ»šé—®é¢˜
        """
        try:
            spider_name = item.get('spider_name', spider.name)
            run_id = item.get('crawl_id', 'unknown')
            
            # 1. å‡†å¤‡æ•°æ®å­—å…¸
            data = {
                'spider_name': spider_name,
                'run_id': run_id,
                'status': 'running' if item.get('success', True) else 'error',
                'completed_tasks': item.get('page_no', 1),
                'total_tasks': item.get('total_pages', 0),
                'current_stage': item.get('stage', 'unknown'),
                'updated_at': func.now()
            }

            # 2. è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
            if data['total_tasks'] > 0:
                data['progress_percent'] = round((data['completed_tasks'] / data['total_tasks']) * 100, 2)
            else:
                data['progress_percent'] = 0.0

            # 3. è®¡ç®— items_scraped (ä»ç„¶éœ€è¦æŸ¥è¯¢ä¸€æ¬¡ï¼Œä½†è¿™æ˜¯è¯»æ“ä½œï¼Œä¸ä¼šé”è¡¨å¤ªä¹…)
            # æ³¨æ„ï¼šå¦‚æœå¯¹æ€§èƒ½è¦æ±‚æé«˜ï¼Œå¯ä»¥æ”¹ä¸º Redis è®¡æ•°æˆ–å¢é‡æ›´æ–°
            total_items = session.query(func.sum(CrawlStatus.items_stored))\
                .filter(CrawlStatus.spider_name == spider_name).scalar() or 0
            data['items_scraped'] = total_items

            # 4. æ„å»ºæè¿°ä¿¡æ¯
            desc = f"Stage: {data['current_stage']}"
            if data['total_tasks'] > 0:
                desc += f" | Page {data['completed_tasks']}/{data['total_tasks']}"
            if item.get('error_message'):
                desc += f" | Error: {item.get('error_message')}"
            data['current_item'] = desc

            # 5. æ‰§è¡ŒåŸå­ Upsert
            stmt = insert(SpiderProgress).values(data)
            
            # æŒ‡å®šå‘ç”Ÿå†²çªæ—¶éœ€è¦æ›´æ–°çš„å­—æ®µ
            # æ³¨æ„ï¼šspider_name æ˜¯å”¯ä¸€é”®ï¼Œä½œä¸ºå†²çªåˆ¤æ–­ä¾æ®
            update_dict = {
                'run_id': stmt.inserted.run_id,
                'status': stmt.inserted.status,
                'current_stage': stmt.inserted.current_stage,
                'items_scraped': stmt.inserted.items_scraped,
                'current_item': stmt.inserted.current_item,
                'updated_at': func.now()
            }

            # å…³é”®ä¿®æ”¹ï¼šåªæœ‰ list_page é˜¶æ®µæ‰æ›´æ–°ä¸»è¿›åº¦
            # è¿™æ ·å¯ä»¥é¿å… detail_page çš„è¿›åº¦ (å¦‚ 1/1) è¦†ç›–äº† list_page çš„æ€»è¿›åº¦ (å¦‚ 8/33)
            current_stage = item.get('stage', '')
            if 'list' in current_stage or current_stage == 'start_requests':
                 update_dict.update({
                    'completed_tasks': stmt.inserted.completed_tasks,
                    'total_tasks': stmt.inserted.total_tasks,
                    'progress_percent': stmt.inserted.progress_percent,
                 })
            
            upsert_stmt = stmt.on_duplicate_key_update(**update_dict)
            
            # ä½¿ç”¨ execute ç›´æ¥æ‰§è¡Œï¼Œç»•è¿‡ ORM å¯¹è±¡ç¼“å­˜
            session.execute(upsert_stmt)
            
        except Exception as e:
            logger.error(f"âš ï¸ æ›´æ–°å®æ—¶è¿›åº¦å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œä¿è¯ä¸»æµç¨‹ç»§ç»­

    def close_spider(self, spider):
        pass
