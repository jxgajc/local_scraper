import logging
import time
import hashlib
from twisted.internet import threads
from itemadapter import ItemAdapter
from .models import SessionLocal, init_db
from .models.crawl_data import CrawlData
from .models.fujian_drug import FujianDrug
from .models.hainan_drug import HainanDrug
from .models.hebei_drug import HebeiDrug
from .models.liaoning_drug import LiaoningDrug
from .models.ningxia_drug import NingxiaDrug
from .models.guangdong_drug import GuangdongDrug
from .models.tianjin_drug import TianjinDrug
from .models.shandong_drug import ShandongDrug

from .models.nhsa_drug import NhsaDrug
from .exceptions import DataValidationError

logger = logging.getLogger(__name__)

class DataCleaningPipeline:
    """æ•°æ®æ¸…æ´—ä¸æ ¡éªŒå±‚"""
    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        
        # 1. å¿…å¡«é¡¹æ ¡éªŒ
        # if not adapter.get('url'):
        #     raise DataValidationError("Drop item: Missing URL")
            
        # # 2. ç”ŸæˆæŒ‡çº¹
        # if not adapter.get('url_hash'):
        #     url = adapter.get('url')
        #     adapter['url_hash'] = hashlib.md5(url.encode('utf-8')).hexdigest()
            
        # 3. åŸºç¡€æ¸…æ´— (å»é™¤é¦–å°¾ç©ºæ ¼)
        for k, v in adapter.items():
            if isinstance(v, str):
                adapter[k] = v.strip()
                
        return item

class AsyncBatchWritePipeline:
    """
    ã€å¼‚æ­¥æ‰¹é‡å†™å…¥å±‚ã€‘
    æ ¸å¿ƒæœºåˆ¶ï¼š
    1. Buffer: å†…å­˜ä¸­æš‚å­˜ Itemã€‚
    2. DeferToThread: å°†è€—æ—¶çš„ DB å†™å…¥æ“ä½œæ‰”åˆ°çº¿ç¨‹æ± ï¼Œé¿å…é˜»å¡ Scrapy çš„ Reactorã€‚
    3. Fallback: æ‰¹é‡å¤±è´¥æ—¶è‡ªåŠ¨é™çº§ã€‚
    
    å­ç±»å¯ä»¥é€šè¿‡é‡å†™ `_get_model_class` å’Œ `_create_orm_object` æ–¹æ³•æ¥æ”¯æŒä¸åŒçš„æ¨¡å‹
    """
    def __init__(self, buffer_size=50, timeout=2):
        self.buffer = []
        self.buffer_size = buffer_size
        self.timeout = timeout
        self.last_flush = time.time()
        self.session_maker = SessionLocal
        self.is_flushing = False  # æ–°å¢ï¼šè·Ÿè¸ªæ˜¯å¦æ­£åœ¨æ‰§è¡Œflushæ“ä½œ

    @classmethod
    def from_crawler(cls, crawler):
        init_db() # ç¡®ä¿è¡¨å­˜åœ¨
        settings = crawler.settings
        return cls(
            buffer_size=settings.getint('BUFFER_THRESHOLD', 100),
            timeout=settings.getfloat('BUFFER_TIMEOUT_SEC', 2.0)
        )

    def process_item(self, item, spider):
        # é˜²æ­¢Noneè¢«æ·»åŠ åˆ°bufferä¸­
        if item is None:
            # å¦‚æœbufferä¸­æœ‰æ•°æ®ï¼Œæ‰‹åŠ¨è§¦å‘flush
            if self._should_flush() and not self.is_flushing:
                self.is_flushing = True
                # åˆ›å»ºå‰¯æœ¬å¹¶æ¸…ç©ºbufferï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
                items_to_flush = self.buffer.copy()
                self.buffer.clear()
                self.last_flush = time.time()
                
                # å¼‚æ­¥è°ƒç”¨ _flush_buffer
                df = threads.deferToThread(self._flush_buffer, items_to_flush)
                df.addCallback(self._on_flush_complete)
                df.addErrback(self._on_flush_error)
            return item
        
        self.buffer.append(item)
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ° æ•°é‡é˜ˆå€¼ æˆ– æ—¶é—´é˜ˆå€¼
        if self._should_flush() and not self.is_flushing:
            self.is_flushing = True
            # åˆ›å»ºå‰¯æœ¬å¹¶æ¸…ç©ºbufferï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
            items_to_flush = self.buffer.copy()
            self.buffer.clear()
            self.last_flush = time.time()
            
            # å¼‚æ­¥è°ƒç”¨ _flush_buffer
            df = threads.deferToThread(self._flush_buffer, items_to_flush)
            df.addCallback(self._on_flush_complete)
            df.addErrback(self._on_flush_error)
        return item

    def _should_flush(self):
        return (len(self.buffer) >= self.buffer_size) or (time.time() - self.last_flush >= self.timeout and self.buffer)

    def _get_model_class(self, item):
        """è·å–å¯¹åº”çš„æ¨¡å‹ç±»ï¼Œå­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•"""
        return CrawlData

    def _create_orm_object(self, item, model_class):
        """åˆ›å»ºORMå¯¹è±¡ï¼Œå­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•è¿›è¡Œè‡ªå®šä¹‰æ˜ å°„"""
        # ç¡®ä¿itemä¸ä¸ºNone
        if item is None:
            raise ValueError("Cannot create ORM object from None item")
            
        # é»˜è®¤å®ç°ï¼šä½¿ç”¨å­—å…¸è§£åŒ…ï¼Œè‡ªåŠ¨å°†itemä¸­çš„å­—æ®µæ˜ å°„åˆ°æ¨¡å‹
        # åªåŒ…å«æ¨¡å‹ä¸­å®šä¹‰çš„å­—æ®µ
        model_fields = [c.key for c in model_class.__table__.columns]
        item_data = {k: v for k, v in item.items() if k in model_fields}
        return model_class(**item_data)

    def _flush_buffer(self, items):
        """åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­æ‰§è¡Œ"""
        session = self.session_maker()
        try:
            orm_objects = []
            valid_items = []
            
            # è¿‡æ»¤æ‰Noneå€¼
            for item in items:
                if item is not None:
                    valid_items.append(item)
                    model_class = self._get_model_class(item)
                    orm_obj = self._create_orm_object(item, model_class)
                    orm_objects.append(orm_obj)
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œç›´æ¥è¿”å›
            if not valid_items:
                logger.info("â„¹ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®éœ€è¦å†™å…¥")
                return
            
            # å°è¯•æ‰¹é‡å†™å…¥
            session.add_all(orm_objects)
            session.commit()
            logger.info(f"âœ… æˆåŠŸæ‰¹é‡å†™å…¥ {len(valid_items)} æ¡æ•°æ®")
        except Exception as e:
            session.rollback()
            logger.error(f"âš ï¸ æ‰¹é‡å†™å…¥å¤±è´¥: {e}ï¼Œæ­£åœ¨å°è¯•é™çº§ä¸ºé€æ¡å†™å…¥...")
            self._fallback_single_write(session, orm_objects)
        finally:
            session.close()

    def _fallback_single_write(self, session, objects):
        """é™çº§ç­–ç•¥ï¼šé€æ¡å†™å…¥ï¼Œéš”ç¦»è„æ•°æ®"""
        success = 0
        for obj in objects:
            try:
                session.merge(obj) # ä½¿ç”¨ merge é¿å…ä¸»é”®é‡å¤æŠ¥é”™
                session.commit()
                success += 1
            except Exception as e:
                session.rollback()
                # å°è¯•è·å–å¯¹è±¡çš„æ ‡è¯†ä¿¡æ¯
                obj_id = getattr(obj, 'url_hash', getattr(obj, 'id', 'Unknown'))
                logger.error(f"âŒ å•æ¡å†™å…¥å¤±è´¥ (ID: {obj_id}): {e}")
        logger.info(f"ğŸ†— é™çº§å†™å…¥å®Œæˆ: æˆåŠŸ {success} / æ€»æ•° {len(objects)}")

    def _on_flush_complete(self, result):
        """å¼‚æ­¥å†™å…¥å®Œæˆåçš„å›è°ƒ"""
        self.is_flushing = False
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®éœ€è¦å¤„ç†
        if self._should_flush():
            self.process_item(None, None)  # è§¦å‘ä¸‹ä¸€æ¬¡flush

    def _on_flush_error(self, failure):
        """å¼‚æ­¥å†™å…¥å¤±è´¥åçš„å›è°ƒ"""
        logger.error(f"ğŸ”¥ å¼‚æ­¥å†™å…¥çº¿ç¨‹ä¸¥é‡å¼‚å¸¸: {failure}")
        self.is_flushing = False

    def _handle_error(self, failure):
        """ä¿ç•™æ—§çš„é”™è¯¯å¤„ç†æ–¹æ³•ï¼Œç¡®ä¿å…¼å®¹æ€§"""
        logger.error(f"ğŸ”¥ å¼‚æ­¥å†™å…¥çº¿ç¨‹ä¸¥é‡å¼‚å¸¸: {failure}")

    def close_spider(self, spider):
        """çˆ¬è™«å…³é—­æ—¶ï¼Œå¼ºåˆ¶åˆ·æ–°å‰©ä½™ Buffer"""
        # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½è¢«å¤„ç†
        if self.buffer:
            self._flush_buffer(self.buffer)
        # ç­‰å¾…å¯èƒ½æ­£åœ¨è¿›è¡Œçš„å¼‚æ­¥æ“ä½œå®Œæˆ
        import time
        while self.is_flushing:
            time.sleep(0.1)

    # å¦‚æœéœ€è¦è‡ªå®šä¹‰å­—æ®µæ˜ å°„ï¼Œå¯ä»¥é‡å†™ _create_orm_object æ–¹æ³•
    # def _create_orm_object(self, item, model_class):
    #     # è‡ªå®šä¹‰æ˜ å°„é€»è¾‘
    #     return model_class(
    #         store_name=item.get('name'),
    #         address=item.get('addr'),
    #         contact=item.get('phone'),
    #         # å…¶ä»–å­—æ®µæ˜ å°„
    #     )

class HainanDrugPipeline(AsyncBatchWritePipeline):
    """æµ·å—è¯åº—æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return HainanDrug

class FujianDrugPipeline(AsyncBatchWritePipeline):
    """ç¦å»ºè¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return FujianDrug

    # å­—æ®µæ˜ å°„å·²ç»åœ¨çˆ¬è™«çš„_create_itemæ–¹æ³•ä¸­å®Œæˆï¼Œè¿™é‡Œå¯ä»¥ä½¿ç”¨é»˜è®¤çš„æ˜ å°„
    # å¦‚æœéœ€è¦é¢å¤–çš„å­—æ®µè½¬æ¢ï¼Œå¯ä»¥é‡å†™ _create_orm_object æ–¹æ³•
class HebeiDrugPipeline(AsyncBatchWritePipeline):
    """æ²³åŒ—è¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return HebeiDrug

class LiaoningDrugPipeline(AsyncBatchWritePipeline):
    """è¾½å®è¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return LiaoningDrug

class NingxiaDrugPipeline(AsyncBatchWritePipeline):
    """ç¦å»ºè¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return NingxiaDrug

class GuangdongDrugPipeline(AsyncBatchWritePipeline):
    """å¹¿ä¸œè¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return GuangdongDrug

class TianjinDrugPipeline(AsyncBatchWritePipeline):
    """å¹¿ä¸œè¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return TianjinDrug

class NhsaDrugPipeline(AsyncBatchWritePipeline):
    """å›½å®¶åŒ»ä¿è¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return NhsaDrug

class ShandongDrugPipeline(AsyncBatchWritePipeline):
    """å›½å®¶åŒ»ä¿è¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return ShandongDrug
    # å­—æ®µæ˜ å°„å·²ç»åœ¨çˆ¬è™«çš„_create_itemæ–¹æ³•ä¸­å®Œæˆï¼Œè¿™é‡Œå¯ä»¥ä½¿ç”¨é»˜è®¤çš„æ˜ å°„
    # å¦‚æœéœ€è¦é¢å¤–çš„å­—æ®µè½¬æ¢ï¼Œå¯ä»¥é‡å†™ _create_orm_object æ–¹æ³•