import logging
import time
import hashlib
from twisted.internet import threads
from itemadapter import ItemAdapter
from .models import SessionLocal, init_db
from .models.crawl_data import CrawlData
from .models.hainan_drug_store import HainanDrugStore
from .models.nhsa_drug import NhsaDrug
from .exceptions import DataValidationError

logger = logging.getLogger(__name__)

class DataCleaningPipeline:
    """æ•°æ®æ¸…æ´—ä¸æ ¡éªŒå±‚"""
    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        
        # 1. å¿…å¡«é¡¹æ ¡éªŒ
        if not adapter.get('url'):
            raise DataValidationError("Drop item: Missing URL")
            
        # 2. ç”ŸæˆæŒ‡çº¹
        if not adapter.get('url_hash'):
            url = adapter.get('url')
            adapter['url_hash'] = hashlib.md5(url.encode('utf-8')).hexdigest()
            
        # 3. åŸºç¡€æ¸…æ´— (å»é™¤é¦–å°¾ç©ºæ ¼)
        for k, v in adapter.items():
            if isinstance(v, str):
                adapter[k] = v.strip()
                
        return item

class AsyncBatchWritePipeline:
    """
    ã€å¼‚æ­¥æ‰¹é‡å†™å…¥å±‚ã€‘
    æ ¸å¿ƒæœºåˆ¶ï¼š
    1. Buffer: å†…å­˜ä¸­æš‚å­˜ Itemã€‚
    2. DeferToThread: å°†è€—æ—¶çš„ DB å†™å…¥æ“ä½œæ‰”åˆ°çº¿ç¨‹æ± ï¼Œé¿å…é˜»å¡ Scrapy çš„ Reactorã€‚
    3. Fallback: æ‰¹é‡å¤±è´¥æ—¶è‡ªåŠ¨é™çº§ã€‚
    
    å­ç±»å¯ä»¥é€šè¿‡é‡å†™ `_get_model_class` å’Œ `_create_orm_object` æ–¹æ³•æ¥æ”¯æŒä¸åŒçš„æ¨¡å‹
    """
    def __init__(self, buffer_size=50, timeout=2):
        self.buffer = []
        self.buffer_size = buffer_size
        self.timeout = timeout
        self.last_flush = time.time()
        self.session_maker = SessionLocal

    @classmethod
    def from_crawler(cls, crawler):
        init_db() # ç¡®ä¿è¡¨å­˜åœ¨
        settings = crawler.settings
        return cls(
            buffer_size=settings.getint('BUFFER_THRESHOLD', 100),
            timeout=settings.getfloat('BUFFER_TIMEOUT_SEC', 2.0)
        )

    def process_item(self, item, spider):
        self.buffer.append(item)
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ° æ•°é‡é˜ˆå€¼ æˆ– æ—¶é—´é˜ˆå€¼
        if self._should_flush():
            # å¼‚æ­¥è°ƒç”¨ _flush_buffer
            df = threads.deferToThread(self._flush_buffer, list(self.buffer))
            df.addErrback(self._handle_error)
            
            # æ¸…ç©º Buffer
            self.buffer.clear()
            self.last_flush = time.time()
        return item

    def _should_flush(self):
        return (len(self.buffer) >= self.buffer_size) or (time.time() - self.last_flush >= self.timeout and self.buffer)

    def _get_model_class(self, item):
        """è·å–å¯¹åº”çš„æ¨¡å‹ç±»ï¼Œå­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•"""
        return CrawlData

    def _create_orm_object(self, item, model_class):
        """åˆ›å»ºORMå¯¹è±¡ï¼Œå­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•è¿›è¡Œè‡ªå®šä¹‰æ˜ å°„"""
        # é»˜è®¤å®ç°ï¼šä½¿ç”¨å­—å…¸è§£åŒ…ï¼Œè‡ªåŠ¨å°†itemä¸­çš„å­—æ®µæ˜ å°„åˆ°æ¨¡å‹
        # åªåŒ…å«æ¨¡å‹ä¸­å®šä¹‰çš„å­—æ®µ
        model_fields = [c.key for c in model_class.__table__.columns]
        item_data = {k: v for k, v in item.items() if k in model_fields}
        return model_class(**item_data)

    def _flush_buffer(self, items):
        """åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­æ‰§è¡Œ"""
        session = self.session_maker()
        try:
            orm_objects = []
            for item in items:
                model_class = self._get_model_class(item)
                orm_obj = self._create_orm_object(item, model_class)
                orm_objects.append(orm_obj)
            
            # å°è¯•æ‰¹é‡å†™å…¥
            session.add_all(orm_objects)
            session.commit()
            logger.info(f"âœ… æˆåŠŸæ‰¹é‡å†™å…¥ {len(items)} æ¡æ•°æ®")
        except Exception as e:
            session.rollback()
            logger.error(f"âš ï¸ æ‰¹é‡å†™å…¥å¤±è´¥: {e}ï¼Œæ­£åœ¨å°è¯•é™çº§ä¸ºé€æ¡å†™å…¥...")
            self._fallback_single_write(session, orm_objects)
        finally:
            session.close()

    def _fallback_single_write(self, session, objects):
        """é™çº§ç­–ç•¥ï¼šé€æ¡å†™å…¥ï¼Œéš”ç¦»è„æ•°æ®"""
        success = 0
        for obj in objects:
            try:
                session.merge(obj) # ä½¿ç”¨ merge é¿å…ä¸»é”®é‡å¤æŠ¥é”™
                session.commit()
                success += 1
            except Exception as e:
                session.rollback()
                # å°è¯•è·å–å¯¹è±¡çš„æ ‡è¯†ä¿¡æ¯
                obj_id = getattr(obj, 'url_hash', getattr(obj, 'id', 'Unknown'))
                logger.error(f"âŒ å•æ¡å†™å…¥å¤±è´¥ (ID: {obj_id}): {e}")
        logger.info(f"ğŸ†— é™çº§å†™å…¥å®Œæˆ: æˆåŠŸ {success} / æ€»æ•° {len(objects)}")

    def _handle_error(self, failure):
        logger.error(f"ğŸ”¥ å¼‚æ­¥å†™å…¥çº¿ç¨‹ä¸¥é‡å¼‚å¸¸: {failure}")

    def close_spider(self, spider):
        """çˆ¬è™«å…³é—­æ—¶ï¼Œå¼ºåˆ¶åˆ·æ–°å‰©ä½™ Buffer"""
        if self.buffer:
            self._flush_buffer(self.buffer)


class HainanDrugStorePipeline(AsyncBatchWritePipeline):
    """æµ·å—è¯åº—æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return HainanDrugStore

    # å¦‚æœéœ€è¦è‡ªå®šä¹‰å­—æ®µæ˜ å°„ï¼Œå¯ä»¥é‡å†™ _create_orm_object æ–¹æ³•
    # def _create_orm_object(self, item, model_class):
    #     # è‡ªå®šä¹‰æ˜ å°„é€»è¾‘
    #     return model_class(
    #         store_name=item.get('name'),
    #         address=item.get('addr'),
    #         contact=item.get('phone'),
    #         # å…¶ä»–å­—æ®µæ˜ å°„
    #     )


class NhsaDrugPipeline(AsyncBatchWritePipeline):
    """å›½å®¶åŒ»ä¿è¯å“æ•°æ®å†™å…¥ç®¡é“"""
    def _get_model_class(self, item):
        return NhsaDrug

    # å­—æ®µæ˜ å°„å·²ç»åœ¨çˆ¬è™«çš„_create_itemæ–¹æ³•ä¸­å®Œæˆï¼Œè¿™é‡Œå¯ä»¥ä½¿ç”¨é»˜è®¤çš„æ˜ å°„
    # å¦‚æœéœ€è¦é¢å¤–çš„å­—æ®µè½¬æ¢ï¼Œå¯ä»¥é‡å†™ _create_orm_object æ–¹æ³•