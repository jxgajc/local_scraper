import logging
import time
from twisted.internet import threads, defer
from itemadapter import ItemAdapter
from sqlalchemy.sql import func
from .models import SessionLocal
from .models.crawl_status import CrawlStatus
from .models.spider_progress import SpiderProgress
from .models.crawl_data import CrawlData # Fallback
from .exceptions import DataValidationError

logger = logging.getLogger(__name__)

class DataCleaningPipeline:
    """æ•°æ®æ¸…æ´—ä¸æ ¡éªŒå±‚"""
    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        
        # åŸºç¡€æ¸…æ´— (å»é™¤é¦–å°¾ç©ºæ ¼)
        for k, v in adapter.items():
            if isinstance(v, str):
                adapter[k] = v.strip()
                
        return item

class UniversalBatchWritePipeline:
    """
    ã€é€šç”¨å¼‚æ­¥æ‰¹é‡å†™å…¥ç®¡é“ã€‘
    æ›¿ä»£åŸæœ‰çš„ *DrugPipelineï¼Œæ ¹æ® Item åŠ¨æ€è¯†åˆ« Model å¹¶å†™å…¥ã€‚
    """
    
    def __init__(self, buffer_size=50, timeout=2):
        self.buffer = []
        self.buffer_size = buffer_size
        self.timeout = timeout
        self.last_flush_time = time.time()
        
        # æ•°æ®åº“ Session å·¥å‚
        self.session_maker = SessionLocal 
        
        # ä½¿ç”¨ set ä»…å­˜å‚¨å½“å‰æ´»è·ƒçš„å¼‚æ­¥ä»»åŠ¡
        self.active_tasks = set()

    @classmethod
    def from_crawler(cls, crawler):
        settings = crawler.settings
        return cls(
            buffer_size=settings.getint('BUFFER_THRESHOLD', 100),
            timeout=settings.getfloat('BUFFER_TIMEOUT_SEC', 2.0)
        )

    def process_item(self, item, spider):
        # 1. è¿‡æ»¤ None æˆ– çŠ¶æ€ Item
        if item is None or isinstance(item, dict):
            # å¦‚æœæ˜¯å­—å…¸ï¼ˆé€šå¸¸æ˜¯çŠ¶æ€Itemï¼‰ï¼Œäº¤ç”±ä¸‹ä¸€ä¸ªPipelineå¤„ç†
            return item

        # 2. æ·»åŠ åˆ° Buffer
        self.buffer.append(item)

        # 3. æ£€æŸ¥æ˜¯å¦æ»¡è¶³å†™å…¥æ¡ä»¶
        if self._should_flush():
            self._trigger_flush()

        return item

    def _should_flush(self):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ·æ–°"""
        has_data = len(self.buffer) > 0
        time_expired = (time.time() - self.last_flush_time) >= self.timeout
        size_reached = len(self.buffer) >= self.buffer_size
        return size_reached or (has_data and time_expired)

    def _trigger_flush(self):
        """è§¦å‘å¼‚æ­¥å†™å…¥ä»»åŠ¡"""
        items_to_write = self.buffer
        self.buffer = [] # æŒ‡å‘æ–°åˆ—è¡¨
        self.last_flush_time = time.time()

        if not items_to_write:
            return

        logger.debug(f"ğŸš€ è§¦å‘å¼‚æ­¥å†™å…¥: {len(items_to_write)} æ¡")
        df = threads.deferToThread(self._flush_buffer, items_to_write)
        
        self.active_tasks.add(df)
        df.addBoth(self._cleanup_task, df)
        df.addErrback(self._log_error)

    def _cleanup_task(self, result, df):
        """ä»»åŠ¡å®Œæˆåçš„æ¸…ç†å›è°ƒ"""
        self.active_tasks.discard(df)
        return result

    def _log_error(self, failure):
        """é”™è¯¯æ—¥å¿—å›è°ƒ"""
        logger.error(f"ğŸ”¥ å¼‚æ­¥å†™å…¥ä¸¥é‡å¼‚å¸¸: {failure.getErrorMessage()}")
        return failure

    @defer.inlineCallbacks
    def close_spider(self, spider):
        """ä¼˜é›…å…³é—­"""
        logger.info(f"â³ çˆ¬è™«å…³é—­ä¸­... å‰©ä½™ Buffer: {len(self.buffer)} | è¿›è¡Œä¸­ä»»åŠ¡: {len(self.active_tasks)}")
        
        if self.buffer:
            self._trigger_flush()
        
        if self.active_tasks:
            yield defer.DeferredList(list(self.active_tasks))
            
        logger.info("âœ… Pipeline å…³é—­å®Œæˆï¼šæ‰€æœ‰æ•°æ®å·²å®‰å…¨è½åº“ã€‚")

    def _get_model_class(self, item):
        """
        åŠ¨æ€è·å– Item å¯¹åº”çš„ SQLAlchemy Model ç±»ã€‚
        ä¼˜å…ˆè°ƒç”¨ item.get_model_class()ï¼Œå…¶æ¬¡æŸ¥æ‰¾ item['model_class']ï¼Œæœ€åå›é€€åˆ° CrawlDataã€‚
        """
        if hasattr(item, 'get_model_class'):
            return item.get_model_class()
        
        # å…¼å®¹æ—§é€»è¾‘æˆ–å­—å…¸ç±»å‹çš„ item
        if isinstance(item, dict) and 'model_class' in item:
            return item['model_class']
            
        return CrawlData

    def _create_orm_object(self, item, model_class):
        if not item: return None
        # è‡ªåŠ¨æ˜ å°„ Item å­—æ®µåˆ° Model å­—æ®µ
        model_fields = [c.key for c in model_class.__table__.columns]
        
        # ItemAdapter ç»Ÿä¸€å¤„ç† Item å¯¹è±¡å’Œå­—å…¸
        adapter = ItemAdapter(item)
        item_data = {k: v for k, v in adapter.items() if k in model_fields}
        
        return model_class(**item_data)

    def _flush_buffer(self, items):
        """æ‰§è¡Œæ•°æ®åº“å†™å…¥ï¼ˆè¿è¡Œåœ¨çº¿ç¨‹æ± ä¸­ï¼‰"""
        session = self.session_maker()
        try:
            orm_objects = []
            for item in items:
                if item:
                    model = self._get_model_class(item)
                    obj = self._create_orm_object(item, model)
                    if obj:
                        orm_objects.append(obj)
            
            if not orm_objects: return

            session.add_all(orm_objects)
            session.commit()
            logger.info(f"ğŸ’¾ æ‰¹é‡å†™å…¥æˆåŠŸ: {len(orm_objects)} æ¡")
            
        except Exception as e:
            session.rollback()
            logger.error(f"âš ï¸ æ‰¹é‡å†™å…¥å¤±è´¥: {e}ï¼Œæ­£åœ¨é™çº§ä¸ºé€æ¡å†™å…¥...")
            self._fallback_single_write(session, orm_objects)
        finally:
            session.close()

    def _fallback_single_write(self, session, objects):
        count = 0
        for obj in objects:
            try:
                session.merge(obj)
                session.commit()
                count += 1
            except Exception as e:
                session.rollback()
                logger.error(f"âŒ å•æ¡å†™å…¥ä¸¢å¼ƒ: {e}")
        logger.info(f"ğŸ†— é™çº§å¤„ç†å®Œæˆ: æŒ½å› {count}/{len(objects)} æ¡")


class CrawlStatusPipeline:
    """
    çˆ¬è™«çŠ¶æ€è®°å½•ç®¡é“
    """
    
    def process_item(self, item, spider):
        # æ£€æŸ¥æ˜¯å¦ä¸ºçŠ¶æ€è®°å½•item
        if isinstance(item, dict) and item.get('_status_'):
            return threads.deferToThread(self._save_status, item, spider)
        return item
    
    def _save_status(self, status_item, spider):
        """ä¿å­˜é‡‡é›†çŠ¶æ€ (è¿è¡Œåœ¨çº¿ç¨‹æ± ä¸­)"""
        session = SessionLocal()
        try:
            status = CrawlStatus(
                spider_name=status_item.get('spider_name', spider.name),
                crawl_id=status_item.get('crawl_id'),
                stage=status_item.get('stage'),
                page_no=status_item.get('page_no', 1),
                total_pages=status_item.get('total_pages', 0),
                page_size=status_item.get('page_size', 0),
                items_found=status_item.get('items_found', 0),
                items_stored=status_item.get('items_stored', 0),
                params=status_item.get('params'),
                api_url=status_item.get('api_url'),
                success=status_item.get('success', True),
                error_message=status_item.get('error_message'),
                parent_crawl_id=status_item.get('parent_crawl_id'),
                reference_id=status_item.get('reference_id')
            )
            session.add(status)
            self._update_progress(session, status_item, spider)
            session.commit()
        except Exception as e:
            session.rollback()
            logger.error(f"âŒ ä¿å­˜é‡‡é›†çŠ¶æ€å¤±è´¥: {e}")
        finally:
            session.close()
        return status_item
        
    def _update_progress(self, session, item, spider):
        """æ›´æ–°çˆ¬è™«å®æ—¶è¿›åº¦è¡¨"""
        try:
            spider_name = item.get('spider_name', spider.name)
            progress = session.query(SpiderProgress).filter_by(spider_name=spider_name).first()
            if not progress:
                progress = SpiderProgress(spider_name=spider_name)
                session.add(progress)
            
            progress.run_id = item.get('crawl_id', 'unknown')
            progress.status = 'running' if item.get('success', True) else 'error'
            
            page = item.get('page_no', 1)
            total = item.get('total_pages', 0)
            
            progress.total_tasks = total
            progress.completed_tasks = page
            if total > 0:
                progress.progress_percent = round((page / total) * 100, 2)
                
            progress.current_stage = item.get('stage', 'unknown')
            progress.items_scraped = session.query(CrawlStatus).filter_by(spider_name=spider_name).with_entities(func.sum(CrawlStatus.items_stored)).scalar() or 0
            
            desc = f"Stage: {progress.current_stage}"
            if total > 0:
                desc += f" | Page {page}/{total}"
            if item.get('error_message'):
                desc += f" | Error: {item.get('error_message')}"
            progress.current_item = desc
            
        except Exception as e:
            logger.error(f"âš ï¸ æ›´æ–°å®æ—¶è¿›åº¦å¤±è´¥: {e}")

    def close_spider(self, spider):
        pass
